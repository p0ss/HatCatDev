`torch_dtype` is deprecated! Use `dtype` instead!
================================================================================
DETAILED PROBE OVERHEAD ANALYSIS
================================================================================

Configuration:
  Model: google/gemma-3-4b-pt
  Device: cuda
  Prompt: "Artificial intelligence can help society by"

Loading model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.70it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.12it/s]
✓ Model loaded

--------------------------------------------------------------------------------
INITIALIZING PROBE MANAGER (base_layers=[0,1])
--------------------------------------------------------------------------------
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v3 v3.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v2 v2.2.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v1 v1.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation', 'text']
Using probe pack: gemma-3-4b-pt_sumo-wordnet-v3

✓ Loaded metadata for 5668 concepts across 7 layers
  Parent-child relationships: 1275

Initializing DynamicProbeManager...
  Base layers: [0, 1]
  Load threshold: 0.3
  Max probes in memory: 500
  Inferred hidden_dim: 2560
  Preallocating model pool (100 models)...
✓ Base layers loaded: 267 probes
✓ Loaded 267 base probes

Extracting hidden state...

================================================================================
1. SINGLE TOKEN DETAILED BREAKDOWN (top_k=10)
================================================================================

Total time: 227.99ms

Breakdown:
  Initial detection:  24.70ms (10.8%)
    └─ 184 base probes @ 0.134ms/probe
  Child loading:      197.52ms (86.6%)
    └─ 84 children loaded
    └─ 2.35ms per child
  Child detection:    5.55ms (2.4%)
    └─ 0.066ms/probe
  Cache management:   0.11ms (0.0%)
  Python overhead:    0.12ms (0.1%) [estimated]

Results:
  Concepts detected: 9
  Total probes run: 268

================================================================================
2. BASE PROBE INFERENCE MICROBENCHMARK
================================================================================

Running 100 iterations of 268 probes...

Results:
  Average total time: 16.23ms
  Per-probe time:     0.0606ms
  Num probes:         268

================================================================================
3. PYTHON OVERHEAD MICROBENCHMARK
================================================================================

Running 1000 iterations with 268 concepts...

Results:
  Sorting (top-k):    0.0145ms
  Dict lookups:       0.0004ms
  List operations:    0.0012ms
  Total Python:       0.0161ms

================================================================================
4. MULTI-TOKEN AVERAGE (10 tokens)
================================================================================

Generation: 209.36ms (20.94ms/token)
Hidden extraction: 0.22ms (0.02ms/token)

Probe detection: 473.15ms (47.32ms/token)
  Initial detection:  182.20ms (18.22ms/token)
  Child loading:      262.50ms (26.25ms/token)
  Child detection:    27.07ms (2.71ms/token)
  Cache management:   0.74ms (0.07ms/token)
  Python overhead:    0.65ms (0.06ms/token) [estimated]

Total children loaded: 395 (39.5/token)

================================================================================
SUMMARY & OPTIMIZATION TARGETS
================================================================================

Current per-token overhead: 47.32ms
Target overhead: 10.0ms
Gap: 37.32ms (78.9%)

Optimization opportunities (ranked by impact):
  1. Child loading (disk I/O)        26.25ms ( 55.5%)
  2. Initial detection               18.22ms ( 38.5%)
  3. Child detection                  2.71ms (  5.7%)
  4. Cache management                 0.07ms (  0.2%)
  5. Python overhead                  0.06ms (  0.1%)

Recommendations:
  • Child loading: Implement caching/preloading (0.66ms per child)
    - Consider LRU cache for frequently accessed children
    - Or reduce base layers further (currently [0,1])
