`torch_dtype` is deprecated! Use `dtype` instead!
================================================================================
TORCH.COMPILE() PROBE INFERENCE BENCHMARK
================================================================================

Configuration:
  Model: google/gemma-3-4b-pt
  Device: cuda
  Prompt: "Artificial intelligence can help society by"

Loading model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.68it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]
/home/poss/Documents/Code/HatCat/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
✓ Model loaded

Initializing DynamicProbeManager...
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v3 v3.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v2 v2.2.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v1 v1.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation', 'text']
Using probe pack: gemma-3-4b-pt_sumo-wordnet-v3

✓ Loaded metadata for 5668 concepts across 7 layers
  Parent-child relationships: 1275

Initializing DynamicProbeManager...
  Base layers: [0, 1]
  Load threshold: 0.3
  Max probes in memory: 500
  Inferred hidden_dim: 2560
  Preallocating model pool (100 models)...
✓ Base layers loaded: 267 probes
✓ Loaded 267 base probes

Extracting hidden state...
✓ Hidden state shape: torch.Size([1, 2560])

================================================================================
1. BASELINE: Uncompiled Probe Inference
================================================================================

Results:
  Average time: 16.62ms
  Per-probe:    0.0622ms
  Num probes:   267

================================================================================
2. COMPILED: Individual Probe Compilation
================================================================================
  Compiling individual probes...
  Warming up compiled probes...

Results:
  Compilation time: 1.30s
  Average time:     46.79ms
  Per-probe:        0.1752ms
  Speedup:          0.36x
  Improvement:      -181.6%

================================================================================
3. COMPILED: Batched Probe Inference
================================================================================
  Attempting batched compilation...
  ✗ Probe structure incompatible: ['net.0.weight', 'net.0.bias', 'net.3.weight', 'net.3.bias', 'net.6.weight', 'net.6.bias']

✗ Batched compilation not feasible for this probe architecture

================================================================================
SUMMARY
================================================================================
Traceback (most recent call last):
  File "/home/poss/Documents/Code/HatCat/scripts/benchmark_probe_compilation.py", line 272, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/poss/Documents/Code/HatCat/scripts/benchmark_probe_compilation.py", line 263, in main
    print(f"\nBaseline: {baseline_time:.2f}ms ({baseline_time/num_probes:.4f}ms per probe)")
                                                ~~~~~~~~~~~~~^~~~~~~~~~~
TypeError: unsupported operand type(s) for /: 'float' and 'NoneType'
