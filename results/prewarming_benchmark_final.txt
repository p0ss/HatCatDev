`torch_dtype` is deprecated! Use `dtype` instead!
================================================================================
PRE-WARMING STRATEGY BENCHMARK
================================================================================

Configuration:
  Model: google/gemma-3-4b-pt
  Device: cuda
  Prompt: "Artificial intelligence can help society by"
  Generation tokens: 10

Loading model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.65it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]
✓ Model loaded

Initializing DynamicLensManager...
✓ Loaded lens pack: gemma-3-4b-pt_sumo-wordnet-v3 v3.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded lens pack: gemma-3-4b-pt_sumo-wordnet-v2 v2.2.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded lens pack: gemma-3-4b-pt_sumo-wordnet-v1 v1.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation', 'text']
Using lens pack: gemma-3-4b-pt_sumo-wordnet-v3

✓ Loaded metadata for 5668 concepts across 7 layers
  Parent-child relationships: 1275

Initializing DynamicLensManager...
  Base layers: [0, 1]
  Load threshold: 0.3
  Max lenses in memory: 500
  Inferred hidden_dim: 2560
  Preallocating model pool (100 models)...
✓ Base layers loaded: 267 lenses
✓ Loaded 267 base lenses

================================================================================
1. BASELINE: Cold Start (load children during generation)
================================================================================

Results:
  Generation time:         518.38ms
  Lens detection:         692.24ms
    └─ Per token:          69.22ms/token
  Child loading (disk I/O):466.38ms
    └─ Per token:          46.64ms/token
  Total children loaded:   367
    └─ Per token:          36.7 children/token

================================================================================
2. PRE-WARMING: Load children during prompt processing
================================================================================

Prompt Phase:
  Total time:              217.61ms
  Lens detection:         197.63ms
  Child loading:           170.07ms
  Children loaded:         84

Generation Phase:
  Generation time:         215.19ms
  Lens detection:         510.20ms
    └─ Per token:          51.02ms/token
  Child loading (disk I/O):265.08ms
    └─ Per token:          26.51ms/token
  Children loaded:         324
    └─ Per token:          32.4 children/token

================================================================================
COMPARISON
================================================================================

Child Loading (disk I/O) per token:
  Baseline:    46.64ms/token
  Pre-warming: 26.51ms/token
  Reduction:   20.13ms/token (43.2% improvement)

Total Lens Overhead per token:
  Baseline:    69.22ms/token
  Pre-warming: 51.02ms/token
  Reduction:   18.20ms/token (26.3% improvement)

Children loaded during generation:
  Baseline:    36.7 children/token
  Pre-warming: 32.4 children/token
  Reduction:   4.3 children/token

Estimated concept overlap:
  Prompt pre-loaded 84 children
  Generation avoided loading ~43 children
  Overlap: ~51%

================================================================================
CONCLUSION
================================================================================

✓ Pre-warming provides significant benefit: 18.20ms/token reduction
  Current overhead: 51.02ms/token (vs 69.22ms baseline)
  Gap to target (<10ms): 41.02ms

Note: Prompt processing overhead (217.61ms) is one-time cost
      amortized over 10 generation tokens = 21.76ms/token
