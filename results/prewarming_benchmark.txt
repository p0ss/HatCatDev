`torch_dtype` is deprecated! Use `dtype` instead!
================================================================================
PRE-WARMING STRATEGY BENCHMARK
================================================================================

Configuration:
  Model: google/gemma-3-4b-pt
  Device: cuda
  Prompt: "Artificial intelligence can help society by"
  Generation tokens: 10

Loading model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.67it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]
✓ Model loaded

Initializing DynamicLensManager...
✓ Loaded lens pack: gemma-3-4b-pt_sumo-wordnet-v3 v3.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded lens pack: gemma-3-4b-pt_sumo-wordnet-v2 v2.2.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded lens pack: gemma-3-4b-pt_sumo-wordnet-v1 v1.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation', 'text']
Using lens pack: gemma-3-4b-pt_sumo-wordnet-v3

✓ Loaded metadata for 5668 concepts across 7 layers
  Parent-child relationships: 1275

Initializing DynamicLensManager...
  Base layers: [0, 1]
  Load threshold: 0.3
  Max lenses in memory: 500
  Inferred hidden_dim: 2560
  Preallocating model pool (100 models)...
✓ Base layers loaded: 267 lenses
✓ Loaded 267 base lenses

================================================================================
1. BASELINE: Cold Start (load children during generation)
================================================================================

Results:
  Generation time:         503.62ms
  Lens detection:         659.73ms
    └─ Per token:          65.97ms/token
  Child loading (disk I/O):441.34ms
    └─ Per token:          44.13ms/token
  Total children loaded:   396
    └─ Per token:          39.6 children/token

================================================================================
2. PRE-WARMING: Load children during prompt processing
================================================================================

Prompt Phase:
  Total time:              209.68ms
  Lens detection:         189.91ms
  Child loading:           164.50ms
  Children loaded:         84

Generation Phase:
  Generation time:         203.00ms
  Lens detection:         367.97ms
    └─ Per token:          36.80ms/token
  Child loading (disk I/O):166.10ms
    └─ Per token:          16.61ms/token
  Children loaded:         211
    └─ Per token:          21.1 children/token

================================================================================
COMPARISON
================================================================================

Child Loading (disk I/O) per token:
  Baseline:    44.13ms/token
  Pre-warming: 16.61ms/token
  Reduction:   27.52ms/token (62.4% improvement)

Total Lens Overhead per token:
  Baseline:    65.97ms/token
  Pre-warming: 36.80ms/token
  Reduction:   29.18ms/token (44.2% improvement)

Children loaded during generation:
  Baseline:    39.6 children/token
  Pre-warming: 21.1 children/token
  Reduction:   18.5 children/token

Estimated concept overlap:
  Prompt pre-loaded 84 children
  Generation avoided loading ~185 children
  Overlap: ~100%

================================================================================
CONCLUSION
================================================================================

✓ Pre-warming provides significant benefit: 29.18ms/token reduction
  Current overhead: 36.80ms/token (vs 65.97ms baseline)
  Gap to target (<10ms): 26.80ms

Note: Prompt processing overhead (209.68ms) is one-time cost
      amortized over 10 generation tokens = 20.97ms/token
