`torch_dtype` is deprecated! Use `dtype` instead!
================================================================================
DETAILED PROBE OVERHEAD ANALYSIS
================================================================================

Configuration:
  Model: google/gemma-3-4b-pt
  Device: cuda
  Prompt: "Artificial intelligence can help society by"

Loading model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.68it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.11it/s]
✓ Model loaded

--------------------------------------------------------------------------------
INITIALIZING PROBE MANAGER (base_layers=[0])
--------------------------------------------------------------------------------
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v3 v3.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v2 v2.2.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v1 v1.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation', 'text']
Using probe pack: gemma-3-4b-pt_sumo-wordnet-v3

✓ Loaded metadata for 5668 concepts across 7 layers
  Parent-child relationships: 1275

Initializing DynamicProbeManager...
  Base layers: [0]
  Load threshold: 0.3
  Max probes in memory: 500
  Inferred hidden_dim: 2560
  Preallocating model pool (100 models)...
✓ Base layers loaded: 10 probes
✓ Loaded 10 base probes

Extracting hidden state...

================================================================================
1. SINGLE TOKEN DETAILED BREAKDOWN (top_k=10)
================================================================================

Total time: 491.30ms

Breakdown:
  Initial detection:  6.13ms (1.2%)
    └─ -230 base probes @ -0.027ms/probe
  Child loading:      468.06ms (95.3%)
    └─ 250 children loaded
    └─ 1.87ms per child
  Child detection:    16.93ms (3.4%)
    └─ 0.068ms/probe
  Cache management:   0.12ms (0.0%)
  Python overhead:    0.07ms (0.0%) [estimated]

Results:
  Concepts detected: 10
  Total probes run: 20

================================================================================
2. BASE PROBE INFERENCE MICROBENCHMARK
================================================================================

Running 100 iterations of 20 probes...

Results:
  Average total time: 1.37ms
  Per-probe time:     0.0683ms
  Num probes:         20

================================================================================
3. PYTHON OVERHEAD MICROBENCHMARK
================================================================================

Running 1000 iterations with 20 concepts...

Results:
  Sorting (top-k):    0.0012ms
  Dict lookups:       0.0004ms
  List operations:    0.0011ms
  Total Python:       0.0027ms

================================================================================
4. MULTI-TOKEN AVERAGE (10 tokens)
================================================================================

Generation: 219.56ms (21.96ms/token)
Hidden extraction: 0.20ms (0.02ms/token)

Probe detection: 363.45ms (36.35ms/token)
  Initial detection:  14.19ms (1.42ms/token)
  Child loading:      297.80ms (29.78ms/token)
  Child detection:    50.82ms (5.08ms/token)
  Cache management:   0.36ms (0.04ms/token)
  Python overhead:    0.28ms (0.03ms/token) [estimated]

Total children loaded: 662 (66.2/token)

================================================================================
SUMMARY & OPTIMIZATION TARGETS
================================================================================

Current per-token overhead: 36.35ms
Target overhead: 10.0ms
Gap: 26.35ms (72.5%)

Optimization opportunities (ranked by impact):
  1. Child loading (disk I/O)        29.78ms ( 81.9%)
  2. Child detection                  5.08ms ( 14.0%)
  3. Initial detection                1.42ms (  3.9%)
  4. Cache management                 0.04ms (  0.1%)
  5. Python overhead                  0.03ms (  0.1%)

Recommendations:
  • Child loading: Implement caching/preloading (0.45ms per child)
    - Consider LRU cache for frequently accessed children
    - Or reduce base layers further (currently [0,1])
