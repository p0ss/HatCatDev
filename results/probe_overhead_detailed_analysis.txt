`torch_dtype` is deprecated! Use `dtype` instead!
================================================================================
DETAILED PROBE OVERHEAD ANALYSIS
================================================================================

Configuration:
  Model: google/gemma-3-4b-pt
  Device: cuda
  Prompt: "Artificial intelligence can help society by"

Loading model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.73it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.17it/s]
✓ Model loaded

--------------------------------------------------------------------------------
INITIALIZING PROBE MANAGER (base_layers=[0,1])
--------------------------------------------------------------------------------
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v3 v3.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v2 v2.2.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v1 v1.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation', 'text']
Using probe pack: gemma-3-4b-pt_sumo-wordnet-v3

✓ Loaded metadata for 5668 concepts across 7 layers
  Parent-child relationships: 1275

Initializing DynamicProbeManager...
  Base layers: [0, 1]
  Load threshold: 0.3
  Max probes in memory: 500
  Inferred hidden_dim: 2560
  Preallocating model pool (100 models)...
✓ Base layers loaded: 267 probes
✓ Loaded 267 base probes

Extracting hidden state...

================================================================================
1. SINGLE TOKEN DETAILED BREAKDOWN (top_k=10)
================================================================================

Total time: 229.18ms

Breakdown:
  Initial detection:  24.18ms (10.5%)
    └─ 233 base probes @ 0.104ms/probe
  Child loading:      198.65ms (86.7%)
    └─ 84 children loaded
    └─ 2.36ms per child
  Child detection:    5.85ms (2.6%)
    └─ 0.070ms/probe
  Cache management:   0.41ms (0.2%)
  Python overhead:    0.09ms (0.0%) [estimated]

Results:
  Concepts detected: 9
  Total probes run: 317

================================================================================
2. BASE PROBE INFERENCE MICROBENCHMARK
================================================================================

Running 100 iterations of 317 probes...

Results:
  Average total time: 19.96ms
  Per-probe time:     0.0630ms
  Num probes:         317

================================================================================
3. PYTHON OVERHEAD MICROBENCHMARK
================================================================================

Running 1000 iterations with 317 concepts...

Results:
  Sorting (top-k):    0.0176ms
  Dict lookups:       0.0005ms
  List operations:    0.0014ms
  Total Python:       0.0194ms

================================================================================
4. MULTI-TOKEN AVERAGE (10 tokens)
================================================================================

Generation: 212.52ms (21.25ms/token)
Hidden extraction: 0.28ms (0.03ms/token)

Probe detection: 769.69ms (76.97ms/token)
  Initial detection:  226.82ms (22.68ms/token)
  Child loading:      520.18ms (52.02ms/token)
  Child detection:    18.56ms (1.86ms/token)
  Cache management:   3.30ms (0.33ms/token)
  Python overhead:    0.83ms (0.08ms/token) [estimated]

Total children loaded: 258 (25.8/token)

================================================================================
SUMMARY & OPTIMIZATION TARGETS
================================================================================

Current per-token overhead: 76.97ms
Target overhead: 10.0ms
Gap: 66.97ms (87.0%)

Optimization opportunities (ranked by impact):
  1. Child loading (disk I/O)        52.02ms ( 67.6%)
  2. Initial detection               22.68ms ( 29.5%)
  3. Child detection                  1.86ms (  2.4%)
  4. Cache management                 0.33ms (  0.4%)
  5. Python overhead                  0.08ms (  0.1%)

Recommendations:
  • Child loading: Implement caching/preloading (2.02ms per child)
    - Consider LRU cache for frequently accessed children
    - Or reduce base layers further (currently [0,1])
