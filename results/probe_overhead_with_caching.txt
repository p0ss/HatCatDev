`torch_dtype` is deprecated! Use `dtype` instead!
================================================================================
DETAILED PROBE OVERHEAD ANALYSIS
================================================================================

Configuration:
  Model: google/gemma-3-4b-pt
  Device: cuda
  Prompt: "Artificial intelligence can help society by"

Loading model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.68it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]
✓ Model loaded

--------------------------------------------------------------------------------
INITIALIZING PROBE MANAGER (base_layers=[0,1])
--------------------------------------------------------------------------------
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v3 v3.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v2 v2.2.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation']
✓ Loaded probe pack: gemma-3-4b-pt_sumo-wordnet-v1 v1.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation', 'text']
Using probe pack: gemma-3-4b-pt_sumo-wordnet-v3

✓ Loaded metadata for 5668 concepts across 7 layers
  Parent-child relationships: 1275

Initializing DynamicProbeManager...
  Base layers: [0, 1]
  Load threshold: 0.3
  Max probes in memory: 500
  Inferred hidden_dim: 2560
  Preallocating model pool (100 models)...
✓ Base layers loaded: 267 probes
✓ Loaded 267 base probes

Extracting hidden state...

================================================================================
1. SINGLE TOKEN DETAILED BREAKDOWN (top_k=10)
================================================================================

Total time: 231.89ms

Breakdown:
  Initial detection:  24.74ms (10.7%)
    └─ 184 base probes @ 0.134ms/probe
  Child loading:      201.34ms (86.8%)
    └─ 84 children loaded
    └─ 2.40ms per child
  Child detection:    5.57ms (2.4%)
    └─ 0.066ms/probe
  Cache management:   0.11ms (0.0%)
  Python overhead:    0.12ms (0.1%) [estimated]

Results:
  Concepts detected: 9
  Total probes run: 268

================================================================================
2. BASE PROBE INFERENCE MICROBENCHMARK
================================================================================

Running 100 iterations of 268 probes...

Results:
  Average total time: 17.53ms
  Per-probe time:     0.0654ms
  Num probes:         268

================================================================================
3. PYTHON OVERHEAD MICROBENCHMARK
================================================================================

Running 1000 iterations with 268 concepts...

Results:
  Sorting (top-k):    0.0143ms
  Dict lookups:       0.0004ms
  List operations:    0.0012ms
  Total Python:       0.0159ms

================================================================================
4. MULTI-TOKEN AVERAGE (10 tokens)
================================================================================

Generation: 215.66ms (21.57ms/token)
Hidden extraction: 0.22ms (0.02ms/token)

Probe detection: 378.22ms (37.82ms/token)
  Initial detection:  192.71ms (19.27ms/token)
  Child loading:      158.70ms (15.87ms/token)
  Child detection:    25.48ms (2.55ms/token)
  Cache management:   0.69ms (0.07ms/token)
  Python overhead:    0.64ms (0.06ms/token) [estimated]

Total children loaded: 342 (34.2/token)

================================================================================
SUMMARY & OPTIMIZATION TARGETS
================================================================================

Current per-token overhead: 37.82ms
Target overhead: 10.0ms
Gap: 27.82ms (73.6%)

Optimization opportunities (ranked by impact):
  1. Initial detection               19.27ms ( 51.0%)
  2. Child loading (disk I/O)        15.87ms ( 42.0%)
  3. Child detection                  2.55ms (  6.7%)
  4. Cache management                 0.07ms (  0.2%)
  5. Python overhead                  0.06ms (  0.2%)

Recommendations:
  • Initial detection: Batching could help (currently 71.109ms/probe)
    - Run all 271 probes in single batch instead of loop
  • Child loading: Implement caching/preloading (0.46ms per child)
    - Consider LRU cache for frequently accessed children
    - Or reduce base layers further (currently [0,1])
