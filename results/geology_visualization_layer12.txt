`torch_dtype` is deprecated! Use `dtype` instead!
ðŸ”¬ Token Timeline Visualization Demo
================================================================================
Loading model and lenses...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.52s/it]
âœ“ Loaded lens pack: gemma-3-4b-pt_sumo-wordnet-v1 v1.0.0
  Model: google/gemma-3-4b-pt
  Concept pack: sumo-wordnet-v1 v1.0.0
  Layers: [0, 1, 2, 3, 4, 5], Types: ['activation', 'text']
Using lens pack: gemma-3-4b-pt_sumo-wordnet-v1

âœ“ Loaded metadata for 121512 concepts across 7 layers
  Parent-child relationships: 660

Initializing DynamicLensManager...
  Base layers: [1, 2]
  Load threshold: 0.5
  Max lenses in memory: 1500
  Inferred hidden_dim: 2560
  Preallocating model pool (100 models)...
âœ“ Base layers loaded: 1335 lenses

Prompt: Mountains are formed when

Capturing prompt processing...
Traceback (most recent call last):
  File "/home/poss/Documents/Code/HatCat/scripts/visualize_token_timeline.py", line 379, in <module>
    main()
  File "/home/poss/Documents/Code/HatCat/scripts/visualize_token_timeline.py", line 371, in main
    output_text, timeline = generate_with_monitoring(prompt, max_tokens=20)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/poss/Documents/Code/HatCat/scripts/visualize_token_timeline.py", line 139, in generate_with_monitoring
    detected, _ = manager.detect_and_expand(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/poss/Documents/Code/HatCat/src/monitoring/dynamic_lens_manager.py", line 584, in detect_and_expand
    self._load_concepts(child_keys_to_load, reason="dynamic_expansion")
  File "/home/poss/Documents/Code/HatCat/src/monitoring/dynamic_lens_manager.py", line 412, in _load_concepts
    lens = SimpleMLP(self.hidden_dim).to(self.device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/poss/Documents/Code/HatCat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/poss/Documents/Code/HatCat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/poss/Documents/Code/HatCat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/poss/Documents/Code/HatCat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/poss/Documents/Code/HatCat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 303.06 MiB is free. Process 1784 has 74.66 MiB memory in use. Process 1781 has 66.66 MiB memory in use. Process 686646 has 18.66 MiB memory in use. Process 803272 has 102.66 MiB memory in use. Process 835469 has 74.66 MiB memory in use. Process 872823 has 62.66 MiB memory in use. Process 873371 has 66.66 MiB memory in use. Process 874144 has 82.66 MiB memory in use. Process 895770 has 66.66 MiB memory in use. Process 913975 has 86.66 MiB memory in use. Process 972504 has 86.65 MiB memory in use. Process 972905 has 42.66 MiB memory in use. Process 975166 has 58.66 MiB memory in use. Including non-PyTorch memory, this process has 18.21 GiB memory in use. Of the allocated memory 17.90 GiB is allocated by PyTorch, and 1.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
