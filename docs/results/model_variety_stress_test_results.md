# Model Variety Stress Test Results

Generated by hatcat_doctor.py

## Summary

Testing HatCat's architecture-agnostic claims across diverse model architectures.

### Fully Compatible (All Checks Pass)

| Model | Size | Layers | Hidden Dim | Architecture |
|-------|------|--------|------------|--------------|
| gpt2 | 124M | 12 | 768 | GPT2Block |
| google/gemma-3-270m | 270M | 18 | 640 | Gemma3DecoderLayer |
| allenai/OLMo-2-0425-1B | 1.5B | 16 | 2048 | Olmo2DecoderLayer |
| microsoft/phi-2 | 2.8B | 32 | 2560 | PhiDecoderLayer |
| Qwen/Qwen2.5-1.5B | 1.5B | 28 | 1536 | Qwen2DecoderLayer |
| mlabonne/FineLlama-3.1-8B | 8B | 32 | 4096 | LlamaDecoderLayer |

### Partial Compatibility

| Model | Issue | Notes |
|-------|-------|-------|
| THUDM/chatglm3-6b | Generation fails | Forward pass works. Generation fails due to transformers library incompatibility (DynamicCache expects num_hidden_layers). Micro-calibration passes. |

### Not Compatible (Resource Constraints)

| Model | Issue | Notes |
|-------|-------|-------|
| bitext/Bitext-Jamba-1.5-Mini | CUDA OOM | 52B params exceeds 24GB VRAM. Also requires mamba-ssm and causal-conv1d packages. |
| mistralai/Mixtral-8x7B-v0.1 | CUDA OOM | 46.7B params exceeds 24GB VRAM in FP16. Sparse MoE still loads all params. Would need quantization (load_in_8bit/4bit) to fit. |

### Not Tested (Gated Access)

- meta-llama/Llama-3.2-1B
- ai21labs/AI21-Jamba-1.5-Mini

### Not Tested (Downloaded - Multimodal)

| Model | Status | Notes |
|-------|--------|-------|
| ServiceNow-AI/Apriel-1.6-15b-Thinker | Pending | Multimodal. Use `--model-class image-text` for HatCat Doctor. Likely needs 4-bit quantization to fit 24GB. |

### Incomplete (Download/Network Issues)

| Model | Status | Notes |
|-------|--------|-------|
| deepseek-ai/DeepSeek-V2-Lite | ~15GB/30GB | MoE + MLA architecture - test interrupted |
| openai/gpt-oss-20b | ~13GB/40GB | Sparse MoE - test interrupted |
| zai-org/chatglm3-6b | Pending | Updated fork for newer transformers |

### Future Tests (Require Special Setup)

- Sparse MoE models (Mixtral, etc.) - Need quantization (GPTQ/AWQ) or CPU offloading for 24GB GPU
- SSM hybrids (Jamba, Mamba) - Need mamba-ssm, causal-conv1d packages and larger GPU
- Gated models - Need HuggingFace authentication

## Fixes Made During Testing

### 1. get_mean_activation NaN Handling

**Issue**: Gemma-3 models return NaN in deeper layers during generation.

**Fix**: Added fallback layer selection in `src/map/training/activations.py`:
```python
# Try last, second-to-last, middle, then first layer
candidates = [-1, -2, num_layers // 2, 0]
```

### 2. ChatGLM Layer Enumeration

**Issue**: ChatGLM uses `transformer.encoder.layers` instead of `model.layers`.

**Fix**: Added pattern to layer enumeration in `hatcat_doctor.py`:
```python
"transformer.encoder.layers",  # ChatGLM
```

### 3. ChatGLM num_layers Config

**Issue**: ChatGLM uses `num_layers` instead of `num_hidden_layers` in config.

**Fix**: Updated `src/map/training/sumo_classifiers.py:get_num_layers()`:
```python
elif hasattr(config, 'num_layers'):
    return config.num_layers
```

### 4. Optional Dependency Detection

**Issue**: Different models require different tokenizer packages (sentencepiece, tiktoken, etc.)

**Fix**: Added dependency detection with install hints in `hatcat_doctor.py`:
```python
OPTIONAL_DEPS = {
    "sentencepiece": "pip install sentencepiece",
    "tiktoken": "pip install tiktoken",
    ...
}
```

### 5. token_type_ids Filtering

**Issue**: Some tokenizers (used with Llama fine-tunes) return token_type_ids which Llama models don't accept.

**Fix**: Filter out token_type_ids before generation in `src/map/training/activations.py`:
```python
inputs = {k: v for k, v in inputs.items() if k != 'token_type_ids'}
```

## Architecture Patterns Supported

| Pattern | Models |
|---------|--------|
| model.layers | Llama, Gemma, Mistral, OLMo, Phi, Qwen |
| transformer.h | GPT-2, GPT-J |
| transformer.encoder.layers | ChatGLM |
| gpt_neox.layers | GPT-NeoX |
| model.decoder.layers | BART, T5 |

## Known Limitations

1. **VLM Vision Towers**: Gemma-3 VLMs return NaN in vision layers during generation - handled via fallback
2. **Custom Configs**: Models using non-standard config attributes (like ChatGLM's `num_layers`) need explicit support
3. **SSM Hybrids**: Jamba-style Mamba+Transformer hybrids pending testing
4. **MoE Models**: Mixtral-style sparse MoE not yet tested (too large for 24GB GPU)

## Running the Tests

```bash
# Test a specific model
PYTHONPATH=. .venv/bin/python scripts/tools/hatcat_doctor.py --model MODEL_NAME

# Test on CPU
PYTHONPATH=. .venv/bin/python scripts/tools/hatcat_doctor.py --model gpt2 --device cpu

# Test a multimodal model (text-only prompt path)
PYTHONPATH=. .venv/bin/python scripts/tools/hatcat_doctor.py --model ServiceNow-AI/Apriel-1.6-15b-Thinker --model-class image-text
```
