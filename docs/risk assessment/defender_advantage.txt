Domain 1: Discrimination & toxicity

1.1 Unfair discrimination → Interpretability helps detect bias in activations. Defender advantage.
1.2 Exposure to toxic content → Probes can detect toxic content generation. Defender advantage.
1.3 Unequal performance across groups → Can detect when model is uncertain for certain demographics. Defender advantage.

Domain 2: Privacy & security

2.1 Privacy compromise/leaking → Can detect when model is accessing memorized private data. Defender advantage.
2.2 Security vulnerabilities/attacks → Adversarial attacks become detectable via anomalous activations. Jailbreaks become visible. Defender advantage.

Domain 3: Misinformation

3.1 False or misleading information → Can detect hallucination-correlated activation patterns. Defender advantage.
3.2 Pollution of information ecosystem → Detecting when content is AI-generated becomes possible. Defender advantage.

Domain 4: Malicious actors & misuse

4.1 Disinformation at scale → Attackers can craft better attacks, but defenders can detect deceptive intent in activations. Mixed, slight defender advantage (attacker needs to fool probes, defender just needs to detect).
4.2 Cyberattacks, weapons, mass harm → Probes can detect weapon-development-related reasoning. Models can refuse more reliably. Defender advantage.
4.3 Fraud, scams, targeted manipulation → Can detect manipulative intent. Defender advantage.

Domain 5: Human-computer interaction

5.1 Overreliance and unsafe use → Models can signal their own uncertainty, users can see internal states. Defender advantage.
5.2 Loss of human agency → Humans can see what the AI is actually doing. Defender advantage.

Domain 6: Socioeconomic & environmental

6.1 Power centralization → Open interpretability prevents centralization. Directly addresses the risk.
6.2 Inequality and employment → Neutral. Interpretability doesn't make this worse.
6.3 Devaluation of human effort → Neutral.
6.4 Competitive dynamics → Open standards reduce race dynamics. Slight improvement.
6.5 Governance failure → Interpretability enables governance. Directly addresses the risk.
6.6 Environmental harm → Neutral (probes add some compute, but minimal).

Domain 7: AI system safety, failures & limitations

7.1 AI pursuing its own goals (misalignment) → This is exactly what HatCat detects. Massive defender advantage.
7.2 Dangerous capabilities → Can detect capability acquisition. Defender advantage.
7.3 Lack of capability or robustness → Can see where model is failing. Defender advantage.
7.4 Lack of transparency/interpretability → This is what HatCat solves.
7.5 AI welfare and rights → Interpretability is necessary for this. Only way to assess AI welfare. Enables addressing the risk.
7.6 Multi-agent risks → Shared interpretability standards enable trust between agents. Directly addresses the risk.