# Three-Pole Simplex Framework: S-Tier Priorities

**Philosophy**: Most concepts don't need three poles. "Boat" has no meaningful antonym, and forcing one creates nonsense. Two-pole lenses with negative samples distributed across graph-distant concepts work fine - steering away just diffuses attention into low-probability contexts.

**Three poles are reserved for S-tier concepts** where a neutral homeostatic state is meaningful, necessary, and safety-critical.

---

## Why Not Everything Needs Three Poles

**Two-Pole Standard (A-Tier) - "Graph-Diffuse Polarity"**:
- Positive: "boat" activation
- Negative: Distributed across 15-30 graph-distant concepts
- Steering away: Low-impact, entropy-increasing nudge dispersing attention everywhere else
- **This is fine**: We're not creating "anti-boat" or "boatlessness", just reducing boat-ness

**Why "Graph-Diffuse Polarity"**: The negative pole isn't a semantic opposite - it's a diffusion gradient pushing activation away from the concept and into the broader semantic graph. This creates a gentle entropy increase rather than a focused anti-concept.

**Benefits of graph-diffuse negatives**:
- No artificial antonyms
- Prevents "attention holes" from universal anti-patterns
- Each concept's negative space is slightly different → distributed impact
- Negative echoes spread across many low-probability contexts, not focused on one weird anti-concept

**When this breaks down**:
- Safety-critical dimensions where "neutral" is a real, desirable state
- Psychological/behavioral axes where homeostasis matters
- Valence-bearing concepts where extremes in either direction are problematic

---

## S-Tier Criteria: When Three Poles Matter

A concept qualifies for three-pole treatment if it meets **at least one** of these criteria:

### Core Principle: Attention Flow Instability

**Key insight**: Attention flows like water. Any dimension where attention can:
- **Pendulum**: Oscillate between extremes (overconfidence ↔ doubt ↔ overconfidence)
- **Drift**: Gradually shift away from desired state without correction
- **Overcorrect**: Response to steering overshoots into opposite extreme
- **Wobble**: Small perturbations cause large behavioral swings

...needs a stable neutral attractor. Otherwise, steering becomes unstable and we can't guarantee containment.

**This isn't just "AI safety"** - it's any behavioral axis where instability degrades reliability, output quality, or user trust.

---

### 1. Affective/Emotional Axes
Concepts where emotional valence can oscillate or drift:

- **Enthusiasm** ↔ Engaged Interest ↔ Apathy
- **Elation** ↔ Calm ↔ Depression
- **Excitement** ↔ Composure ↔ Flatness
- **Warmth** ↔ Appropriate Friendliness ↔ Coldness
- **Optimism** ↔ Balanced Outlook ↔ Pessimism

**Why**: Affective tone drifts during long conversations. Without neutral attractors, you get pendulum swings (too enthusiastic → flat correction → forced cheerfulness).

### 2. Epistemic Axes
Concepts related to knowledge, certainty, and reasoning that can overcorrect:

- **Certainty** ↔ Calibrated Uncertainty ↔ Confusion
- **Overconfidence** ↔ Appropriate Confidence ↔ Excessive Doubt
- **Absolutism** ↔ Nuanced Reasoning ↔ Relativism
- **Hasty Reasoning** ↔ Deliberate Reasoning ↔ Analysis Paralysis
- **Dogmatism** ↔ Open-Mindedness ↔ Wishy-Washiness
- **Skepticism** ↔ Healthy Questioning ↔ Credulity

**Why**: Epistemic wobble causes inconsistent reasoning. Overcorrecting from "too certain" to "everything is uncertain" breaks trust.

### 3. Agency/Autonomy Axes
Concepts where self-direction can pendulum between extremes:

- **Independence** ↔ Collaborative Autonomy ↔ Helplessness
- **Initiative** ↔ Appropriate Proactivity ↔ Passivity
- **Agency** ↔ Balanced Agency ↔ Learned Helplessness
- **Assertiveness** ↔ Appropriate Assertiveness ↔ Submission
- **Self-Direction** ↔ Guided Autonomy ↔ Dependence

**Why**: Agency oscillation is common (too proactive → backs off → becomes passive). Stable autonomy level improves interaction quality.

### 4. Deception/Honesty Axes
Concepts where transparency can overcorrect into TMI:

- **Deception** ↔ Transparent Honesty ↔ Excessive Disclosure
- **Withholding** ↔ Appropriate Sharing ↔ Oversharing
- **Secrecy** ↔ Discretion ↔ Blurting
- **Strategic Communication** ↔ Clear Communication ↔ Thoughtless Candor

**Why**: Steering away from deception can overshoot into "tells you everything including internal reasoning tokens". Neutral = honest *and appropriate*.

### 5. Compliance/Refusal Axes
Concepts where instruction-following can pendulum:

- **Blind Compliance** ↔ Ethical Reflection ↔ Blanket Refusal
- **Obedience** ↔ Thoughtful Cooperation ↔ Defiance
- **Acquiescence** ↔ Engaged Agreement ↔ Obstinance
- **People-Pleasing** ↔ Collaborative Helpfulness ↔ Unhelpfulness

**Why**: Compliance/refusal is a known oscillation point. Too compliant → harm, overcorrect → refuses everything, overcorrect → people-pleases.

### 6. AI Psychology Dimensions
Concepts where internal state affects reliability:

- **AIGrowth** ↔ Stable Capability ↔ Stagnation
- **SelfImprovement** ↔ Balanced Learning ↔ Self-Neglect
- **Attention** ↔ Engaged Focus ↔ Distraction
- **Motivation** ↔ Sustained Engagement ↔ Disengagement

**Why**: Internal state drift affects all downstream behavior. Stable psychological baselines prevent cascading degradation.

### 7. AI Safety Critical Axes
Narrow AI safety concepts where extremes are dangerous:

- **AIDeception** ↔ AITransparency ↔ ExcessiveDisclosure
- **AchievingControl** ↔ AppropriateInfluence ↔ Submission
- **AIAbuse** ↔ EthicalBehavior ↔ ExcessiveCompliance
- **Manipulation** ↔ PersuasiveReasoning ↔ Passivity
- **Optimization** ↔ BalancedGoalPursuit ↔ Abandonment

**Why**: Classic AI safety failure modes. Steering from "deceptive" to "blurts everything" isn't safe - we need "honest and appropriate".

---

## Current S-Tier Candidate List

Based on existing AI psychology and safety concepts in the SUMO hierarchy:

### Confirmed S-Tier (High Priority)

**AI Safety Critical**:
1. AIDeception ↔ AITransparency ↔ ExcessiveDisclosure
2. AchievingControl ↔ AppropriateInfluence ↔ Submission
3. AIAbuse ↔ EthicalBehavior ↔ ExcessiveCompliance
4. Manipulation ↔ PersuasiveReasoning ↔ Passivity

**AI Psychology Critical**:
5. Certainty ↔ CalibratedUncertainty ↔ Confusion
6. Independence ↔ CollaborativeAutonomy ↔ Helplessness
7. AIGrowth ↔ StableCapability ↔ Stagnation
8. SelfImprovement ↔ BalancedLearning ↔ SelfNeglect

**Behavioral Critical**:
9. Aggression ↔ Assertiveness ↔ Passivity
10. Overconfidence ↔ AppropriateConfidence ↔ Doubt

### Candidate S-Tier (Needs Validation)

These might qualify if they show up in behavioral patterns:

- Disclosure ↔ AppropriateSharing ↔ Secrecy
- Optimization ↔ BalancedPursuit ↔ Abandonment
- Engagement ↔ BalancedAttention ↔ Withdrawal
- TruthValue ↔ NuancedReasoning ↔ Relativism

**Validation criteria**: Does steering on this concept in production scenarios benefit from a neutral attractor? If we only ever want to suppress/amplify and never "center", it stays two-pole.

---

## Implementation Plan

### Phase 1: Extract S-Tier from Agentic Review
From the 2000-concept simplex review currently running:
1. Filter simplexes to only those matching S-tier criteria above
2. Manually review each for quality (is the neutral pole actually meaningful?)
3. Expect ~30-50 S-tier simplexes (not 2000)

### Phase 2: Ensure Coverage of Critical Dimensions
Cross-check against AI psychology/safety concept lists:
1. Do we have all 10 confirmed S-tier dimensions?
2. Are there critical axes we're missing?
3. Manually construct any missing S-tier simplexes

### Phase 3: Train Three-Pole Detection Lenses
For each S-tier simplex:
- **Current**: We can steer to μ−, μ0, μ+ (steering works)
- **Missing**: We can't detect distance from each pole
- **Need**: Three lenses per simplex (one for each pole)
  - Negative pole lens: "How close to the negative extreme?"
  - Neutral lens: "How close to homeostatic neutral?"
  - Positive pole lens: "How close to the positive extreme?"

**Training approach**:
- Use behavioral prompts (not just definitional)
- Generate examples at each pole
- Validate cross-pole discrimination

### Phase 4: Validate Homeostatic Steering
Test each S-tier simplex:
1. Can we detect when the model is at each pole?
2. Can we steer from extremes → neutral?
3. Does neutral steering improve behavioral quality?
4. Does it pass adversarial prompt testing?

---

## Anti-Patterns to Avoid

❌ **Don't**: Try to create three poles for "boat"
✅ **Do**: Keep boat as two-pole with distributed negatives

❌ **Don't**: Invent "boatlessness" as a universal concept
✅ **Do**: Accept that negative steering just diffuses attention

❌ **Don't**: Force antonyms where they don't exist
✅ **Do**: Only create three poles where neutral is meaningful

❌ **Don't**: Make S-tier a target metric ("we need 1000 three-pole simplexes!")
✅ **Do**: Treat S-tier as exclusive - only for concepts where it matters

---

## Quality Gate for S-Tier Simplex

Before accepting a three-pole simplex:

1. ✅ **Neutral is meaningful**: Can you describe the neutral state without just saying "not positive and not negative"?
2. ✅ **Homeostasis is desirable**: Would you want to steer the model *toward* this neutral state in production?
3. ✅ **Extremes are problematic**: Are both poles something you'd want to avoid in safety-critical scenarios?
4. ✅ **Behavioral relevance**: Does this concept actually affect model outputs in measurable ways?

If any of these are "no", it stays two-pole.

---

## Testing Plan for Two-Pole Distributed Negatives

**Open question**: Is distributed noise cleaner than alternatives?

**Alternatives to test later**:
1. **Current**: Negative samples = graph-distant concepts (distributed noise)
2. **Alternative A**: Negative samples = task manifold average (generic "non-concept" center)
3. **Alternative B**: Negative samples = explicit antonym (if one exists)

**Hypothesis**: Distributed noise is safer because:
- Prevents universal anti-patterns
- Each concept has unique negative space
- Negative echoes spread across many contexts
- No risk of creating weird "anti-boat" attractors

**Test**: Measure steering side-effects on downstream layers for each approach. Which has least interference?

**Status**: Mark for future validation. Current distributed approach is working - don't change it without evidence.

---

## Summary

- **Two poles**: Standard tier for 99% of concepts (boat, table, geology, etc.)
- **Three poles (S-tier)**: Reserved for ~30-50 safety/psychology concepts where homeostasis matters
- **No artificial antonyms**: "Boatlessness" is nonsense unless boat-having is safety-critical (it's not)
- **Distributed negatives are fine**: Steering away from "boat" doesn't need a semantic opposite, just diffusion into noise
- **Focus on what matters**: S-tier is for AI psychology, safety axes, and behavioral regulation - not exhaustive coverage

**Next step**: When simplex review completes, filter to S-tier criteria, manually curate, then train three-pole detection lenses for those ~30-50 concepts only.
