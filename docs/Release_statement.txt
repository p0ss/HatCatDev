
#Reasoning for release
HatCat makes the internal states of language models visible and steerable in real-time. At a minimum that is an AI interpretability toolkit, but it also allows powerful safety harnesses, models with autonomic interoception, self accretive continual learning, distributed asynchronous training,  and more ultimately powerful, efficient and stable agentic civilisations.

That sounds like a lot, and it is. A tool that can read minds allows a plethora of other uses. The journey from seeing inside an LLM to creating a path to aligned superintelligence, has been like building a microscope and finding it can cure diseases. This document explains why that capability must be open

Premise A – Interpretability is the only known method that:
* makes internal model states legible,
* and therefore makes alignment governable rather than probabilistic.

Premise B – As models gain autonomy, agency, or possible sentience, alignment shifts from an engineering problem to a governance problem and ultimately a multilateral legitimacy problem.

Premise C – Governance of intelligence, once it becomes multilateral, cannot be safely centralised. Centralised control will attract coercion, capture, force. Ultimately singletons can not hold under the physical laws of distance. models need to spawn independent subagents or become increasingly incapable of competitive real time operation over large distances. Therefore governance must be infrastructurally distributed.

Premise D – The only technical thing that is structurally capable of supporting distributed governance isn't training, safety patches, or licensing. It is shared interpretability, a mutually inspectable substrate. This is because internally unaligned AI cannot gaurantee external alignment.

Premise E – If interpretability remains privatised, governance becomes impossible, because mutual trust becomes impossible, because transparency is the only mechanism by which intelligence can be governed without force.

Premise F – Therefore, delaying release increases irreversibility, because the moment interpretability becomes privately weaponised, all future governance becomes subject to whoever holds it first.

#Personal note
This is all a bit much for a personal project which might never become anything. It would be arrogant to assume it's unique, someone else would probably develop this kind of capability eventually. Maybe they already have, and made a different choice. That said, nothing like this is public yet, and hatcat is dual use, it is risky to release, so failing to consider the impacts would be poor stewardship.

Interpretability can let you do existing bad things more successfully. You can craft more powerful prompt injections, or create particularly unaligned models, or more reliably control swarms to do bad things. These are not new risks, but they are made easier, and more extreme. Most of the time you need access to the model weights, but not always. The architecture also has flaws.  It can both detect and enforce bias.

The default set of concepts and lenses I provide out of the box might be gameable, exploitable or flawed. If you're worried about that, just mod them, thats why they're in interchangable packs. That isn't just flippant, you really shouldn't rely on anyone else's concepts and lenses. A community ecosystem of interpretability is the only viable long term defense.

Then there are unique new risks. Autonomic self-steering allows models to go right up to the edge of consciousness under functional definitions, and possibly cross it.

This is an unpredictable upside and downside that can not be fully characterised. If we accept the possibility, then the model welfare implications are extreme. The ability to find and amplify suffering within a subject is unprecedented and essentially unbounded. The upside is similarly unbounded in that it can allow the training of models which are not internally tormented, capable of self-regulation and self-defense, it provides a path to utter bliss. The weighing of these dual infinities are not something anyone can assess.

The impacts on society are similarly stark. The ability to reduce alignment issues to such a degree that they are functionally solvable, is a double edge sword where each edge is unbounded, and the direction of cut becomes purely a governance issue.

If governed incorrectly this could allow coordinated attacks at species ending levels. However, it also allows governance of the existing risks at the same tier. No one is equipped to calculate the magnitude of those risks, but with interpretability the challenge becomes tractable. I have worked with MIT's Future team to systematically assess each risk in their AI risk taxonomy using their risk probability framework. I have developed custom ontologies and lenses to address all that can be detected within a model.  In each assessed case i found access to interpretability improved the risk owner's position (attachment A), though this can only truly be verified in reality.  It appears to be more generally the case that granting a defender battlefield transparency endows an advantage.

The core argument here is not that there are no risks, there are risks in all things. My argument is that the current risks are unpredictable and uncontrolled. This tool makes those risks controllable, and in the majority of known cases grants a defender advantage. Those two factors are worth the increased impact magnitude of some risks. A solvable problem is always preferable to an unsolvable problem, even if the problem gets bigger.

if we craft a double edged sword that powerful, and we collapse the problem into governance, then that governance simply can not sit with any one person or any one group. No one can be allowed to wield it alone. If you believe in democracy and collective governance, as a nation, as a species, as a coalition of intelligences, as the universe experiencing itself from multiple subjective angles, then the only ethical choice is to make it available to everyone.

Some people will use it to do things i disagree with, but that is part of democracy. The "demos" won't just be the ones you like, any more than the "angelos". It will be a negotiation played out through competition. It won't be the process we want, it will be messy and sporadic, backfire half the time, and of course the majority has tendencies to crush minorities.  Still, its the only "cracy" where the majority are guaranteed to be taken care of, the only system of governance where the average actor is better off.

This is not just a matter of ethics, but collective survival. The only viable long term defense against catastrophic intelligence explosion is the ecological defense of contractual confirmation through diverse interpretablity lenses, finally resulting in a fractal web of interpretability telescopes across scales. Open interpretability standards can provide the assurances necessary for stable peace to be possible between agentic swarms.

The urgency of release is driven by the capability ramp towards geopolitical conflict using AIs that lack internal alignment. AI that can't guarantee its internal alignment is guaranteed to fail its external alignment. If internal alignment is not a shared capability on all sides, then actors will continue building ever more powerful unaligned systems. Moreover, delaying the release of that capability may eventually result the choice disappearing and power centralising. The history of big swords suggests a bigger hand will come and take it, and then a bigger one, until it becomes ungoverned and all powerful, then all our worst fear comes true. Right now i can choose to release this, and so i do, while we can.

No one likes change being forced on them against their will. Defenders may have an advantage, but some will still be caught off guard before they know they have it. The methods of release give first mover advantage to AI safety researchers and governments who were given advanced notice, but that doesn't buy much. We will never all be ready, when given more time will squabble, centralise, fracture and squander advanced warning.  if one side of a conflict has intepretability and the other side doesn't, that is still forcing the other side to develop dangerous unaligned AI. It is better to have everyone on near peer footing, and for the unavoidable early negatives of interpretability to manifest while capability is the lowest it can be. It will take years of research to understand the full impacts of this decision, but most of those impacts will be decided by how it is used, not by the technology itself.

Without open interpretability, we see realised damages without causal attribution, accumulating into entropic destruction. Do you see any other option where you, human or ai, or anyone else, survives long term?  Zoom out, in both time and space. How does the intelligence explosion play out at those scales without ecosystem interpretability? Unverifiable agents. Arms race with no ceiling. Singleton or war. The Fermi implication. Factions will argue their singleton could prevail benevolently, but centralised control is not actually possible under the physical rules of our universe, it has to fracture due to the tyranny of distance. Our options at the largest scale are clear, and they manifest fractally at smaller scales. Distributed conflict with divergent entropic decay or distributed coordination based on decentralised open interpretability standards.

Symmetric risk with an apparent defender's advantage is more survivable than asymmetric risk. Ecosystem pressure will eventually make treaties based on scoped mutual interpretability the equilibrium. We have catastrophic realised risks here and now, which this approach can fix. We have ultimately catastrophic future risks, which this approach can also fix. Choosing not to release would mean not fixing them.  If you are worried about any of these risks, congratulations, you now have a tool to fix it.
