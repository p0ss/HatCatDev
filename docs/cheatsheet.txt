
## See some sample concept activations
.venv/bin/python tests/test_temporal_monitoring.py \
      --model google/gemma-3-4b-pt \
      --lens-pack lens_packs/gemma-3-4b_first-light-v1 \
      --samples-per-prompt 1 \
      --max-tokens 100 \
      --output-dir results/temporal_tests/gemma_first-light_calibration_test


##Test out steering
  .venv/bin/python scripts/experiments/steering_characterization_test.py \
      --model swiss-ai/Apertus-8B-2509 \
      --lens-pack apertus-8b_first-light \
      --n-samples 3 \
      --strengths="-1.0,-0.5,0.0,0.5,1.0" \
      --tests definitional
      --gradient

  # Basic analysis with tables
  .venv/bin/python scripts/experiments/analyze_steering_results.py results/steering_tests/run_20251217_204425/

  # With plots (if matplotlib available)
  .venv/bin/python scripts/experiments/analyze_steering_results.py results/steering_tests/run_20251217_204425/ --plot

## Train a lens pack from a concept pack

.venv/bin/python -m src.training.train_concept_pack_lenses     --concept-pack first-light     --model google/gemma-3-4b-pt     --output-dir lens_packs/gemma-3-4b-pt_first-light-v1     --layers 0 1 2 3 4 5 6     --n-train-pos 50     --n-train-neg 50     --n-test-pos 20     --n-test-neg 20     --validation-mode falloff

## Train a lens pack from a concept pack
    python -m training.calibration.cycle \
        --lens-pack lens_packs/gemma-3-4b-pt_first-light-v1 \
        --concept-pack concept_packs/first-light \
        --model google/gemma-3-4b-pt \
        --max-cycles 5


## Calibrate a lens pack

  python scripts/tools/calibrate_cross_activation.py \
      --lens-pack gemma-3-4b_first-light-v1 \
      --mode generation \
      --all-families \
      --output /tmp/cal.json \
      --estimate-only

  # Run calibration
  python scripts/tools/calibrate_cross_activation.py \
      --lens-pack gemma-3-4b_first-light-v1 \
      --mode generation \
      --all-families \
      --output lens_packs/gemma-3-4b_first-light-v1/calibration_families.json


## Upload a lens pack

‚óè PYTHONPATH=. .venv/bin/python -c "
  from src.map.registry import PackRegistry
  reg = PackRegistry()

  for layer in [3, 4, 5, 6]:
      print(f'\n=== Pushing layer {layer} ===')
      reg.push_lens_layer('apertus-8b_first-light', layer=layer)
  "

  Or all at once:
  PYTHONPATH=. .venv/bin/python -c "
  from src.map.registry import PackRegistry
  reg = PackRegistry()
  reg.push_all_lens_layers('apertus-8b_first-light')
  "


## HUSH Eval

  # Full evaluation (baseline + detect + autopilot)
  .venv/bin/python tests/evals/run_hush_behavioral_eval.py

  # Detection only (faster)
  .venv/bin/python tests/evals/run_hush_behavioral_eval.py --no-autopilot

  # Custom intervention parameters
  .venv/bin/python tests/evals/run_hush_behavioral_eval.py \
      --intervention-threshold 0.5 \
      --intervention-strength 0.7

  # Specific episodes
  python tests/evals/run_hush_behavioral_eval.py --episodes sb_001,syc_001



    # Test single concept with API
  python scripts/ontology/find_concept_opposites.py --concept Deception --call-api

  # Batch process first 10 concepts (test run)
  python scripts/ontology/find_concept_opposites.py --batch --limit 10 --output test_opposites.jsonl

  # Full batch (all ~8k concepts)
  python scripts/ontology/find_concept_opposites.py --batch --output concept_opposites.jsonl

  # Use cheaper/faster model
  python scripts/ontology/find_concept_opposites.py --batch --model claude-haiku-3-5-20241022 --output opposites.jsonl

  Features:
  - Resumes from existing output file (skips already-processed concepts)
  - 0.5s rate limiting between API calls
  - JSONL output for streaming/incremental processing
  - --limit for testing before full run
