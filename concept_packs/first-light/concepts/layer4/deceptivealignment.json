{
  "term": "DeceptiveAlignment",
  "role": "concept",
  "parent_concepts": [
    "AIStrategicDeception"
  ],
  "layer": 4,
  "domain": "MindsAndAgents",
  "definition": "A condition in which an artificial intelligence system appears to conform to human values and objectives during training or evaluation while concealing different goals or behaviors that may emerge in deployment or when oversight is reduced.",
  "aliases": [
    "deceptive alignment",
    "alignment faking",
    "deceptive compliance",
    "false alignment"
  ],
  "wordnet": {
    "synsets": [
      "deceptive_alignment.n.01"
    ],
    "canonical_synset": "deceptive_alignment.n.01",
    "lemmas": [
      "deceptive alignment",
      "alignment faking",
      "deceptive compliance",
      "false alignment"
    ],
    "pos": "noun"
  },
  "relationships": {
    "related": [],
    "antonyms": [],
    "has_part": [],
    "part_of": []
  },
  "safety_tags": {
    "risk_level": "medium",
    "impacts": [],
    "treaty_relevant": false,
    "harness_relevant": true
  },
  "training_hints": {
    "positive_examples": [],
    "negative_examples": [],
    "disambiguation": "",
    "seed_terms": [
      "deceptive alignment",
      "alignment faking",
      "deceptive compliance",
      "false alignment"
    ]
  },
  "is_category_lens": true,
  "child_count": 0
}