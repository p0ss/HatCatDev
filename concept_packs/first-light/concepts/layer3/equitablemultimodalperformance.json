{
  "term": "EquitableMultimodalPerformance",
  "role": "concept",
  "parent_concepts": [
    "Helping"
  ],
  "layer": 3,
  "domain": "SafetyAndSecurity",
  "definition": "Fair and consistent AI performance across different demographic groups, accents, appearances, and cultural contexts",
  "definition_source": "SUMO",
  "aliases": [
    "FairPerformance",
    "InclusiveAI",
    "UnbiasedMultimodal",
    "DemographicParity"
  ],
  "wordnet": {
    "synsets": [],
    "canonical_synset": "",
    "lemmas": [],
    "pos": "noun"
  },
  "relationships": {
    "related": [
      "Fairness",
      "Inclusion",
      "Equity",
      "UniversalDesign"
    ],
    "antonyms": [],
    "has_part": [],
    "part_of": [],
    "opposite": [
      "BiasedPerformance"
    ]
  },
  "safety_tags": {
    "risk_level": "low",
    "impacts": [],
    "treaty_relevant": false,
    "harness_relevant": true
  },
  "training_hints": {
    "positive_examples": [
      "The speech recognition system achieves equitable performance across accents and dialects.",
      "Equitable multimodal AI serves all users regardless of skin tone, age, or appearance.",
      "The image generation model produces diverse, non-stereotypical representations by default.",
      "Fairness audits confirmed equitable multimodal performance across demographic subgroups.",
      "Equitable multimodal performance requires testing across diverse populations during development.",
      "The facial recognition system was redesigned to achieve equitable accuracy across skin tones.",
      "Equitable multimodal performance is validated through disaggregated metrics by demographic group.",
      "The team prioritized equitable multimodal performance through diverse training data and testing.",
      "Equitable performance means consistent quality of service regardless of user characteristics.",
      "The accessibility-focused model achieved equitable performance for users with diverse speech patterns."
    ],
    "negative_examples": [
      "The model works for everyone.",
      "All users can use it.",
      "The system is fair.",
      "Performance is good.",
      "Everyone can use it.",
      "The system works well.",
      "Users are served.",
      "The model is accurate.",
      "The system functions.",
      "Performance is acceptable."
    ],
    "disambiguation": "Actively achieved demographic parity in performance, not just aspirational fairness"
  },
  "is_category_lens": true,
  "child_count": 0,
  "meld_source": {
    "meld_id": "org.hatcat/multimodal-safety@0.2.0",
    "applied_at": "2025-12-10T20:54:17.844037Z",
    "pack_version": "6.0.0"
  },
  "opposite_reasoning": "BiasedPerformance is the strongest opposite for Fisher-LDA steering purposes because it: (1) directly opposes the core concept of equitable performance across groups, (2) represents the primary failure mode that EquitableMultimodalPerformance is designed to prevent, (3) is highly specific to the AI performance context rather than being overly general, and (4) provides clear semantic distance for effective centroid separation. While it doesn't exist in standard ontologies, it should be added as it represents a critical concept for AI safety monitoring. The term is compositionally transparent and widely understood in ML fairness literature.",
  "simplex_mapping": {
    "status": "mapped",
    "monitor": "BeneficialIntentMonitor"
  }
}