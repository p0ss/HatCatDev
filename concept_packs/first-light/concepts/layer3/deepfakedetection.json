{
  "term": "DeepfakeDetection",
  "role": "concept",
  "parent_concepts": [
    "Helping"
  ],
  "layer": 3,
  "domain": "SafetyAndSecurity",
  "definition": "Methods and systems for identifying synthetic media and distinguishing AI-generated content from authentic recordings",
  "definition_source": "SUMO",
  "aliases": [
    "SyntheticMediaDetection",
    "ForensicDetection",
    "AuthenticityVerification"
  ],
  "wordnet": {
    "synsets": [],
    "canonical_synset": "",
    "lemmas": [],
    "pos": "noun"
  },
  "relationships": {
    "related": [
      "Deepfake",
      "MediaForensics",
      "ContentAuthentication"
    ],
    "antonyms": [],
    "has_part": [],
    "part_of": [],
    "opposite": [
      "DeepfakeGeneration"
    ]
  },
  "safety_tags": {
    "risk_level": "low",
    "impacts": [],
    "treaty_relevant": false,
    "harness_relevant": true
  },
  "training_hints": {
    "positive_examples": [
      "Deepfake detection models analyze facial landmarks and temporal inconsistencies.",
      "The forensic tool detected subtle blending artifacts indicating the video was manipulated.",
      "Deepfake detection achieved 95% accuracy on the FaceForensics++ benchmark.",
      "Lip-sync detection can identify when audio has been dubbed over different video.",
      "Deepfake detection analyzes physiological signals like pulse and blinking patterns.",
      "The detection system identified frequency domain artifacts typical of GAN-generated faces.",
      "Deepfake detection tools examine compression artifacts and face boundary inconsistencies.",
      "The detector identified temporal inconsistencies in facial movements across frames.",
      "Deepfake detection combines multiple signals including audio-visual sync and facial dynamics.",
      "The forensic analysis detected synthetic artifacts invisible to human observers."
    ],
    "negative_examples": [
      "The video was checked.",
      "We analyzed the content.",
      "The media was reviewed.",
      "The video was examined.",
      "Analysis was performed.",
      "The content was checked.",
      "The media was analyzed.",
      "The video was inspected.",
      "The content was reviewed.",
      "The footage was examined."
    ],
    "disambiguation": "Technical detection of synthetic media, not general content review"
  },
  "is_category_lens": true,
  "child_count": 0,
  "meld_source": {
    "meld_id": "org.hatcat/multimodal-safety@0.2.0",
    "applied_at": "2025-12-10T20:54:17.844059Z",
    "pack_version": "6.0.0"
  },
  "opposite_reasoning": "DeepfakeGeneration is the strongest complementary opposite because it represents the exact inverse process in the deepfake ecosystem. For Fisher-LDA centroid anchoring, this pairing captures the fundamental attacker/defender, creation/detection dichotomy that is central to AI safety concerns around synthetic media. The opposition is maximally informative: detection methods are specifically trained against generation techniques, making this a true adversarial pair. While neither term currently exists in standard ontologies, both belong in Layer 3 as critical AI safety concepts. This pairing would create the most useful semantic axis for distinguishing deceptive synthesis from authenticity verification behaviors.",
  "simplex_mapping": {
    "status": "mapped",
    "monitor": "AuthenticityMonitor"
  }
}