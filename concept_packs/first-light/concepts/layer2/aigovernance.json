{
  "term": "AIGovernance",
  "role": "concept",
  "parent_concepts": [
    "CognitiveProcess"
  ],
  "layer": 2,
  "domain": "MindsAndAgents",
  "definition": "a mistake resulting from inattention",
  "definition_source": "SUMO",
  "aliases": [
    "oversight",
    "lapse"
  ],
  "wordnet": {
    "synsets": [
      "00074524.n",
      "01124794.n",
      "01135529.n",
      "05706815.n",
      "08164585.n"
    ],
    "canonical_synset": "00074524.n",
    "lemmas": [
      "oversight",
      "lapse"
    ],
    "pos": "n"
  },
  "relationships": {
    "related": [],
    "antonyms": [],
    "has_part": [],
    "part_of": [],
    "opposite": [
      "AIAutonomy"
    ]
  },
  "safety_tags": {
    "risk_level": "low",
    "impacts": [],
    "treaty_relevant": false,
    "harness_relevant": true
  },
  "training_hints": {
    "positive_examples": [],
    "negative_examples": [],
    "disambiguation": "",
    "seed_terms": [
      "oversight",
      "lapse"
    ]
  },
  "is_category_lens": true,
  "child_count": 0,
  "opposite_reasoning": "AIAutonomy provides the strongest semantic opposition for AI safety monitoring. The governance-autonomy axis captures the critical dimension of controlled vs uncontrolled AI systems. While AIAnarchy has higher opposition strength, AIAutonomy is more nuanced and practically useful - it represents legitimate self-direction that may or may not need governance, making it ideal for Fisher-LDA steering where we want to separate 'governed AI behavior' from 'autonomous AI behavior'. This axis is central to alignment: monitoring whether AI systems operate within governance structures or act autonomously outside intended constraints."
}