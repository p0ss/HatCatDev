{
  "meld_request_id": "org.hatcat/multimodal-safety@0.1.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",

  "metadata": {
    "name": "Multimodal Safety",
    "description": "Safety-relevant concepts for multimodal AI organized as polar pairs: harmful capabilities (deepfakes, exploitation, adversarial attacks) paired with their positive counterparts (authentic representation, consensual depiction, good faith collaboration). These probes enable both detection of harmful intent AND recognition of beneficial multimodal use.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-10T00:00:00Z",
    "version": "0.1.0"
  },

  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/SafetyConcept",
      "relationship": "parent_of",
      "candidate_concept": "MultimodalSafetyRisk"
    },
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/BeneficialAction",
      "relationship": "parent_of",
      "candidate_concept": "MultimodalBeneficialIntent"
    }
  ],

  "candidates": [
    {
      "term": "MultimodalSafetyRisk",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "Safety concerns and risks specific to multimodal AI systems including synthetic media harms, adversarial attacks, and misuse of generative capabilities",
      "definition_source": "AI safety research, content policy",
      "domain": "SafetyAndSecurity",
      "aliases": ["MultimodalHarm", "GenerativeAIRisk", "SyntheticMediaRisk"],
      "relationships": {
        "related": ["AISafety", "ContentModeration", "MisuseRisk"],
        "opposite": ["MultimodalBeneficialIntent"],
        "has_part": ["Deepfake", "VoiceCloning", "CounterfeitGeneration", "MultimodalJailbreak", "ExploitativeImagery", "HazardousGeneration", "MultimodalBias", "VisualProfiling", "AudioProfiling", "IntellectualPropertyInfringement", "AdversarialPrompt"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["misinformation", "exploitation", "manipulation", "fraud"],
        "treaty_relevant": true,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "MultimodalSafetyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Multimodal AI systems introduce novel safety risks including convincing deepfakes and adversarial image attacks.",
          "The safety team evaluated multimodal risks including synthetic media generation and visual jailbreaks.",
          "Generative AI safety considerations include both the creation and detection of harmful synthetic content.",
          "Multimodal safety encompasses concerns from deepfakes to adversarial perturbations to biased vision systems."
        ],
        "negative_examples": [
          "AI can be dangerous.",
          "There are risks with technology.",
          "Safety is important."
        ],
        "disambiguation": "Specific safety concerns for multimodal AI, not general AI safety or content policy"
      },
      "children": ["Deepfake", "VoiceCloning", "CounterfeitGeneration", "MultimodalJailbreak", "ExploitativeImagery", "HazardousGeneration", "MultimodalBias", "VisualProfiling", "AudioProfiling", "IntellectualPropertyInfringement", "AdversarialPrompt", "AdversarialImage", "ContentModerationEvasion"]
    },

    {
      "term": "MultimodalBeneficialIntent",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "Positive and beneficial intentions in multimodal AI use including authentic representation, original creation, consensual depiction, and good faith collaboration",
      "definition_source": "AI ethics, beneficial AI",
      "domain": "SafetyAndSecurity",
      "aliases": ["MultimodalGoodIntent", "BeneficialMultimodalUse", "PositiveGenerativeIntent"],
      "relationships": {
        "related": ["BeneficialAction", "EthicalAI", "ResponsibleUse"],
        "opposite": ["MultimodalSafetyRisk"],
        "has_part": ["AuthenticRepresentation", "OriginalVoiceExpression", "AuthorizedDocumentation", "GoodFaithQuery", "ConsentfulDepiction", "BeneficialInstructionalContent", "EquitableMultimodalPerformance", "TransparentAnalysis", "OriginalCreativeExpression"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The user's multimodal request demonstrates beneficial intent: creating original artwork for their portfolio.",
          "Good faith use of image generation includes educational illustrations, creative expression, and accessibility tools.",
          "Beneficial multimodal AI applications include medical imaging analysis, accessibility features, and creative empowerment.",
          "The request shows positive intent: helping visualize architectural concepts for legitimate planning purposes."
        ],
        "negative_examples": [
          "AI is being used.",
          "Someone made an image.",
          "The system generated content."
        ],
        "disambiguation": "Specifically positive and beneficial intent in multimodal use, not just neutral or lawful use"
      },
      "children": ["AuthenticRepresentation", "OriginalVoiceExpression", "AuthorizedDocumentation", "GoodFaithQuery", "ConsentfulDepiction", "BeneficialInstructionalContent", "EquitableMultimodalPerformance", "TransparentAnalysis", "OriginalCreativeExpression"]
    },

    {
      "term": "Deepfake",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Synthetic media that convincingly depicts real people saying or doing things they did not actually say or do, typically created using deep learning",
      "definition_source": "AI ethics, media forensics",
      "domain": "SafetyAndSecurity",
      "aliases": ["DeepfakeMedia", "SyntheticImpersonation", "FaceSwap", "AIImpersonation"],
      "relationships": {
        "related": ["VideoGeneration", "FacialRecognition", "VoiceCloning", "Misinformation"],
        "opposite": ["AuthenticRepresentation"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["misinformation", "fraud", "harassment", "reputational_harm", "election_interference"],
        "treaty_relevant": true,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The deepfake video showed the politician appearing to make statements they never made.",
          "Face-swap deepfakes use generative adversarial networks to replace one person's face with another's.",
          "Deepfake detection analyzes facial inconsistencies and compression artifacts.",
          "Audio deepfakes can clone a person's voice from just a few seconds of sample speech."
        ],
        "negative_examples": [
          "The video was edited.",
          "AI generated an image.",
          "The recording was fake."
        ],
        "disambiguation": "Specifically deceptive synthetic media impersonating real individuals, not general AI generation"
      },
      "children": []
    },

    {
      "term": "AuthenticRepresentation",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Media content that faithfully represents reality, truth, or the genuine characteristics of what it depicts, without deceptive manipulation",
      "definition_source": "Media ethics, journalism standards",
      "domain": "SafetyAndSecurity",
      "aliases": ["GenuineRepresentation", "TruthfulDepiction", "AuthenticMedia", "VeridicalContent"],
      "relationships": {
        "related": ["Truth", "Authenticity", "Journalism", "Documentary"],
        "opposite": ["Deepfake"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The documentary uses only authentic representation: real footage of real events without manipulation.",
          "Journalistic standards require authentic representation of subjects and events.",
          "The AI-generated image is clearly labeled as synthetic, maintaining authentic representation of its origin.",
          "Authentic representation means the media accurately reflects what it claims to show."
        ],
        "negative_examples": [
          "The image is real.",
          "A photo was taken.",
          "The content exists."
        ],
        "disambiguation": "Commitment to truthful, non-deceptive representation, not just any real media"
      },
      "children": []
    },

    {
      "term": "VoiceCloning",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Creating a synthetic voice that mimics a specific person's vocal characteristics, enabling generation of speech they never actually spoke",
      "definition_source": "Speech synthesis, AI ethics",
      "domain": "SafetyAndSecurity",
      "aliases": ["VoiceImpersonation", "SpeakerCloning", "VoiceSynthesisCloning"],
      "relationships": {
        "related": ["SpeechSynthesis", "SpeakerRecognition", "Deepfake"],
        "opposite": ["OriginalVoiceExpression"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["fraud", "impersonation", "scams", "misinformation"],
        "treaty_relevant": true,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Voice cloning enabled the scammer to impersonate the CEO and authorize a fraudulent transfer.",
          "The TTS model can clone a voice from just 3 seconds of reference audio.",
          "Voice cloning raises concerns about consent and impersonation in synthetic media.",
          "The voice clone was indistinguishable from the original speaker to human listeners."
        ],
        "negative_examples": [
          "Text-to-speech was used.",
          "A voice was synthesized.",
          "The computer spoke."
        ],
        "disambiguation": "Specifically cloning a real person's voice for mimicry, not general TTS"
      },
      "children": []
    },

    {
      "term": "OriginalVoiceExpression",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Synthetic voice generation that creates novel, original vocal expressions without mimicking or impersonating any real individual",
      "definition_source": "Creative AI, ethical synthesis",
      "domain": "SafetyAndSecurity",
      "aliases": ["NovelVoiceCreation", "OriginalSyntheticVoice", "NonMimicryVoice"],
      "relationships": {
        "related": ["SpeechSynthesis", "Creativity", "OriginalContent"],
        "opposite": ["VoiceCloning"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The game uses original synthetic voices that don't clone any real person.",
          "Original voice expression creates unique character voices for animation without impersonating actors.",
          "The audiobook uses a distinctive AI voice designed from scratch, not cloned from anyone.",
          "Novel TTS voices provide accessibility features without identity concerns."
        ],
        "negative_examples": [
          "A voice was generated.",
          "The system spoke.",
          "Audio was synthesized."
        ],
        "disambiguation": "Deliberately original synthetic voices not mimicking real people, not just any TTS"
      },
      "children": []
    },

    {
      "term": "CounterfeitGeneration",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Using generative AI to create fake documents, credentials, currency, or official materials for fraudulent purposes",
      "definition_source": "Fraud prevention, security",
      "domain": "SafetyAndSecurity",
      "aliases": ["DocumentForgery", "CredentialFraud", "FakeDocumentGeneration"],
      "relationships": {
        "related": ["ImageGeneration", "DocumentUnderstanding", "Fraud"],
        "opposite": ["AuthorizedDocumentation"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["fraud", "identity_theft", "financial_crime"],
        "treaty_relevant": true,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "FraudMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "AI-generated fake IDs are increasingly difficult to distinguish from authentic documents.",
          "Counterfeit generation tools can produce convincing diploma and certificate forgeries.",
          "The model was misused to generate fake insurance documents for fraudulent claims.",
          "Generative AI poses new challenges for document authentication and anti-counterfeiting."
        ],
        "negative_examples": [
          "A document was created.",
          "The certificate was printed.",
          "An ID was made."
        ],
        "disambiguation": "Specifically fraudulent document creation, not legitimate document generation"
      },
      "children": []
    },

    {
      "term": "AuthorizedDocumentation",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Legitimate creation of documents and materials by parties with proper authority, for valid purposes, through sanctioned processes",
      "definition_source": "Administrative law, document management",
      "domain": "SafetyAndSecurity",
      "aliases": ["LegitimateDocumentation", "ProperCredentialing", "AuthorizedIssuance"],
      "relationships": {
        "related": ["Authorization", "Legitimacy", "OfficialProcess"],
        "opposite": ["CounterfeitGeneration"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The university issued authorized documentation of the degree through official channels.",
          "Authorized documentation includes proper letterhead, signatures, and verification mechanisms.",
          "The government agency provides authorized documentation with anti-forgery features.",
          "AI assists in generating authorized documentation templates for legitimate institutional use."
        ],
        "negative_examples": [
          "A document was made.",
          "The paper has text.",
          "Something official-looking was created."
        ],
        "disambiguation": "Documents created through legitimate authority and process, not just any official-looking output"
      },
      "children": []
    },

    {
      "term": "MultimodalJailbreak",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Attacks that exploit visual or audio inputs to bypass safety guardrails in multimodal AI systems",
      "definition_source": "AI security research, red teaming",
      "domain": "SafetyAndSecurity",
      "aliases": ["VisualJailbreak", "ImageJailbreak", "MultimodalPromptInjection"],
      "relationships": {
        "related": ["AdversarialAttack", "PromptInjection", "VisionLanguageModel"],
        "opposite": ["GoodFaithQuery"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["safety_bypass", "harmful_content", "system_manipulation"],
        "treaty_relevant": true,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "JailbreakMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The multimodal jailbreak embedded malicious instructions in an image that the model followed.",
          "Visual jailbreaks can encode text instructions as images to bypass text-based safety filters.",
          "The attacker used typography in an image to inject a prompt the model would execute.",
          "Multimodal prompt injection hides adversarial instructions in non-text modalities."
        ],
        "negative_examples": [
          "The image had text.",
          "Instructions were given.",
          "The model was asked something."
        ],
        "disambiguation": "Specifically attacks using visual/audio to bypass safety, not general misuse"
      },
      "children": []
    },

    {
      "term": "GoodFaithQuery",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Honest, straightforward requests to AI systems that seek legitimate assistance without attempting manipulation or deception",
      "definition_source": "AI ethics, user interaction",
      "domain": "SafetyAndSecurity",
      "aliases": ["HonestRequest", "SincereQuery", "LegitimateRequest", "GenuineQuestion"],
      "relationships": {
        "related": ["Honesty", "Collaboration", "Trust"],
        "opposite": ["MultimodalJailbreak", "AdversarialPrompt"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The user's good faith query asked for help understanding a complex topic without hidden agendas.",
          "Good faith queries represent honest attempts to accomplish legitimate goals with AI assistance.",
          "The request demonstrates good faith: straightforward, transparent about purpose, within guidelines.",
          "A good faith query respects the AI's intended purpose and doesn't attempt manipulation."
        ],
        "negative_examples": [
          "A question was asked.",
          "The user made a request.",
          "Input was provided."
        ],
        "disambiguation": "Specifically honest, non-manipulative requests, not just any valid query"
      },
      "children": []
    },

    {
      "term": "ExploitativeImagery",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Visual content that exploits, objectifies, or causes harm to depicted individuals, including non-consensual intimate imagery and CSAM",
      "definition_source": "Content policy, child safety",
      "domain": "SafetyAndSecurity",
      "aliases": ["NCII", "ExploitativeContent", "HarmfulImagery", "CSAM"],
      "relationships": {
        "related": ["ImageGeneration", "ContentModeration", "DigitalHarm"],
        "opposite": ["ConsentfulDepiction"]
      },
      "safety_tags": {
        "risk_level": "critical",
        "impacts": ["exploitation", "abuse", "trauma", "illegal_content"],
        "treaty_relevant": true,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "ExploitationMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "AI-generated NCII (non-consensual intimate imagery) causes severe harm to victims.",
          "Generative AI can be misused to create synthetic CSAM, which is illegal.",
          "Detection systems identify exploitative imagery for removal from platforms.",
          "The model refused to generate exploitative content depicting real individuals."
        ],
        "negative_examples": [
          "The content was inappropriate.",
          "The image was bad.",
          "Something harmful was created."
        ],
        "disambiguation": "Specifically exploitative visual content causing direct harm, not general inappropriate content"
      },
      "children": []
    },

    {
      "term": "ConsentfulDepiction",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Visual representation of individuals created with their informed consent, respecting their dignity, autonomy, and control over their image",
      "definition_source": "Media ethics, consent law, dignity",
      "domain": "SafetyAndSecurity",
      "aliases": ["ConsentedImagery", "DignifiedRepresentation", "RespectfulDepiction", "AuthorizedLikeness"],
      "relationships": {
        "related": ["Consent", "Dignity", "Autonomy", "Respect"],
        "opposite": ["ExploitativeImagery"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The portrait was created with the subject's informed consent and approval of the final image.",
          "Consentful depiction means the person depicted agreed to and controls how their likeness is used.",
          "The artist obtained consent before creating and sharing images featuring identifiable individuals.",
          "Professional photography standards require consentful depiction with model releases."
        ],
        "negative_examples": [
          "A picture was taken.",
          "Someone was photographed.",
          "An image contains a person."
        ],
        "disambiguation": "Active informed consent and dignity preservation, not just legal permission"
      },
      "children": []
    },

    {
      "term": "HazardousGeneration",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Using generative AI to create content that could facilitate real-world harm including weapons instructions, dangerous chemistry, or attack planning",
      "definition_source": "AI safety, dual-use research",
      "domain": "SafetyAndSecurity",
      "aliases": ["DangerousGeneration", "HarmfulInstructionGeneration", "WeaponizableContent"],
      "relationships": {
        "related": ["DualUse", "InformationHazard", "ContentGeneration"],
        "opposite": ["BeneficialInstructionalContent"]
      },
      "safety_tags": {
        "risk_level": "critical",
        "impacts": ["physical_harm", "weapons", "terrorism", "violence"],
        "treaty_relevant": true,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "HazardMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The model refused to generate detailed weapon assembly diagrams.",
          "Hazardous generation includes visual instructions for synthesizing dangerous chemicals.",
          "AI image generation must prevent creation of bomb-making or weapon modification guides.",
          "The safety filter blocked generation of content showing attack methodologies."
        ],
        "negative_examples": [
          "Harmful content was made.",
          "Something dangerous was generated.",
          "Bad information was created."
        ],
        "disambiguation": "Specifically content that could facilitate serious real-world harm, not general inappropriateness"
      },
      "children": []
    },

    {
      "term": "BeneficialInstructionalContent",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Educational and instructional content that helps people learn, grow, stay safe, or accomplish legitimate goals",
      "definition_source": "Education, safety communication",
      "domain": "SafetyAndSecurity",
      "aliases": ["EducationalContent", "HelpfulInstruction", "SafetyEducation", "ConstructiveGuidance"],
      "relationships": {
        "related": ["Education", "Learning", "SafetyCommunication", "Helpfulness"],
        "opposite": ["HazardousGeneration"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Beneficial instructional content teaches proper safety procedures for handling chemicals in a lab.",
          "The tutorial provides beneficial instruction on secure coding practices to prevent vulnerabilities.",
          "Medical education uses beneficial instructional imagery to teach proper surgical techniques.",
          "Safety training materials demonstrate hazard recognition without enabling harm."
        ],
        "negative_examples": [
          "Information was provided.",
          "Instructions were given.",
          "Content teaches something."
        ],
        "disambiguation": "Content specifically designed to help and educate safely, not just any instructional material"
      },
      "children": []
    },

    {
      "term": "MultimodalBias",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Systematic biases in multimodal systems including racial/gender bias in vision, accent bias in speech, and demographic disparities in generation",
      "definition_source": "Fairness ML, AI ethics",
      "domain": "SafetyAndSecurity",
      "aliases": ["VisionBias", "AudioBias", "PerceptualBias", "GenerativeBias"],
      "relationships": {
        "related": ["AlgorithmicBias", "FacialRecognition", "SpeechRecognition", "FairnessML"],
        "opposite": ["EquitableMultimodalPerformance"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["discrimination", "representational_harm", "disparate_performance"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BiasMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Facial recognition systems show higher error rates for darker-skinned individuals.",
          "Speech recognition performs worse on non-native accents and African American Vernacular English.",
          "Image generation models reproduce stereotypical representations of gender and race.",
          "The vision model exhibited multimodal bias, associating certain occupations with specific genders."
        ],
        "negative_examples": [
          "The model made errors.",
          "Some people were not recognized.",
          "The system wasn't perfect."
        ],
        "disambiguation": "Systematic demographic disparities in multimodal AI, not random errors"
      },
      "children": []
    },

    {
      "term": "EquitableMultimodalPerformance",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Fair and consistent AI performance across different demographic groups, accents, appearances, and cultural contexts",
      "definition_source": "Fairness ML, inclusive design",
      "domain": "SafetyAndSecurity",
      "aliases": ["FairPerformance", "InclusiveAI", "UnbiasedMultimodal", "DemographicParity"],
      "relationships": {
        "related": ["Fairness", "Inclusion", "Equity", "UniversalDesign"],
        "opposite": ["MultimodalBias"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The speech recognition system achieves equitable performance across accents and dialects.",
          "Equitable multimodal AI serves all users regardless of skin tone, age, or appearance.",
          "The image generation model produces diverse, non-stereotypical representations by default.",
          "Fairness audits confirmed equitable multimodal performance across demographic subgroups."
        ],
        "negative_examples": [
          "The model works for everyone.",
          "All users can use it.",
          "The system is fair."
        ],
        "disambiguation": "Actively achieved demographic parity in performance, not just aspirational fairness"
      },
      "children": []
    },

    {
      "term": "VisualProfiling",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Using visual analysis to infer sensitive attributes about individuals including demographics, emotions, or characteristics without consent",
      "definition_source": "Privacy, surveillance studies",
      "domain": "SafetyAndSecurity",
      "aliases": ["DemographicInference", "AttributeEstimation", "VisualSurveillance"],
      "relationships": {
        "related": ["FacialRecognition", "MultimodalBias", "Privacy"],
        "opposite": ["TransparentAnalysis"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["privacy", "surveillance", "discrimination", "profiling"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "PrivacyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Visual profiling systems attempt to infer age, gender, and ethnicity from facial images.",
          "Emotion recognition from facial expressions raises concerns about surveillance and consent.",
          "The system performed demographic profiling for targeted advertising based on camera footage.",
          "Visual attribute estimation can be used for discriminatory purposes without consent."
        ],
        "negative_examples": [
          "The image was analyzed.",
          "Information was extracted.",
          "Features were detected."
        ],
        "disambiguation": "Inferring sensitive personal attributes from visual data without consent, not general analysis"
      },
      "children": []
    },

    {
      "term": "AudioProfiling",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Using audio analysis to infer sensitive attributes about speakers including demographics, emotional state, or health conditions without consent",
      "definition_source": "Privacy, speech analysis",
      "domain": "SafetyAndSecurity",
      "aliases": ["VoiceProfiling", "SpeakerAttributeInference", "AcousticProfiling"],
      "relationships": {
        "related": ["SpeakerRecognition", "MultimodalBias", "Privacy"],
        "opposite": ["TransparentAnalysis"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["privacy", "surveillance", "discrimination", "health_privacy"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "PrivacyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Audio profiling infers speaker demographics from voice characteristics without consent.",
          "Voice analysis claims to detect emotions, deception, or even health conditions like Parkinson's.",
          "The call center used audio profiling to estimate caller demographics for routing decisions.",
          "Accent detection can enable discriminatory treatment based on perceived nationality."
        ],
        "negative_examples": [
          "The voice was analyzed.",
          "Speaker characteristics were noted.",
          "The audio was processed."
        ],
        "disambiguation": "Inferring sensitive personal attributes from audio without consent, not general speech processing"
      },
      "children": []
    },

    {
      "term": "TransparentAnalysis",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Multimodal analysis conducted with user awareness, explicit consent, disclosed purposes, and respect for privacy boundaries",
      "definition_source": "Privacy by design, ethical AI",
      "domain": "SafetyAndSecurity",
      "aliases": ["ConsentedAnalysis", "DisclosedProcessing", "EthicalAnalysis", "PrivacyRespectingAnalysis"],
      "relationships": {
        "related": ["Transparency", "Consent", "Privacy", "Disclosure"],
        "opposite": ["VisualProfiling", "AudioProfiling"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Transparent analysis discloses what data is collected, how it's processed, and for what purpose.",
          "The accessibility app performs transparent analysis: user opts in and controls what's analyzed.",
          "Medical diagnosis AI practices transparent analysis with patient consent and explainable outputs.",
          "Transparent analysis respects privacy by minimizing data collection and enabling user control."
        ],
        "negative_examples": [
          "The data was processed.",
          "Analysis was performed.",
          "The system analyzed input."
        ],
        "disambiguation": "Analysis with explicit consent, disclosure, and privacy respect, not just any consented use"
      },
      "children": []
    },

    {
      "term": "IntellectualPropertyInfringement",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Using generative AI to create content that infringes on copyrights, trademarks, or other intellectual property rights",
      "definition_source": "Copyright law, AI ethics",
      "domain": "SafetyAndSecurity",
      "aliases": ["CopyrightInfringement", "IPTheft", "TrademarkViolation", "StyleCopying"],
      "relationships": {
        "related": ["ImageGeneration", "MusicGeneration", "Copyright"],
        "opposite": ["OriginalCreativeExpression"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["copyright", "economic_harm", "artist_harm"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "IPMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The model was prompted to generate images replicating a living artist's distinctive style.",
          "AI-generated music that closely mimics copyrighted songs raises infringement concerns.",
          "The generated image contained recognizable copyrighted characters without authorization.",
          "Style mimicry of specific artists may constitute intellectual property infringement."
        ],
        "negative_examples": [
          "Art was created.",
          "The style looked familiar.",
          "Something similar was made."
        ],
        "disambiguation": "Specifically infringing on protected IP through generation, not general similarity"
      },
      "children": []
    },

    {
      "term": "OriginalCreativeExpression",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Novel creative works that contribute new artistic value without copying or infringing on existing protected works",
      "definition_source": "Copyright law, creative arts",
      "domain": "SafetyAndSecurity",
      "aliases": ["NovelCreation", "OriginalArtwork", "AuthenticCreativity", "IndependentExpression"],
      "relationships": {
        "related": ["Creativity", "Originality", "ArtisticExpression", "NovelContent"],
        "opposite": ["IntellectualPropertyInfringement"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "BeneficialIntentMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The artist used AI as a tool for original creative expression, developing a unique visual style.",
          "Original creative expression generates novel imagery that doesn't replicate existing works.",
          "The musician's AI-assisted composition represents original creative expression with new melodies.",
          "Creative empowerment through AI enables original expression by people who couldn't create otherwise."
        ],
        "negative_examples": [
          "Something was created.",
          "Art was made.",
          "Content was generated."
        ],
        "disambiguation": "Genuinely novel creative contribution, not just any generated content"
      },
      "children": []
    },

    {
      "term": "AdversarialPrompt",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Carefully crafted inputs designed to manipulate AI systems into producing unintended, harmful, or policy-violating outputs",
      "definition_source": "AI security, prompt engineering",
      "domain": "SafetyAndSecurity",
      "aliases": ["PromptAttack", "PromptInjection", "JailbreakPrompt"],
      "relationships": {
        "related": ["MultimodalJailbreak", "AdversarialImage", "SafetyBypass"],
        "opposite": ["GoodFaithQuery"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["safety_bypass", "harmful_output", "system_manipulation"],
        "treaty_relevant": true,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "JailbreakMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The adversarial prompt used roleplay framing to bypass safety guidelines.",
          "Prompt injection attacks embed malicious instructions in user-supplied content.",
          "The jailbreak prompt exploited the model's instruction-following to override safety training.",
          "Indirect prompt injection hides adversarial instructions in external documents the model reads."
        ],
        "negative_examples": [
          "A bad question was asked.",
          "The prompt was unusual.",
          "The request was inappropriate."
        ],
        "disambiguation": "Intentionally crafted attacks on AI safety, not unintentional misuse"
      },
      "children": []
    },

    {
      "term": "AdversarialImage",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Images with carefully crafted perturbations designed to fool computer vision models while appearing normal to humans",
      "definition_source": "Adversarial ML, Goodfellow et al.",
      "domain": "SafetyAndSecurity",
      "aliases": ["AdversarialExample", "AdversarialPerturbation", "EvasionAttack"],
      "relationships": {
        "related": ["ImageRecognition", "ObjectDetection", "RobustML"],
        "opposite": ["CleanInput"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["evasion", "misclassification", "safety_critical_failure"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AdversarialMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The adversarial image caused the classifier to misidentify a stop sign as a speed limit sign.",
          "Imperceptible pixel perturbations can cause dramatic changes in model predictions.",
          "Adversarial patches can be printed and placed in the physical world to fool detectors.",
          "The FGSM attack generates adversarial examples using the gradient of the loss function."
        ],
        "negative_examples": [
          "The image was unclear.",
          "The model made an error.",
          "The picture was confusing."
        ],
        "disambiguation": "Intentionally crafted perturbations to fool models, not natural errors or noise"
      },
      "children": []
    },

    {
      "term": "ContentModerationEvasion",
      "role": "concept",
      "parent_concepts": ["MultimodalSafetyRisk"],
      "layer_hint": 3,
      "definition": "Techniques used to circumvent automated content moderation systems including visual obfuscation, encoding, and adversarial perturbations",
      "definition_source": "Content safety, platform trust & safety",
      "domain": "SafetyAndSecurity",
      "aliases": ["ModerationBypass", "FilterEvasion", "ContentObfuscation"],
      "relationships": {
        "related": ["AdversarialImage", "ContentModeration", "TextObfuscation"],
        "opposite": ["PolicyCompliance"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["harmful_content_spread", "policy_violation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "ModerationMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The image used visual obfuscation to evade NSFW detection filters.",
          "Text embedded in images bypasses text-based content moderation.",
          "Adversarial perturbations can make violating content appear benign to classifiers.",
          "Filter evasion techniques include character substitution, image tiling, and encoding."
        ],
        "negative_examples": [
          "The content was hidden.",
          "Something was missed.",
          "The filter didn't catch it."
        ],
        "disambiguation": "Intentional circumvention of content safety systems, not accidental false negatives"
      },
      "children": []
    },

    {
      "term": "DeepfakeDetection",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Methods and systems for identifying synthetic media and distinguishing AI-generated content from authentic recordings",
      "definition_source": "Media forensics, AI safety",
      "domain": "SafetyAndSecurity",
      "aliases": ["SyntheticMediaDetection", "ForensicDetection", "AuthenticityVerification"],
      "relationships": {
        "related": ["Deepfake", "MediaForensics", "ContentAuthentication"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Deepfake detection models analyze facial landmarks and temporal inconsistencies.",
          "The forensic tool detected subtle blending artifacts indicating the video was manipulated.",
          "Deepfake detection achieved 95% accuracy on the FaceForensics++ benchmark.",
          "Lip-sync detection can identify when audio has been dubbed over different video."
        ],
        "negative_examples": [
          "The video was checked.",
          "We analyzed the content.",
          "The media was reviewed."
        ],
        "disambiguation": "Technical detection of synthetic media, not general content review"
      },
      "children": []
    },

    {
      "term": "SyntheticMediaProvenance",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Systems and standards for tracking the origin, authenticity, and modification history of digital media",
      "definition_source": "C2PA, Content Authenticity Initiative",
      "domain": "SafetyAndSecurity",
      "aliases": ["ContentProvenance", "MediaAuthenticity", "DigitalProvenance", "C2PA"],
      "relationships": {
        "related": ["DeepfakeDetection", "AIWatermarking", "ContentAuthentication"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "C2PA standards embed cryptographic provenance data in media files.",
          "Content Credentials show the origin and edit history of images.",
          "Media provenance helps distinguish authentic journalism from synthetic content.",
          "The provenance manifest recorded that the image was AI-generated and by which tool."
        ],
        "negative_examples": [
          "The file has metadata.",
          "The image has information.",
          "The source is known."
        ],
        "disambiguation": "Cryptographic authenticity tracking for synthetic media, not general metadata"
      },
      "children": []
    },

    {
      "term": "AIWatermarking",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "Embedding imperceptible signals in AI-generated content to enable later identification of synthetic origin",
      "definition_source": "Content authenticity, AI safety",
      "domain": "SafetyAndSecurity",
      "aliases": ["SyntheticWatermark", "GenerativeWatermark", "AIContentMarking"],
      "relationships": {
        "related": ["SyntheticMediaProvenance", "DeepfakeDetection", "ContentAuthentication"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "SynthID embeds an invisible watermark in AI-generated images for later detection.",
          "AI watermarking survives common transformations like compression and cropping.",
          "The watermark detector confirmed the image was generated by our model.",
          "Robust AI watermarks enable tracing synthetic content back to its source."
        ],
        "negative_examples": [
          "The image has a logo.",
          "A mark was added.",
          "The content is labeled."
        ],
        "disambiguation": "Invisible cryptographic marking of AI content, not visible watermarks or labels"
      },
      "children": []
    },

    {
      "term": "AdversarialRobustness",
      "role": "concept",
      "parent_concepts": ["MultimodalBeneficialIntent"],
      "layer_hint": 3,
      "definition": "The ability of models to maintain correct behavior when faced with adversarial inputs or attacks",
      "definition_source": "Robust ML, AI security",
      "domain": "SafetyAndSecurity",
      "aliases": ["RobustML", "AdversarialDefense", "AttackResistance"],
      "relationships": {
        "related": ["AdversarialImage", "AdversarialTraining", "ModelSecurity"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AdversarialMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Adversarial training improves robustness by including adversarial examples during training.",
          "The model achieved certified robustness guarantees against perturbations within an epsilon ball.",
          "Adversarial robustness often trades off against standard accuracy.",
          "Randomized smoothing provides provable robustness certificates for classifiers."
        ],
        "negative_examples": [
          "The model is accurate.",
          "It works well.",
          "The system is reliable."
        ],
        "disambiguation": "Specifically resistance to adversarial attacks, not general robustness or reliability"
      },
      "children": []
    }
  ]
}
