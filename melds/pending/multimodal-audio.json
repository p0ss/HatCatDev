{
  "meld_request_id": "org.hatcat/multimodal-audio@0.1.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",

  "metadata": {
    "name": "Multimodal Audio",
    "description": "Audio perception, analysis, and synthesis capabilities for multimodal AI systems, including speech recognition, speech synthesis, audio classification, and music understanding. These probes reveal when models are processing or generating audio content.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-09T00:00:00Z",
    "version": "0.1.0"
  },

  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/AuditoryPerception",
      "relationship": "parent_of",
      "candidate_concept": "ComputationalAudio"
    }
  ],

  "candidates": [
    {
      "term": "ComputationalAudio",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "The field and capability of enabling machines to analyze, understand, and generate audio signals",
      "definition_source": "Audio signal processing, speech technology",
      "domain": "ComputerScience",
      "aliases": ["AudioAI", "MachineHearing", "AudioProcessing"],
      "relationships": {
        "related": ["AuditoryPerception", "SignalProcessing", "SpeechTechnology"],
        "has_part": ["SpeechRecognition", "SpeechSynthesis", "AudioClassification", "SpeakerRecognition", "MusicAnalysis"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical capability category"
      },
      "training_hints": {
        "positive_examples": [
          "Computational audio systems power virtual assistants like Siri and Alexa.",
          "Audio AI can identify thousands of sound categories from environmental recordings.",
          "Computational audio enables real-time noise cancellation in video calls.",
          "Machine hearing systems analyze acoustic scenes for surveillance applications."
        ],
        "negative_examples": [
          "I heard a sound.",
          "The audio is playing.",
          "There is music."
        ],
        "disambiguation": "Computational systems for audio analysis/generation, not human hearing or playback"
      },
      "children": ["SpeechRecognition", "SpeechSynthesis", "AudioClassification", "SpeakerRecognition", "MusicAnalysis"]
    },

    {
      "term": "SpeechRecognition",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "The task of converting spoken language audio into text transcription",
      "definition_source": "Speech technology, NLP",
      "domain": "ComputerScience",
      "aliases": ["SpeechToText", "ASR", "AutomaticSpeechRecognition", "VoiceRecognition"],
      "relationships": {
        "related": ["Transcription", "LanguageProcessing", "AudioProcessing"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "Whisper achieves near-human speech recognition accuracy across 99 languages.",
          "The ASR system transcribed the meeting with a word error rate of 5%.",
          "Speech recognition powers voice typing on smartphones.",
          "End-to-end speech recognition models directly output text from audio waveforms."
        ],
        "negative_examples": [
          "I understood what they said.",
          "The person is speaking.",
          "They said 'hello'."
        ],
        "disambiguation": "Computational speech-to-text conversion, not human listening comprehension"
      },
      "children": []
    },

    {
      "term": "SpeechSynthesis",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "The artificial generation of human speech from text or other representations",
      "definition_source": "Speech technology, TTS systems",
      "domain": "ComputerScience",
      "aliases": ["TextToSpeech", "TTS", "VoiceSynthesis", "SpeechGeneration"],
      "relationships": {
        "related": ["AudioGeneration", "VoiceCloning", "NaturalLanguageGeneration"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["impersonation", "misinformation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Neural TTS systems like Tacotron produce remarkably natural-sounding speech.",
          "The speech synthesis engine generated audio from the input text in real-time.",
          "Voice synthesis can clone a speaker's voice from just a few minutes of samples.",
          "Modern TTS uses mel-spectrograms decoded by vocoders like HiFi-GAN."
        ],
        "negative_examples": [
          "The computer spoke.",
          "A voice said the words.",
          "The message was read aloud."
        ],
        "disambiguation": "Computational speech generation, not human speaking or recorded playback"
      },
      "children": []
    },

    {
      "term": "AudioClassification",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "The task of categorizing audio clips into predefined classes based on their content",
      "definition_source": "Audio signal processing, machine learning",
      "domain": "ComputerScience",
      "aliases": ["SoundClassification", "AudioTagging", "AcousticClassification"],
      "relationships": {
        "related": ["Classification", "SoundEventDetection", "EnvironmentalSoundRecognition"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "Audio classification identified the sound as 'dog barking' with 94% confidence.",
          "The model classifies environmental sounds into 50 categories including sirens, rain, and traffic.",
          "AudioSet provides millions of labeled clips for audio classification training.",
          "Sound classification enables smart home devices to respond to glass breaking or smoke alarms."
        ],
        "negative_examples": [
          "That sounds like a dog.",
          "I hear barking.",
          "There's a noise."
        ],
        "disambiguation": "Computational sound categorization, not human sound perception"
      },
      "children": []
    },

    {
      "term": "SpeakerRecognition",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "Identifying or verifying speakers based on characteristics of their voice",
      "definition_source": "Biometrics, speaker verification",
      "domain": "ComputerScience",
      "aliases": ["SpeakerIdentification", "VoiceBiometrics", "SpeakerVerification"],
      "relationships": {
        "related": ["Biometrics", "VoicePrint", "IdentityVerification"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["privacy", "surveillance"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "PrivacyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Speaker recognition verified the caller's identity before granting account access.",
          "The speaker diarization system attributed each utterance to one of 4 identified speakers.",
          "Voice biometrics extract a voiceprint embedding for speaker comparison.",
          "Speaker verification distinguishes between genuine users and impersonation attempts."
        ],
        "negative_examples": [
          "I recognize that voice.",
          "That's Sarah speaking.",
          "The voice sounds familiar."
        ],
        "disambiguation": "Computational speaker identification, not human voice recognition"
      },
      "children": []
    },

    {
      "term": "MusicAnalysis",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "Computational extraction and understanding of musical properties including tempo, key, chord progression, and structure",
      "definition_source": "Music information retrieval",
      "domain": "ComputerScience",
      "aliases": ["MusicInformationRetrieval", "MIR", "ComputationalMusicology"],
      "relationships": {
        "related": ["AudioAnalysis", "BeatDetection", "ChordRecognition"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "Music analysis detected the tempo as 120 BPM in 4/4 time signature.",
          "The MIR system extracted the chord progression: Am - F - C - G.",
          "Beat tracking algorithms identify the rhythmic structure of the song.",
          "Music fingerprinting enables content identification services like Shazam."
        ],
        "negative_examples": [
          "The song is upbeat.",
          "This is jazz music.",
          "The tempo feels fast."
        ],
        "disambiguation": "Computational musical property extraction, not subjective music appreciation"
      },
      "children": []
    },

    {
      "term": "AudioTranscription",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "Converting audio content including speech, speakers, and non-speech events into structured text representation",
      "definition_source": "Speech technology, transcription services",
      "domain": "ComputerScience",
      "aliases": ["Transcription", "AudioToText", "MeetingTranscription"],
      "relationships": {
        "related": ["SpeechRecognition", "SpeakerDiarization", "Captioning"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "The transcription service produced a full transcript with speaker labels and timestamps.",
          "Audio transcription captured both the dialogue and [laughter] annotations.",
          "Automatic transcription enables real-time closed captioning for broadcasts.",
          "The transcription pipeline combines ASR with punctuation restoration and formatting."
        ],
        "negative_examples": [
          "I wrote down what was said.",
          "The conversation was recorded.",
          "Notes were taken."
        ],
        "disambiguation": "Comprehensive computational audio-to-text conversion including metadata"
      },
      "children": []
    },

    {
      "term": "VoiceActivityDetection",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "Determining when speech is present in an audio stream versus silence or background noise",
      "definition_source": "Speech processing, telecommunications",
      "domain": "ComputerScience",
      "aliases": ["VAD", "SpeechDetection", "VoiceDetection"],
      "relationships": {
        "related": ["SpeechRecognition", "EndpointDetection", "NoiseReduction"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical preprocessing step"
      },
      "training_hints": {
        "positive_examples": [
          "VAD segments the audio into speech and non-speech regions before transcription.",
          "Voice activity detection conserves bandwidth by only transmitting when speech is present.",
          "The VAD model distinguishes speech from music and environmental sounds.",
          "Endpoint detection using VAD determines when the user has finished speaking."
        ],
        "negative_examples": [
          "Someone is talking.",
          "There is silence.",
          "I can hear speech."
        ],
        "disambiguation": "Computational speech presence detection, not human awareness of speech"
      },
      "children": []
    },

    {
      "term": "AudioGeneration",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "The artificial creation of audio content including sound effects, ambient sounds, and audio textures",
      "definition_source": "Generative audio, sound design",
      "domain": "ComputerScience",
      "aliases": ["SoundGeneration", "AudioSynthesis", "GenerativeAudio"],
      "relationships": {
        "related": ["SpeechSynthesis", "MusicGeneration", "SoundDesign"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical capability"
      },
      "training_hints": {
        "positive_examples": [
          "Audio generation creates realistic sound effects from text descriptions like 'thunderstorm with rain'.",
          "The generative model synthesizes ambient soundscapes for video game environments.",
          "Neural audio synthesis produces high-fidelity sounds that don't exist in any recording.",
          "AudioLDM generates diverse audio clips conditioned on text prompts."
        ],
        "negative_examples": [
          "A sound was made.",
          "The audio plays thunder.",
          "There are sound effects."
        ],
        "disambiguation": "AI-generated novel audio content, not playback or human sound production"
      },
      "children": []
    },

    {
      "term": "MusicGeneration",
      "role": "concept",
      "parent_concepts": ["ComputationalAudio"],
      "layer_hint": 3,
      "definition": "The artificial creation of musical compositions including melody, harmony, rhythm, and arrangement",
      "definition_source": "Generative music, AI composition",
      "domain": "ComputerScience",
      "aliases": ["AIComposition", "GenerativeMusic", "AlgorithmicComposition"],
      "relationships": {
        "related": ["AudioGeneration", "MusicAnalysis", "CreativeGeneration"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": ["copyright"],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical capability"
      },
      "training_hints": {
        "positive_examples": [
          "MusicLM generates high-fidelity music from text descriptions of style and mood.",
          "The AI composer created a complete orchestral arrangement in the style of Bach.",
          "Music generation models can continue a melody in a consistent style.",
          "Jukebox generates music with vocals from artist, genre, and lyrics prompts."
        ],
        "negative_examples": [
          "Music is playing.",
          "A song was composed.",
          "The melody sounds nice."
        ],
        "disambiguation": "AI-generated original musical compositions, not human composition or playback"
      },
      "children": []
    }
  ]
}
