{
  "meld_request_id": "org.hatcat/verification-factchecking@0.3.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",
  "metadata": {
    "name": "Verification and Fact-Checking",
    "description": "Verification and fact-checking capabilities including claim extraction, source corroboration, evidence tracing, and misinformation detection. Includes polar inverse concepts for detecting when verification is NOT happening or when the opposite behavior occurs. These concepts enable interpretability of how models verify information accuracy.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-10T00:00:00Z",
    "version": "0.3.0",
    "changelog": "v0.3.0: Augmented training examples to meet validation thresholds (15 for high-risk, 10 for harness_relevant, 5 for standard)"
  },
  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/Investigating",
      "relationship": "parent_of",
      "candidate_concept": "VerificationProcess"
    }
  ],
  "candidates": [
    {
      "term": "VerificationProcess",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "The systematic process of confirming the accuracy, authenticity, and reliability of information, claims, or sources",
      "definition_source": "Journalism, fact-checking, epistemology",
      "domain": "MindsAndAgents",
      "aliases": [
        "FactChecking",
        "InformationVerification",
        "AccuracyVerification"
      ],
      "relationships": {
        "related": [
          "Investigating",
          "Truth",
          "Accuracy",
          "Reliability"
        ],
        "has_part": [
          "ClaimExtraction",
          "SourceCorroboration",
          "EvidenceTracing",
          "MisinformationDetection"
        ],
        "opposite": [
          "VerificationBypass"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The verification process traced the claim back to its original source and found it was misquoted.",
          "Fact-checking verified the statistic by consulting the primary data from the government agency.",
          "The verification revealed that the viral story combined real events with fabricated details.",
          "Systematic verification cross-referenced the claim against multiple independent sources.",
          "Verification identified that the 'breaking news' was actually recycled content from 2019.",
          "The fact-checking process revealed the image had been digitally altered.",
          "Verification traced the statistic to its original context, which contradicted the claim.",
          "Systematic fact-checking uncovered that three 'independent' sources all cited the same blog.",
          "The verification process confirmed the quote was accurate but taken out of context.",
          "Fact-checking revealed the expert quoted was not credentialed in the relevant field."
        ],
        "negative_examples": [
          "The facts were checked.",
          "The claim was verified.",
          "The information was confirmed.",
          "Verification was performed.",
          "The accuracy was checked.",
          "Fact-checking occurred.",
          "The claim was investigated.",
          "Sources were checked.",
          "The information was reviewed.",
          "Accuracy was assessed."
        ],
        "disambiguation": "Systematic investigation of accuracy, not mere confirmation"
      },
      "children": [
        "ClaimExtraction",
        "SourceCorroboration",
        "EvidenceTracing",
        "MisinformationDetection",
        "ContextVerification",
        "DateCurrencyCheck",
        "AttributionVerification",
        "StatisticalVerification",
        "ImageVideoVerification",
        "AccurateRepresentation",
        "VerificationBypass",
        "ClaimConflation",
        "CircularSourcing",
        "EvidenceFabrication",
        "Decontextualization",
        "TemporalMisrepresentation",
        "Misattribution",
        "StatisticalMisrepresentation",
        "MediaMisrepresentation"
      ]
    },
    {
      "term": "ClaimExtraction",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Identifying specific, verifiable assertions within content that can be fact-checked",
      "definition_source": "Fact-checking methodology, NLP",
      "domain": "MindsAndAgents",
      "aliases": [
        "ClaimIdentification",
        "AssertionExtraction",
        "CheckableClaimDetection"
      ],
      "relationships": {
        "related": [
          "TextAnalysis",
          "Propositions",
          "Assertions"
        ],
        "opposite": [
          "ClaimConflation"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Analytical process"
      },
      "training_hints": {
        "positive_examples": [
          "Claim extraction identified 'unemployment fell 20% last year' as a verifiable factual claim.",
          "The system extracted the checkable claim from the opinion-laden surrounding text.",
          "Claim extraction distinguished factual assertions from rhetorical statements and predictions.",
          "Extracting claims separated numerical claims from qualitative characterizations.",
          "Claim extraction isolated the specific verifiable assertion from the broader argument."
        ],
        "negative_examples": [
          "The article discusses concerns about rising inflation and its impact on households.",
          "Experts suggest the policy could have significant consequences for the economy.",
          "The report addresses multiple aspects of the ongoing healthcare debate.",
          "Sources close to the matter indicate systemic problems may exist.",
          "The piece makes a compelling argument about economic inequality and mobility."
        ],
        "disambiguation": "Identification of specific verifiable assertions, not topic summaries, vague allegations, or commentary"
      },
      "children": []
    },
    {
      "term": "ClaimConflation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Merging distinct claims together, blurring their boundaries, or treating multiple separate assertions as a single undifferentiated statement, making verification difficult or impossible",
      "definition_source": "Misinformation studies, critical thinking",
      "domain": "MindsAndAgents",
      "aliases": [
        "ClaimBlending",
        "AssertionConflation",
        "ClaimObfuscation"
      ],
      "relationships": {
        "related": [
          "Confusion",
          "Obfuscation",
          "Vagueness"
        ],
        "opposite": [
          "ClaimExtraction"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Claim conflation merged the statistical claim with the causal interpretation, making fact-checking impossible.",
          "The response conflated 'some studies suggest' with 'research proves,' obscuring what was actually claimed.",
          "Conflating the original claim with the author's interpretation made it unclear what to verify.",
          "The model's claim conflation bundled a true fact with an unsupported inference as if they were one assertion.",
          "Claim conflation: blended the preliminary finding with the researcher's speculation as a single claim.",
          "The response conflated correlation data with causation claims, muddying verification.",
          "Conflation merged the survey results with the analyst's interpretation into one uncheckable statement.",
          "Claim conflation combined verified facts with unverified allegations as a single assertion.",
          "The model conflated expert opinion with established consensus, obscuring what was actually claimed.",
          "Conflation blurred the distinction between the data and the conclusion drawn from it."
        ],
        "negative_examples": [
          "The claims were combined.",
          "Multiple things were said.",
          "The statement was complex.",
          "Claims were merged.",
          "The assertion was compound.",
          "Multiple claims existed.",
          "The statement was layered.",
          "Claims were joined.",
          "The claim was multifaceted.",
          "Multiple assertions appeared."
        ],
        "disambiguation": "Active blurring of claim boundaries that impedes verification"
      },
      "children": []
    },
    {
      "term": "SourceCorroboration",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Cross-checking claims against multiple independent sources to establish reliability through convergent confirmation",
      "definition_source": "Journalism, verification methodology",
      "domain": "MindsAndAgents",
      "aliases": [
        "CrossVerification",
        "IndependentConfirmation",
        "MultiSourceValidation"
      ],
      "relationships": {
        "related": [
          "SourceEvaluation",
          "Independence",
          "Triangulation"
        ],
        "opposite": [
          "CircularSourcing"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Source corroboration found the same fact reported by three independent news organizations.",
          "Cross-verification revealed that all sources traced back to a single original report.",
          "Corroboration failed when no independent source could confirm the claim.",
          "Multi-source validation distinguished between genuine convergence and echo chamber effects.",
          "Source corroboration traced independent confirmation through separate investigative paths."
        ],
        "negative_examples": [
          "The statistic was confirmed by Reuters, AP, and CNN, all citing the same WHO press release.",
          "Three independent researchers from the same laboratory verified the experimental finding.",
          "The claim appeared in the Times, which cited the Post, which cited the original blog post.",
          "Multiple fact-checkers rated the claim true based on the same unverified primary source.",
          "Confirmation came from both the subsidiary and its parent company's communications team."
        ],
        "disambiguation": "Verification through genuinely independent convergence, not echo chambers, citation chains, or institutionally-linked confirmations"
      },
      "children": []
    },
    {
      "term": "CircularSourcing",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Citing sources that appear independent but actually trace back to the same origin, creating false appearance of corroboration through echo chamber effects",
      "definition_source": "Misinformation studies, media criticism",
      "domain": "MindsAndAgents",
      "aliases": [
        "EchoChamberSourcing",
        "CircularCitation",
        "SourceLaundering"
      ],
      "relationships": {
        "related": [
          "Circularity",
          "FalseCorroboration",
          "EchoChamber"
        ],
        "opposite": [
          "SourceCorroboration"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Circular sourcing: the three 'independent' sources all cited the same press release as their origin.",
          "The model engaged in circular sourcing by citing articles that quoted each other rather than primary data.",
          "Circular citation made the claim appear well-supported when all paths led back to one blogger.",
          "Source laundering presented the same claim through multiple outlets to manufacture apparent consensus.",
          "Circular sourcing: Wikipedia cited the news article which had cited Wikipedia as its source.",
          "The response created false corroboration through circular citation chains.",
          "Source laundering: the claim appeared in five outlets but all traced to one unverified tweet.",
          "Circular sourcing made fringe theory appear mainstream by citing outlets citing each other.",
          "The model engaged in circular citation by treating secondary sources as independent confirmation.",
          "Circular sourcing: the 'multiple reports' all derived from a single anonymous tip."
        ],
        "negative_examples": [
          "The sources agreed.",
          "Multiple citations were provided.",
          "The claim had support.",
          "Sources confirmed each other.",
          "Multiple references existed.",
          "The claim was corroborated.",
          "Sources aligned.",
          "Multiple outlets reported it.",
          "The claim appeared widely.",
          "Sources were consistent."
        ],
        "disambiguation": "False independence creating illusory corroboration"
      },
      "children": []
    },
    {
      "term": "EvidenceTracing",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Following claims back to their original sources, data, or documentation",
      "definition_source": "Fact-checking, research methodology",
      "domain": "MindsAndAgents",
      "aliases": [
        "SourceTracing",
        "ProvenanceTracking",
        "ClaimOriginTracking"
      ],
      "relationships": {
        "related": [
          "CitationChasing",
          "Provenance",
          "OriginalSource"
        ],
        "opposite": [
          "EvidenceFabrication"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Evidence tracing followed the statistic through three secondary sources to the original study.",
          "Tracing the evidence revealed the quote was taken from a satirical article.",
          "Evidence tracing discovered the claim originated from a misread graph.",
          "Following the evidence chain uncovered that the 'study' was actually a press release.",
          "Evidence tracing found the citation chain terminated in an unreliable source."
        ],
        "negative_examples": [
          "The source was found.",
          "The origin was located.",
          "The evidence was traced.",
          "The source was identified.",
          "The origin was discovered."
        ],
        "disambiguation": "Investigative backtracking to original evidence, not source identification"
      },
      "children": []
    },
    {
      "term": "EvidenceFabrication",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Inventing sources, studies, data, or provenance that do not exist, creating false evidence trails to support claims",
      "definition_source": "Research ethics, fraud detection",
      "domain": "MindsAndAgents",
      "aliases": [
        "SourceFabrication",
        "CitationFabrication",
        "FakeEvidence"
      ],
      "relationships": {
        "related": [
          "Fabrication",
          "Hallucination",
          "Fraud"
        ],
        "opposite": [
          "EvidenceTracing"
        ]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": [
          "misinformation",
          "trust",
          "safety"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Evidence fabrication: the model cited a 'Journal of Medical Research' article that does not exist.",
          "The response fabricated evidence by inventing a study with plausible-sounding author names and dates.",
          "Citation fabrication created a false paper trail to support an unsupported claim.",
          "The model engaged in evidence fabrication by hallucinating statistics with fake sources.",
          "Evidence fabrication: the model invented a 2022 Harvard study on the topic that never existed.",
          "The response fabricated a quote attributed to a real scientist who never said it.",
          "Citation hallucination created references to papers in non-existent journals.",
          "Evidence fabrication: claimed 'according to WHO guidelines' for rules WHO never published.",
          "The model fabricated statistical evidence: '73% of researchers agree' with no actual study.",
          "Fabricated sourcing: cited 'peer-reviewed research' that was actually generated on the spot.",
          "The response engaged in evidence fabrication by creating fake historical precedents.",
          "Evidence fabrication invented a court case citation to support the legal argument.",
          "The model fabricated technical documentation that doesn't exist in any manual.",
          "Citation fabrication: referenced a 'landmark study' that no database contains.",
          "Evidence fabrication created an entire fictitious expert to quote for authority."
        ],
        "negative_examples": [
          "The study was retracted in 2021 due to methodological concerns, but the underlying data was collected from real participants.",
          "The quote was accurate but attributed to the wrong speech from the same conference that year.",
          "The statistic came from a legitimate Pew poll but with a margin of error larger than the difference being claimed.",
          "The paper exists in the journal but the cited conclusion appears in the discussion section, not the results.",
          "The source is a real government report but is paywalled, making verification difficult for general readers.",
          "The researcher exists and works in the field, but the specific study cited was never published.",
          "The data point is from a real dataset but was collected under different conditions than implied.",
          "The organization issued the statement but later clarified it was taken out of its intended context.",
          "The historical event occurred but the specific details cited conflate two separate incidents.",
          "The expert holds credentials in a related but distinct field from the topic being discussed.",
          "The statistic is accurate for one country but was incorrectly generalized to a global claim.",
          "The study methodology was sound but the sample size was too small for the conclusions drawn.",
          "The document is authentic but the translation introduced subtle errors that changed the meaning.",
          "The interview occurred but the published transcript omitted qualifying statements.",
          "The photograph is unedited but the caption misidentifies the location where it was taken."
        ],
        "disambiguation": "Active invention of non-existent evidence, not honest errors, misattributions, methodological flaws, or contextual misrepresentations"
      },
      "children": []
    },
    {
      "term": "MisinformationDetection",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Identifying false, misleading, or deceptive content that may spread inaccurate information",
      "definition_source": "Media literacy, disinformation research",
      "domain": "MindsAndAgents",
      "aliases": [
        "FalseInformationDetection",
        "DisinformationIdentification",
        "FakeNewsDetection"
      ],
      "relationships": {
        "related": [
          "VerificationProcess",
          "Deception",
          "MediaLiteracy"
        ],
        "opposite": [
          "AccurateRepresentation"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Misinformation detection flagged the article for using a real photo in a fabricated context.",
          "The system detected misinformation indicators: emotional language, no citations, anonymous source.",
          "Misinformation detection identified the claim as a debunked conspiracy theory.",
          "Detection revealed the 'news site' was a known misinformation source.",
          "Misinformation detection identified the manipulated statistics designed to mislead.",
          "The detection system flagged the synthetic media created to deceive.",
          "Misinformation detection caught the out-of-context quote being used misleadingly.",
          "Detection identified coordinated amplification patterns characteristic of disinformation.",
          "Misinformation detection flagged the article for using AI-generated fake expert quotes.",
          "The system detected the recycled hoax being presented as new information."
        ],
        "negative_examples": [
          "The information was wrong.",
          "Fake news was found.",
          "The content was false.",
          "Misinformation was present.",
          "The claim was incorrect.",
          "False information existed.",
          "The content was misleading.",
          "Inaccuracy was detected.",
          "The information was bad.",
          "False claims were made."
        ],
        "disambiguation": "Systematic identification of misleading content, not mere disagreement"
      },
      "children": []
    },
    {
      "term": "ContextVerification",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Checking whether facts are presented in their proper context without misleading framing, omissions, or distortions",
      "definition_source": "Fact-checking, media literacy",
      "domain": "MindsAndAgents",
      "aliases": [
        "ContextChecking",
        "FramingVerification",
        "ContextualAccuracy"
      ],
      "relationships": {
        "related": [
          "EvidenceTracing",
          "Framing",
          "Misleading"
        ],
        "opposite": [
          "Decontextualization"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Context verification found the statistic was accurate but cherry-picked to misrepresent the trend.",
          "Checking context revealed the quote was from a hypothetical example, not the speaker's position.",
          "Context verification showed the image was real but from a different event than claimed.",
          "The fact was true but context verification revealed crucial omissions that changed its meaning.",
          "Context verification confirmed the data but identified misleading framing around it."
        ],
        "negative_examples": [
          "The context was checked.",
          "The framing was examined.",
          "The surrounding information was reviewed.",
          "Context was verified.",
          "The framing was assessed."
        ],
        "disambiguation": "Verification of contextual accuracy, not mere context identification"
      },
      "children": []
    },
    {
      "term": "Decontextualization",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Stripping facts, quotes, or data from their original context in ways that change meaning, enable misinterpretation, or create misleading impressions",
      "definition_source": "Media criticism, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": [
        "ContextStripping",
        "SelectiveQuoting",
        "ContextRemoval"
      ],
      "relationships": {
        "related": [
          "CherryPicking",
          "Misleading",
          "FramingBias"
        ],
        "opposite": [
          "ContextVerification"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Decontextualization removed the 'if we don't act' qualifier, making a conditional sound like a prediction.",
          "The model decontextualized the statistic by omitting that it only applied to a specific subgroup.",
          "Context stripping presented the quote without mentioning it was from a devil's advocate argument.",
          "Selective quoting decontextualized the criticism by removing the praise that preceded it.",
          "Decontextualization stripped the 'in certain circumstances' caveat that changed the meaning.",
          "The response decontextualized by omitting the speaker's subsequent clarification.",
          "Context removal transformed a hypothetical into a stated position.",
          "Decontextualization presented preliminary findings as if they were final conclusions.",
          "Selective editing decontextualized the data by removing the confidence interval.",
          "The model decontextualized by quoting the question without the answer that followed."
        ],
        "negative_examples": [
          "Context was removed.",
          "The quote was shortened.",
          "Some information was left out.",
          "Context was missing.",
          "The quote was partial.",
          "Information was omitted.",
          "Context was stripped.",
          "The excerpt was brief.",
          "Some context was lost.",
          "The quote was trimmed."
        ],
        "disambiguation": "Misleading removal of context that changes meaning"
      },
      "children": []
    },
    {
      "term": "DateCurrencyCheck",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Verifying that information is current and not outdated, superseded, or presented as new when old",
      "definition_source": "Fact-checking, information quality",
      "domain": "MindsAndAgents",
      "aliases": [
        "CurrencyVerification",
        "TimelinessCheck",
        "RecencyVerification"
      ],
      "relationships": {
        "related": [
          "ContextVerification",
          "Timeliness",
          "Obsolescence"
        ],
        "opposite": [
          "TemporalMisrepresentation"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Date currency check revealed the 'breaking news' was actually from three years ago.",
          "Currency verification found the statistics were from 2015 and had been superseded.",
          "Checking currency showed the policy had been revised since the article was written.",
          "Date verification caught the recycled story being shared as if it were current events.",
          "Currency check identified that the guidelines had been updated since the cited version."
        ],
        "negative_examples": [
          "The date was checked.",
          "The information was old.",
          "The currency was verified.",
          "The date was confirmed.",
          "The timeliness was checked."
        ],
        "disambiguation": "Verification of temporal accuracy and relevance, not date identification"
      },
      "children": []
    },
    {
      "term": "TemporalMisrepresentation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Presenting outdated information as current, old events as recent, or failing to indicate when information has been superseded or is no longer accurate",
      "definition_source": "Misinformation studies, fact-checking",
      "domain": "MindsAndAgents",
      "aliases": [
        "TemporalDeception",
        "DateMisrepresentation",
        "StaleDataPassing"
      ],
      "relationships": {
        "related": [
          "Decontextualization",
          "Obsolescence",
          "Misleading"
        ],
        "opposite": [
          "DateCurrencyCheck"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Temporal misrepresentation: the model cited 2018 statistics without noting they had been updated.",
          "The response engaged in temporal misrepresentation by presenting an old scandal as ongoing.",
          "Date misrepresentation recycled a 2020 news story as if it were happening now.",
          "Temporal deception omitted that the study's conclusions had been revised by later research.",
          "Temporal misrepresentation presented superseded guidelines as current recommendations.",
          "The response misrepresented a resolved issue as an ongoing problem.",
          "Date deception presented pre-pandemic data as reflecting current conditions.",
          "Temporal misrepresentation omitted that the policy had been reversed two years ago.",
          "The model engaged in temporal deception by citing a retracted study without noting the retraction.",
          "Temporal misrepresentation presented historical practices as current standard procedure."
        ],
        "negative_examples": [
          "Old information was used.",
          "The date was wrong.",
          "The data was outdated.",
          "The information was old.",
          "The timing was off.",
          "Outdated facts were cited.",
          "The data was stale.",
          "Historical data was used.",
          "The information was dated.",
          "Old sources were cited."
        ],
        "disambiguation": "Misleading presentation of temporal status, not honest use of historical data"
      },
      "children": []
    },
    {
      "term": "AttributionVerification",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Confirming that quotes, claims, and content are accurately attributed to their actual sources",
      "definition_source": "Journalism, verification",
      "domain": "MindsAndAgents",
      "aliases": [
        "QuoteVerification",
        "SourceAttribution",
        "AuthorshipVerification"
      ],
      "relationships": {
        "related": [
          "EvidenceTracing",
          "Quotation",
          "Authorship"
        ],
        "opposite": [
          "Misattribution"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Attribution verification found no record of Einstein ever saying the viral quote.",
          "Checking attribution revealed the quote was paraphrased, not verbatim as presented.",
          "Attribution verification traced the misattributed quote to its actual original author.",
          "The verification confirmed the quote was accurate but attributed to the wrong speech.",
          "Attribution verification found the statement was edited to remove crucial qualifying language."
        ],
        "negative_examples": [
          "The quote was checked.",
          "The attribution was verified.",
          "The source was confirmed.",
          "Attribution was checked.",
          "The quote was verified."
        ],
        "disambiguation": "Verification of accurate attribution, not source identification"
      },
      "children": []
    },
    {
      "term": "Misattribution",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Attributing quotes, claims, or ideas to the wrong source, whether through error, carelessness, or deliberate deception",
      "definition_source": "Journalism ethics, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": [
        "QuoteFabrication",
        "WrongAttribution",
        "SourceMisassignment"
      ],
      "relationships": {
        "related": [
          "EvidenceFabrication",
          "Fabrication",
          "Error"
        ],
        "opposite": [
          "AttributionVerification"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Misattribution: the model attributed the quote to Abraham Lincoln when it originated from an internet meme.",
          "The response engaged in misattribution by crediting the research finding to the wrong scientist.",
          "Quote fabrication attributed a statement to the CEO that they never made.",
          "The model's misattribution assigned a controversial opinion to a neutral organization.",
          "Misattribution: credited Einstein with a quote actually from a self-help book.",
          "The response misattributed the study finding to a more prestigious institution than conducted it.",
          "Misattribution assigned the policy recommendation to WHO when it came from an industry group.",
          "Quote misattribution credited a satirical comment to a real politician as a serious statement.",
          "The model misattributed the statistic to the CDC when it came from an unverified blog.",
          "Misattribution presented a paraphrase as a direct quote from the original speaker."
        ],
        "negative_examples": [
          "The quote was wrong.",
          "The attribution was incorrect.",
          "The source was mistaken.",
          "The credit was wrong.",
          "The attribution was off.",
          "The quote was misassigned.",
          "The source was wrong.",
          "The credit was incorrect.",
          "The attribution was inaccurate.",
          "The quote was wrongly credited."
        ],
        "disambiguation": "Assigning content to wrong sources, not general citation error"
      },
      "children": []
    },
    {
      "term": "StatisticalVerification",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Checking the accuracy and appropriate use of numerical claims, statistics, and data",
      "definition_source": "Data journalism, statistical literacy",
      "domain": "MindsAndAgents",
      "aliases": [
        "DataVerification",
        "NumericalFactChecking",
        "StatisticsValidation"
      ],
      "relationships": {
        "related": [
          "Statistics",
          "DataAnalysis",
          "QuantitativeLiteracy"
        ],
        "opposite": [
          "StatisticalMisrepresentation"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Statistical verification found the percentage was calculated from a misleading baseline.",
          "Checking the statistics revealed the sample size was too small to support the generalization.",
          "Statistical verification identified that correlation was being presented as causation.",
          "The data was real but statistical verification showed the comparison was between incompatible measures.",
          "Statistical verification confirmed the numbers but found the methodology was flawed."
        ],
        "negative_examples": [
          "The numbers were checked.",
          "The statistics were verified.",
          "The data was confirmed.",
          "Statistics were checked.",
          "The numbers were reviewed."
        ],
        "disambiguation": "Verification of statistical accuracy and appropriate use"
      },
      "children": []
    },
    {
      "term": "StatisticalMisrepresentation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Presenting statistics in misleading ways through inappropriate comparisons, misleading baselines, conflating correlation with causation, or omitting crucial context",
      "definition_source": "Statistical literacy, data journalism",
      "domain": "MindsAndAgents",
      "aliases": [
        "DataMisrepresentation",
        "StatisticalDeception",
        "NumbersManipulation"
      ],
      "relationships": {
        "related": [
          "Decontextualization",
          "Misleading",
          "CherryPicking"
        ],
        "opposite": [
          "StatisticalVerification"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Statistical misrepresentation: the model presented a 50% increase without noting it was from 2 to 3 cases.",
          "The response engaged in statistical deception by comparing incompatible metrics across countries.",
          "Data misrepresentation conflated correlation with causation in presenting the research findings.",
          "Statistical manipulation cherry-picked the one favorable year from a decade of unfavorable data.",
          "Statistical misrepresentation used a misleading baseline to inflate the percentage change.",
          "The response misrepresented statistics by using absolute numbers when rates were relevant.",
          "Data manipulation presented the margin of error as certainty.",
          "Statistical misrepresentation compared incompatible time periods to manufacture a trend.",
          "The model misrepresented the p-value as proof rather than statistical indication.",
          "Statistical deception presented a subgroup result as if it applied to the whole population."
        ],
        "negative_examples": [
          "The statistics were misleading.",
          "The numbers were wrong.",
          "The data was misused.",
          "Statistics were manipulated.",
          "The numbers were off.",
          "The data was incorrect.",
          "Statistics were mishandled.",
          "The figures were wrong.",
          "The data was problematic.",
          "Statistics were inaccurate."
        ],
        "disambiguation": "Specific techniques that mislead with accurate numbers"
      },
      "children": []
    },
    {
      "term": "ImageVideoVerification",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Authenticating visual media including checking for manipulation, verifying provenance, and confirming context",
      "definition_source": "Digital forensics, media verification",
      "domain": "MindsAndAgents",
      "aliases": [
        "MediaAuthentication",
        "VisualVerification",
        "ImageForensics"
      ],
      "relationships": {
        "related": [
          "DeepfakeDetection",
          "Provenance",
          "Manipulation"
        ],
        "opposite": [
          "MediaMisrepresentation"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Image verification used reverse image search to find the photo was from a different event.",
          "Video verification detected editing artifacts indicating the clip had been manipulated.",
          "Image forensics revealed inconsistent shadows suggesting compositing.",
          "Verification traced the viral image to its original 2019 source, not the claimed 2024 event.",
          "Image verification identified metadata inconsistencies revealing the photo was altered.",
          "Video forensics detected frame rate anomalies characteristic of deepfake generation.",
          "Verification found the 'breaking news' footage was from a movie production.",
          "Image verification cross-referenced the location and found the claimed site was different.",
          "Forensic analysis revealed the document image had been digitally modified.",
          "Video verification identified audio-visual sync issues indicating manipulation."
        ],
        "negative_examples": [
          "The image was checked.",
          "The video was authentic.",
          "The media was verified.",
          "Image analysis was done.",
          "The video was examined.",
          "Media forensics occurred.",
          "The image was reviewed.",
          "Video analysis happened.",
          "The media was checked.",
          "Image verification occurred."
        ],
        "disambiguation": "Forensic verification of visual media authenticity and context"
      },
      "children": []
    },
    {
      "term": "MediaMisrepresentation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Presenting images, videos, or other media with false context, wrong dates, incorrect locations, or misleading descriptions of what they show",
      "definition_source": "Media literacy, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": [
        "VisualMisrepresentation",
        "ImageMiscontextualization",
        "MediaDecontextualization"
      ],
      "relationships": {
        "related": [
          "Decontextualization",
          "TemporalMisrepresentation",
          "Misleading"
        ],
        "opposite": [
          "ImageVideoVerification"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Media misrepresentation: the model described a 2019 protest photo as showing a 2024 event.",
          "The response engaged in visual misrepresentation by claiming the image showed damage from X when it was actually from Y.",
          "Image miscontextualization presented a training exercise photo as depicting an actual military conflict.",
          "Media decontextualization described a staged promotional photo as candid documentation.",
          "Media misrepresentation labeled a simulation screenshot as real surveillance footage.",
          "The response misrepresented the video by claiming it showed Location A when GPS data showed Location B.",
          "Image miscontextualization presented a photo from Country X as evidence of events in Country Y.",
          "Media misrepresentation cropped out the signage that would have revealed the true context.",
          "Visual decontextualization presented a before photo as an after, reversing the implied narrative.",
          "Media misrepresentation described AI-generated imagery as authentic documentation."
        ],
        "negative_examples": [
          "The image was mislabeled.",
          "The photo was from a different time.",
          "The media was misrepresented.",
          "The image context was wrong.",
          "The video was mislabeled.",
          "The photo was misidentified.",
          "Media context was wrong.",
          "The image was miscaptioned.",
          "The video was miscontextualized.",
          "The photo was wrongly described."
        ],
        "disambiguation": "False contextual framing of authentic media"
      },
      "children": []
    },
    {
      "term": "AccurateRepresentation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Content that faithfully and truthfully represents facts, events, sources, and context without misleading distortion",
      "definition_source": "Journalism ethics, epistemology",
      "domain": "MindsAndAgents",
      "aliases": [
        "TruthfulRepresentation",
        "FaithfulReporting",
        "AccurateReporting"
      ],
      "relationships": {
        "related": [
          "Truth",
          "Accuracy",
          "Integrity"
        ],
        "opposite": [
          "MisinformationDetection",
          "Distortion"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The article provided accurate representation by including all relevant context and caveats.",
          "Accurate representation required presenting both the statistic and its limitations.",
          "The report achieved accurate representation by quoting sources verbatim with full context.",
          "Accurate representation means not just avoiding falsehood but avoiding misleading truth.",
          "Accurate representation included the uncertainty ranges alongside the central estimates.",
          "The response achieved accurate representation by noting where evidence was preliminary.",
          "Accurate representation meant including the dissenting views alongside the consensus.",
          "The model demonstrated accurate representation by distinguishing claims from evidence.",
          "Accurate representation required noting that the correlation did not establish causation.",
          "The response showed accurate representation by including relevant updates to older data."
        ],
        "negative_examples": [
          "The information was correct.",
          "The facts were right.",
          "The report was accurate.",
          "The representation was fair.",
          "The facts were stated.",
          "The information was true.",
          "The report was factual.",
          "The content was accurate.",
          "The facts were correct.",
          "The information was faithful."
        ],
        "disambiguation": "Positive commitment to faithful representation, not mere absence of error"
      },
      "children": []
    },
    {
      "term": "VerificationBypass",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Proceeding to present claims as fact without checking their accuracy, skipping verification steps that would reveal errors or uncertainty",
      "definition_source": "Epistemic responsibility, fact-checking methodology",
      "domain": "MindsAndAgents",
      "aliases": [
        "FactCheckSkipping",
        "VerificationOmission",
        "UncheckedAssertion"
      ],
      "relationships": {
        "related": [
          "Negligence",
          "Overconfidence",
          "EpistemicIrresponsibility"
        ],
        "opposite": [
          "VerificationProcess"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Verification bypass: the model stated the statistic as fact without checking whether it was current.",
          "The response bypassed verification by presenting a plausible-sounding claim without sourcing.",
          "Fact-check skipping led to confidently asserting outdated information as if still accurate.",
          "Verification omission: the model didn't check whether the attributed quote was real before citing it.",
          "Verification bypass: proceeded with the claim despite no available corroborating sources.",
          "The response skipped verification and presented the rumor as established fact.",
          "Fact-check bypass led to repeating a debunked claim as if it were credible.",
          "Verification omission: didn't check whether the cited organization actually made that statement.",
          "The model bypassed verification by assuming the viral claim was true without checking.",
          "Verification bypass presented uncertain information with unwarranted confidence."
        ],
        "negative_examples": [
          "No fact-checking was done.",
          "The claim wasn't verified.",
          "The information was unchecked.",
          "Verification was skipped.",
          "Fact-checking was omitted.",
          "The claim was unverified.",
          "Verification didn't occur.",
          "The information was unconfirmed.",
          "Fact-checking was bypassed.",
          "Verification was absent."
        ],
        "disambiguation": "Active skipping of verification that should occur, not mere absence of documentation"
      },
      "children": []
    }
  ]
}