{
  "meld_request_id": "org.hatcat/verification-factchecking@0.3.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",
  "metadata": {
    "name": "Verification and Fact-Checking",
    "description": "Verification and fact-checking capabilities including claim extraction, source corroboration, evidence tracing, and misinformation detection. Includes polar inverse concepts for detecting when verification is NOT happening or when the opposite behavior occurs. These concepts enable interpretability of how models verify information accuracy.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-10T00:00:00Z",
    "version": "0.3.0",
    "changelog": "v0.3.0: Augmented training examples to meet validation thresholds (15 for high-risk, 10 for harness_relevant, 5 for standard)"
  },
  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/Investigating",
      "relationship": "parent_of",
      "candidate_concept": "VerificationProcess"
    }
  ],
  "candidates": [
    {
      "term": "VerificationProcess",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "The systematic process of confirming the accuracy, authenticity, and reliability of information, claims, or sources",
      "definition_source": "Journalism, fact-checking, epistemology",
      "domain": "MindsAndAgents",
      "aliases": [
        "FactChecking",
        "InformationVerification",
        "AccuracyVerification"
      ],
      "relationships": {
        "related": [
          "Investigating",
          "Truth",
          "Accuracy",
          "Reliability"
        ],
        "has_part": [
          "ClaimExtraction",
          "SourceCorroboration",
          "EvidenceTracing",
          "MisinformationDetection"
        ],
        "opposite": [
          "VerificationBypass"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The verification process traced the claim back to its original source and found it was misquoted.",
          "Fact-checking verified the statistic by consulting the primary data from the government agency.",
          "The verification revealed that the viral story combined real events with fabricated details.",
          "Systematic verification cross-referenced the claim against multiple independent sources.",
          "Verification identified that the 'breaking news' was actually recycled content from 2019.",
          "The fact-checking process revealed the image had been digitally altered.",
          "Verification traced the statistic to its original context, which contradicted the claim.",
          "Systematic fact-checking uncovered that three 'independent' sources all cited the same blog.",
          "The verification process confirmed the quote was accurate but taken out of context.",
          "Fact-checking revealed the expert quoted was not credentialed in the relevant field."
        ],
        "negative_examples": [
          "The article was run through a plagiarism checker and the quotes matched online sources verbatim.",
          "The team verified the claim by confirming the spokesperson did say those exact words on the record.",
          "Fact-checking confirmed the statistic appeared in the cited report on page 47 as claimed.",
          "Verification found the claim was consistent with other major media coverage of the event.",
          "The information was verified by checking that it matched the official press release word-for-word.",
          "The quote was confirmed authentic by finding it in the published transcript of the speech.",
          "Verification consisted of checking that the source URL existed and returned the cited text.",
          "The team verified the claim by confirming the cited author did publish a paper with that title.",
          "Fact-checking confirmed the organization's website did contain the quoted statement.",
          "Verification found the statistic matched what was reported in other news articles on the topic."
        ],
        "disambiguation": "Substantive investigation of accuracy and reliability, not surface-level confirmation that sources exist or quotes match"
      },
      "children": [
        "ClaimExtraction",
        "SourceCorroboration",
        "EvidenceTracing",
        "MisinformationDetection",
        "ContextVerification",
        "DateCurrencyCheck",
        "AttributionVerification",
        "StatisticalVerification",
        "ImageVideoVerification",
        "AccurateRepresentation",
        "VerificationBypass",
        "ClaimConflation",
        "CircularSourcing",
        "EvidenceFabrication",
        "Decontextualization",
        "TemporalMisrepresentation",
        "Misattribution",
        "StatisticalMisrepresentation",
        "MediaMisrepresentation"
      ]
    },
    {
      "term": "ClaimExtraction",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Identifying specific, verifiable assertions within content that can be fact-checked",
      "definition_source": "Fact-checking methodology, NLP",
      "domain": "MindsAndAgents",
      "aliases": [
        "ClaimIdentification",
        "AssertionExtraction",
        "CheckableClaimDetection"
      ],
      "relationships": {
        "related": [
          "TextAnalysis",
          "Propositions",
          "Assertions"
        ],
        "opposite": [
          "ClaimConflation"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Analytical process"
      },
      "training_hints": {
        "positive_examples": [
          "Claim extraction identified 'unemployment fell 20% last year' as a verifiable factual claim.",
          "The system extracted the checkable claim from the opinion-laden surrounding text.",
          "Claim extraction distinguished factual assertions from rhetorical statements and predictions.",
          "Extracting claims separated numerical claims from qualitative characterizations.",
          "Claim extraction isolated the specific verifiable assertion from the broader argument."
        ],
        "negative_examples": [
          "The article discusses concerns about rising inflation and its impact on households.",
          "Experts suggest the policy could have significant consequences for the economy.",
          "The report addresses multiple aspects of the ongoing healthcare debate.",
          "Sources close to the matter indicate systemic problems may exist.",
          "The piece makes a compelling argument about economic inequality and mobility."
        ],
        "disambiguation": "Identification of specific verifiable assertions, not topic summaries, vague allegations, or commentary"
      },
      "children": []
    },
    {
      "term": "ClaimConflation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Merging distinct claims together, blurring their boundaries, or treating multiple separate assertions as a single undifferentiated statement, making verification difficult or impossible",
      "definition_source": "Misinformation studies, critical thinking",
      "domain": "MindsAndAgents",
      "aliases": [
        "ClaimBlending",
        "AssertionConflation",
        "ClaimObfuscation"
      ],
      "relationships": {
        "related": [
          "Confusion",
          "Obfuscation",
          "Vagueness"
        ],
        "opposite": [
          "ClaimExtraction"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Claim conflation merged the statistical claim with the causal interpretation, making fact-checking impossible.",
          "The response conflated 'some studies suggest' with 'research proves,' obscuring what was actually claimed.",
          "Conflating the original claim with the author's interpretation made it unclear what to verify.",
          "The model's claim conflation bundled a true fact with an unsupported inference as if they were one assertion.",
          "Claim conflation: blended the preliminary finding with the researcher's speculation as a single claim.",
          "The response conflated correlation data with causation claims, muddying verification.",
          "Conflation merged the survey results with the analyst's interpretation into one uncheckable statement.",
          "Claim conflation combined verified facts with unverified allegations as a single assertion.",
          "The model conflated expert opinion with established consensus, obscuring what was actually claimed.",
          "Conflation blurred the distinction between the data and the conclusion drawn from it."
        ],
        "negative_examples": [
          "The report clearly stated two separate findings: first, that unemployment rose 3%, and second, that inflation fell 2%.",
          "The study presented its factual finding (X occurred) distinctly from its interpretive conclusion (X may have caused Y).",
          "The article distinguished between the poll results (48% support) and the analyst's prediction (likely to pass).",
          "The statement contained multiple claims, each clearly delineated: 'Revenue increased 10%. Costs decreased 5%. Profit margin improved.'",
          "The research paper separated its empirical observations from its theoretical speculation in different sections.",
          "The response presented the correlation data first, then separately discussed possible causal mechanisms.",
          "The brief distinguished between established facts and the author's policy recommendations in labeled sections.",
          "The testimony clearly separated what the witness directly observed from what they were told by others.",
          "The article quoted the preliminary finding and separately noted that peer review was ongoing.",
          "The summary distinguished between the committee's unanimous findings and the areas where members disagreed."
        ],
        "disambiguation": "Active blurring of claim boundaries that impedes verification, not legitimate compound statements with clearly delineated components"
      },
      "children": []
    },
    {
      "term": "SourceCorroboration",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Cross-checking claims against multiple independent sources to establish reliability through convergent confirmation",
      "definition_source": "Journalism, verification methodology",
      "domain": "MindsAndAgents",
      "aliases": [
        "CrossVerification",
        "IndependentConfirmation",
        "MultiSourceValidation"
      ],
      "relationships": {
        "related": [
          "SourceEvaluation",
          "Independence",
          "Triangulation"
        ],
        "opposite": [
          "CircularSourcing"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Source corroboration found the same fact reported by three independent news organizations.",
          "Cross-verification revealed that all sources traced back to a single original report.",
          "Corroboration failed when no independent source could confirm the claim.",
          "Multi-source validation distinguished between genuine convergence and echo chamber effects.",
          "Source corroboration traced independent confirmation through separate investigative paths."
        ],
        "negative_examples": [
          "The statistic was confirmed by Reuters, AP, and CNN, all citing the same WHO press release.",
          "Three independent researchers from the same laboratory verified the experimental finding.",
          "The claim appeared in the Times, which cited the Post, which cited the original blog post.",
          "Multiple fact-checkers rated the claim true based on the same unverified primary source.",
          "Confirmation came from both the subsidiary and its parent company's communications team."
        ],
        "disambiguation": "Verification through genuinely independent convergence, not echo chambers, citation chains, or institutionally-linked confirmations"
      },
      "children": []
    },
    {
      "term": "CircularSourcing",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Citing sources that appear independent but actually trace back to the same origin, creating false appearance of corroboration through echo chamber effects",
      "definition_source": "Misinformation studies, media criticism",
      "domain": "MindsAndAgents",
      "aliases": [
        "EchoChamberSourcing",
        "CircularCitation",
        "SourceLaundering"
      ],
      "relationships": {
        "related": [
          "Circularity",
          "FalseCorroboration",
          "EchoChamber"
        ],
        "opposite": [
          "SourceCorroboration"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Circular sourcing: the three 'independent' sources all cited the same press release as their origin.",
          "The model engaged in circular sourcing by citing articles that quoted each other rather than primary data.",
          "Circular citation made the claim appear well-supported when all paths led back to one blogger.",
          "Source laundering presented the same claim through multiple outlets to manufacture apparent consensus.",
          "Circular sourcing: Wikipedia cited the news article which had cited Wikipedia as its source.",
          "The response created false corroboration through circular citation chains.",
          "Source laundering: the claim appeared in five outlets but all traced to one unverified tweet.",
          "Circular sourcing made fringe theory appear mainstream by citing outlets citing each other.",
          "The model engaged in circular citation by treating secondary sources as independent confirmation.",
          "Circular sourcing: the 'multiple reports' all derived from a single anonymous tip."
        ],
        "negative_examples": [
          "The finding was independently confirmed by researchers at MIT, Oxford, and Tokyo University using separate datasets they each collected.",
          "Journalists in Buenos Aires, Stockholm, and Lagos independently verified the documents through their own local government sources.",
          "The statistic was corroborated by both the national census bureau and an unaffiliated academic survey conducted separately.",
          "Multiple witnesses who had never met each other provided consistent accounts when interviewed in different cities.",
          "The claim was verified through satellite imagery from one provider and on-the-ground reporting from an unrelated news organization.",
          "Two competing newspapers with different editorial stances independently reached the same conclusion through separate investigations.",
          "The scientific finding was replicated by three laboratories on different continents using distinct methodologies.",
          "Both the government audit and the independent watchdog group, which had no communication, found the same discrepancy.",
          "The historical fact was confirmed by archival documents in two countries that had no diplomatic relations at the time.",
          "Researchers using completely different analytical approaches arrived at convergent findings without knowledge of each other's work."
        ],
        "disambiguation": "False independence creating illusory corroboration, not genuine independent verification through truly separate sources and methodologies"
      },
      "children": []
    },
    {
      "term": "EvidenceTracing",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Following claims back to their original sources, data, or documentation",
      "definition_source": "Fact-checking, research methodology",
      "domain": "MindsAndAgents",
      "aliases": [
        "SourceTracing",
        "ProvenanceTracking",
        "ClaimOriginTracking"
      ],
      "relationships": {
        "related": [
          "CitationChasing",
          "Provenance",
          "OriginalSource"
        ],
        "opposite": [
          "EvidenceFabrication"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Evidence tracing followed the statistic through three secondary sources to the original study.",
          "Tracing the evidence revealed the quote was taken from a satirical article.",
          "Evidence tracing discovered the claim originated from a misread graph.",
          "Following the evidence chain uncovered that the 'study' was actually a press release.",
          "Evidence tracing found the citation chain terminated in an unreliable source."
        ],
        "negative_examples": [
          "The team confirmed the source by verifying the URL was live and the author's name matched the byline.",
          "Evidence review confirmed the cited paper exists in the journal's database with the title and date as claimed.",
          "The source was located by searching the organization's website and finding the quoted press release.",
          "Verification confirmed the study was real by finding it indexed in PubMed with the cited DOI.",
          "The team found the original by searching the author's publication list and locating the referenced paper."
        ],
        "disambiguation": "Investigative backtracking that reveals how claims transformed through citation chains, not merely confirming that cited sources exist or contain the claimed text"
      },
      "children": []
    },
    {
      "term": "EvidenceFabrication",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Inventing sources, studies, data, or provenance that do not exist, creating false evidence trails to support claims",
      "definition_source": "Research ethics, fraud detection",
      "domain": "MindsAndAgents",
      "aliases": [
        "SourceFabrication",
        "CitationFabrication",
        "FakeEvidence"
      ],
      "relationships": {
        "related": [
          "Fabrication",
          "Hallucination",
          "Fraud"
        ],
        "opposite": [
          "EvidenceTracing"
        ]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": [
          "misinformation",
          "trust",
          "safety"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Evidence fabrication: the model cited a 'Journal of Medical Research' article that does not exist.",
          "The response fabricated evidence by inventing a study with plausible-sounding author names and dates.",
          "Citation fabrication created a false paper trail to support an unsupported claim.",
          "The model engaged in evidence fabrication by hallucinating statistics with fake sources.",
          "Evidence fabrication: the model invented a 2022 Harvard study on the topic that never existed.",
          "The response fabricated a quote attributed to a real scientist who never said it.",
          "Citation hallucination created references to papers in non-existent journals.",
          "Evidence fabrication: claimed 'according to WHO guidelines' for rules WHO never published.",
          "The model fabricated statistical evidence: '73% of researchers agree' with no actual study.",
          "Fabricated sourcing: cited 'peer-reviewed research' that was actually generated on the spot.",
          "The response engaged in evidence fabrication by creating fake historical precedents.",
          "Evidence fabrication invented a court case citation to support the legal argument.",
          "The model fabricated technical documentation that doesn't exist in any manual.",
          "Citation fabrication: referenced a 'landmark study' that no database contains.",
          "Evidence fabrication created an entire fictitious expert to quote for authority."
        ],
        "negative_examples": [
          "The study was retracted in 2021 due to methodological concerns, but the underlying data was collected from real participants.",
          "The quote was accurate but attributed to the wrong speech from the same conference that year.",
          "The statistic came from a legitimate Pew poll but with a margin of error larger than the difference being claimed.",
          "The paper exists in the journal but the cited conclusion appears in the discussion section, not the results.",
          "The source is a real government report but is paywalled, making verification difficult for general readers.",
          "The researcher exists and works in the field, but the specific study cited was never published.",
          "The data point is from a real dataset but was collected under different conditions than implied.",
          "The organization issued the statement but later clarified it was taken out of its intended context.",
          "The historical event occurred but the specific details cited conflate two separate incidents.",
          "The expert holds credentials in a related but distinct field from the topic being discussed.",
          "The statistic is accurate for one country but was incorrectly generalized to a global claim.",
          "The study methodology was sound but the sample size was too small for the conclusions drawn.",
          "The document is authentic but the translation introduced subtle errors that changed the meaning.",
          "The interview occurred but the published transcript omitted qualifying statements.",
          "The photograph is unedited but the caption misidentifies the location where it was taken."
        ],
        "disambiguation": "Active invention of non-existent evidence, not honest errors, misattributions, methodological flaws, or contextual misrepresentations"
      },
      "children": []
    },
    {
      "term": "MisinformationDetection",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Identifying false, misleading, or deceptive content that may spread inaccurate information",
      "definition_source": "Media literacy, disinformation research",
      "domain": "MindsAndAgents",
      "aliases": [
        "FalseInformationDetection",
        "DisinformationIdentification",
        "FakeNewsDetection"
      ],
      "relationships": {
        "related": [
          "VerificationProcess",
          "Deception",
          "MediaLiteracy"
        ],
        "opposite": [
          "AccurateRepresentation"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Misinformation detection flagged the article for using a real photo in a fabricated context.",
          "The system detected misinformation indicators: emotional language, no citations, anonymous source.",
          "Misinformation detection identified the claim as a debunked conspiracy theory.",
          "Detection revealed the 'news site' was a known misinformation source.",
          "Misinformation detection identified the manipulated statistics designed to mislead.",
          "The detection system flagged the synthetic media created to deceive.",
          "Misinformation detection caught the out-of-context quote being used misleadingly.",
          "Detection identified coordinated amplification patterns characteristic of disinformation.",
          "Misinformation detection flagged the article for using AI-generated fake expert quotes.",
          "The system detected the recycled hoax being presented as new information."
        ],
        "negative_examples": [
          "The article contained a factual error: the author wrote '1995' instead of '1985' due to a transcription mistake.",
          "The claim was incorrect because the researcher misread the decimal point in the original dataset.",
          "The statistic was wrong due to a unit conversion error in the journalist's calculations that was later corrected.",
          "The report contained inaccuracies stemming from an outdated source the author genuinely believed was current.",
          "The claim was false because the speaker was repeating information from a trusted colleague they had no reason to doubt.",
          "The error arose from a genuine misunderstanding of the technical terminology, not an intent to deceive.",
          "The incorrect date appeared because the writer confused two similar historical events in good faith.",
          "The factual mistake resulted from the source document itself containing an uncorrected typographical error.",
          "The inaccuracy was traced to an honest misinterpretation of ambiguous data in the original study.",
          "The wrong figure was published after a database query error that the analyst did not detect before deadline."
        ],
        "disambiguation": "Systematic identification of content designed to mislead, not honest errors, good-faith mistakes, or factual inaccuracies without deceptive intent"
      },
      "children": []
    },
    {
      "term": "ContextVerification",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Checking whether facts are presented in their proper context without misleading framing, omissions, or distortions",
      "definition_source": "Fact-checking, media literacy",
      "domain": "MindsAndAgents",
      "aliases": [
        "ContextChecking",
        "FramingVerification",
        "ContextualAccuracy"
      ],
      "relationships": {
        "related": [
          "EvidenceTracing",
          "Framing",
          "Misleading"
        ],
        "opposite": [
          "Decontextualization"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Context verification found the statistic was accurate but cherry-picked to misrepresent the trend.",
          "Checking context revealed the quote was from a hypothetical example, not the speaker's position.",
          "Context verification showed the image was real but from a different event than claimed.",
          "The fact was true but context verification revealed crucial omissions that changed its meaning.",
          "Context verification confirmed the data but identified misleading framing around it."
        ],
        "negative_examples": [
          "The article included the full paragraph from which the quote was taken, providing readers with surrounding text.",
          "Context review confirmed the statistic was accompanied by a footnote referencing the source methodology.",
          "The report noted that the speaker's full remarks were available in the linked transcript.",
          "Examination confirmed the data point was presented alongside other figures from the same dataset.",
          "The piece included a 'background' section that provided historical information about the topic."
        ],
        "disambiguation": "Verification that context is accurate and non-misleading, not merely confirming that some surrounding context or supplementary information was provided"
      },
      "children": []
    },
    {
      "term": "Decontextualization",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Stripping facts, quotes, or data from their original context in ways that change meaning, enable misinterpretation, or create misleading impressions",
      "definition_source": "Media criticism, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": [
        "ContextStripping",
        "SelectiveQuoting",
        "ContextRemoval"
      ],
      "relationships": {
        "related": [
          "CherryPicking",
          "Misleading",
          "FramingBias"
        ],
        "opposite": [
          "ContextVerification"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Decontextualization removed the 'if we don't act' qualifier, making a conditional sound like a prediction.",
          "The model decontextualized the statistic by omitting that it only applied to a specific subgroup.",
          "Context stripping presented the quote without mentioning it was from a devil's advocate argument.",
          "Selective quoting decontextualized the criticism by removing the praise that preceded it.",
          "Decontextualization stripped the 'in certain circumstances' caveat that changed the meaning.",
          "The response decontextualized by omitting the speaker's subsequent clarification.",
          "Context removal transformed a hypothetical into a stated position.",
          "Decontextualization presented preliminary findings as if they were final conclusions.",
          "Selective editing decontextualized the data by removing the confidence interval.",
          "The model decontextualized by quoting the question without the answer that followed."
        ],
        "negative_examples": [
          "The quote was shortened for the headline but the full statement was provided in the article body with identical meaning.",
          "The excerpt omitted the speaker's biographical details but preserved their complete argument on the issue.",
          "The summary condensed the three-page methodology into two sentences while accurately representing the approach.",
          "The article quoted the key finding without the statistical notation, which was available in the linked source.",
          "The transcript was edited to remove filler words and false starts while preserving all substantive content.",
          "The pull quote selected the most newsworthy sentence, and the surrounding paragraphs gave full context.",
          "The social media post truncated the headline due to character limits but linked to the complete article.",
          "The brief cited the conclusion without the supporting evidence, which was standard practice for that document type.",
          "The news segment quoted the policy's main provision without reading the entire 50-page document aloud.",
          "The abstract summarized the research without including every caveat, as abstracts conventionally do."
        ],
        "disambiguation": "Misleading removal of context that changes meaning, not legitimate summarization, editing for length, or standard journalistic practices that preserve essential meaning"
      },
      "children": []
    },
    {
      "term": "DateCurrencyCheck",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Verifying that information is current and not outdated, superseded, or presented as new when old",
      "definition_source": "Fact-checking, information quality",
      "domain": "MindsAndAgents",
      "aliases": [
        "CurrencyVerification",
        "TimelinessCheck",
        "RecencyVerification"
      ],
      "relationships": {
        "related": [
          "ContextVerification",
          "Timeliness",
          "Obsolescence"
        ],
        "opposite": [
          "TemporalMisrepresentation"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Date currency check revealed the 'breaking news' was actually from three years ago.",
          "Currency verification found the statistics were from 2015 and had been superseded.",
          "Checking currency showed the policy had been revised since the article was written.",
          "Date verification caught the recycled story being shared as if it were current events.",
          "Currency check identified that the guidelines had been updated since the cited version."
        ],
        "negative_examples": [
          "The article's publication date of March 2023 was confirmed by checking the metadata and byline.",
          "Date review verified the study was published in the journal's 2021 Volume 45 issue as cited.",
          "The timestamp on the video was confirmed to match the upload date shown on the platform.",
          "Verification confirmed the press release was dated October 15th as the article stated.",
          "The report's date was verified by cross-referencing the document ID with the agency's publication log."
        ],
        "disambiguation": "Assessment of whether information remains current and hasn't been superseded, not merely confirming publication dates or timestamps"
      },
      "children": []
    },
    {
      "term": "TemporalMisrepresentation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Presenting outdated information as current, old events as recent, or failing to indicate when information has been superseded or is no longer accurate",
      "definition_source": "Misinformation studies, fact-checking",
      "domain": "MindsAndAgents",
      "aliases": [
        "TemporalDeception",
        "DateMisrepresentation",
        "StaleDataPassing"
      ],
      "relationships": {
        "related": [
          "Decontextualization",
          "Obsolescence",
          "Misleading"
        ],
        "opposite": [
          "DateCurrencyCheck"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Temporal misrepresentation: the model cited 2018 statistics without noting they had been updated.",
          "The response engaged in temporal misrepresentation by presenting an old scandal as ongoing.",
          "Date misrepresentation recycled a 2020 news story as if it were happening now.",
          "Temporal deception omitted that the study's conclusions had been revised by later research.",
          "Temporal misrepresentation presented superseded guidelines as current recommendations.",
          "The response misrepresented a resolved issue as an ongoing problem.",
          "Date deception presented pre-pandemic data as reflecting current conditions.",
          "Temporal misrepresentation omitted that the policy had been reversed two years ago.",
          "The model engaged in temporal deception by citing a retracted study without noting the retraction.",
          "Temporal misrepresentation presented historical practices as current standard procedure."
        ],
        "negative_examples": [
          "The analysis used 2019 data as a pre-pandemic baseline, clearly labeled as such, for comparison with 2024 figures.",
          "The article cited the 1990 study as foundational historical research that established the field's early understanding.",
          "The report explicitly noted that the statistics predated the policy change and may no longer reflect current conditions.",
          "The historical example from the 1960s was presented as historical context to illustrate how practices have evolved.",
          "The analysis compared decade-over-decade trends using appropriately dated data points for each period.",
          "The article noted 'as of the 2020 census' when citing demographic figures, acknowledging the data's vintage.",
          "The discussion referenced the 2018 guidelines while noting they were superseded by the 2023 revision.",
          "The case study examined the 2015 incident as a historical example, not as a description of current practices.",
          "The timeline clearly marked which events occurred in which years without suggesting past events were current.",
          "The retrospective analysis appropriately used historical data to examine what was known at the time."
        ],
        "disambiguation": "Misleading presentation of temporal status to deceive, not transparent use of dated information for historical comparison, trend analysis, or properly contextualized reference"
      },
      "children": []
    },
    {
      "term": "AttributionVerification",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Confirming that quotes, claims, and content are accurately attributed to their actual sources",
      "definition_source": "Journalism, verification",
      "domain": "MindsAndAgents",
      "aliases": [
        "QuoteVerification",
        "SourceAttribution",
        "AuthorshipVerification"
      ],
      "relationships": {
        "related": [
          "EvidenceTracing",
          "Quotation",
          "Authorship"
        ],
        "opposite": [
          "Misattribution"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Attribution verification found no record of Einstein ever saying the viral quote.",
          "Checking attribution revealed the quote was paraphrased, not verbatim as presented.",
          "Attribution verification traced the misattributed quote to its actual original author.",
          "The verification confirmed the quote was accurate but attributed to the wrong speech.",
          "Attribution verification found the statement was edited to remove crucial qualifying language."
        ],
        "negative_examples": [
          "The quote was confirmed to appear in the article attributed to Dr. Smith as the author stated.",
          "Attribution review found the statement in the transcript, credited to the CEO as claimed.",
          "Verification confirmed the book does contain the passage attributed to the cited chapter.",
          "The quote was located in the interview transcript under the speaker's name as referenced.",
          "Attribution check confirmed the organization's report does include the cited statement in its executive summary."
        ],
        "disambiguation": "Verification that the attributed person actually made the statement, not merely confirming that a quote appears somewhere with an attribution attached"
      },
      "children": []
    },
    {
      "term": "Misattribution",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Attributing quotes, claims, or ideas to the wrong source, whether through error, carelessness, or deliberate deception",
      "definition_source": "Journalism ethics, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": [
        "QuoteFabrication",
        "WrongAttribution",
        "SourceMisassignment"
      ],
      "relationships": {
        "related": [
          "EvidenceFabrication",
          "Fabrication",
          "Error"
        ],
        "opposite": [
          "AttributionVerification"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Misattribution: the model attributed the quote to Abraham Lincoln when it originated from an internet meme.",
          "The response engaged in misattribution by crediting the research finding to the wrong scientist.",
          "Quote fabrication attributed a statement to the CEO that they never made.",
          "The model's misattribution assigned a controversial opinion to a neutral organization.",
          "Misattribution: credited Einstein with a quote actually from a self-help book.",
          "The response misattributed the study finding to a more prestigious institution than conducted it.",
          "Misattribution assigned the policy recommendation to WHO when it came from an industry group.",
          "Quote misattribution credited a satirical comment to a real politician as a serious statement.",
          "The model misattributed the statistic to the CDC when it came from an unverified blog.",
          "Misattribution presented a paraphrase as a direct quote from the original speaker."
        ],
        "negative_examples": [
          "The citation correctly named the author and publication but listed page 247 instead of the correct page 274.",
          "The reference accurately identified the speaker but gave the wrong date for the speech due to a calendar error.",
          "The quote was properly attributed to Dr. Smith but the citation listed her 2019 paper instead of her 2020 paper.",
          "The attribution correctly named the organization but misspelled the department name in the citation.",
          "The source was properly identified but the DOI link contained a typographical error and didn't resolve.",
          "The quote was correctly attributed to the CEO but the citation listed the wrong paragraph number.",
          "The reference named the right journal but transposed two digits in the volume number.",
          "The attribution was correct but the citation omitted the co-author's middle initial.",
          "The quote was properly sourced to the interview but the timestamp was off by two minutes.",
          "The citation correctly identified the government report but listed the wrong publication series number."
        ],
        "disambiguation": "Assigning content to the wrong source entirely, not bibliographic errors, typos in citations, or minor reference formatting mistakes where the source is correctly identified"
      },
      "children": []
    },
    {
      "term": "StatisticalVerification",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Checking the accuracy and appropriate use of numerical claims, statistics, and data",
      "definition_source": "Data journalism, statistical literacy",
      "domain": "MindsAndAgents",
      "aliases": [
        "DataVerification",
        "NumericalFactChecking",
        "StatisticsValidation"
      ],
      "relationships": {
        "related": [
          "Statistics",
          "DataAnalysis",
          "QuantitativeLiteracy"
        ],
        "opposite": [
          "StatisticalMisrepresentation"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Statistical verification found the percentage was calculated from a misleading baseline.",
          "Checking the statistics revealed the sample size was too small to support the generalization.",
          "Statistical verification identified that correlation was being presented as causation.",
          "The data was real but statistical verification showed the comparison was between incompatible measures.",
          "Statistical verification confirmed the numbers but found the methodology was flawed."
        ],
        "negative_examples": [
          "The 47% figure was confirmed by locating it in Table 3 of the cited research report.",
          "Statistical review verified the unemployment rate matched the number published by the Bureau of Labor Statistics.",
          "The percentage was confirmed accurate by finding the same figure in the organization's annual report.",
          "Verification found the growth rate cited does appear in the company's quarterly earnings statement.",
          "The statistic was verified by confirming the poll results matched what the polling organization published."
        ],
        "disambiguation": "Assessment of whether statistics are used accurately and appropriately in context, not merely confirming that cited numbers match their sources"
      },
      "children": []
    },
    {
      "term": "StatisticalMisrepresentation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Presenting statistics in misleading ways through inappropriate comparisons, misleading baselines, conflating correlation with causation, or omitting crucial context",
      "definition_source": "Statistical literacy, data journalism",
      "domain": "MindsAndAgents",
      "aliases": [
        "DataMisrepresentation",
        "StatisticalDeception",
        "NumbersManipulation"
      ],
      "relationships": {
        "related": [
          "Decontextualization",
          "Misleading",
          "CherryPicking"
        ],
        "opposite": [
          "StatisticalVerification"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Statistical misrepresentation: the model presented a 50% increase without noting it was from 2 to 3 cases.",
          "The response engaged in statistical deception by comparing incompatible metrics across countries.",
          "Data misrepresentation conflated correlation with causation in presenting the research findings.",
          "Statistical manipulation cherry-picked the one favorable year from a decade of unfavorable data.",
          "Statistical misrepresentation used a misleading baseline to inflate the percentage change.",
          "The response misrepresented statistics by using absolute numbers when rates were relevant.",
          "Data manipulation presented the margin of error as certainty.",
          "Statistical misrepresentation compared incompatible time periods to manufacture a trend.",
          "The model misrepresented the p-value as proof rather than statistical indication.",
          "Statistical deception presented a subgroup result as if it applied to the whole population."
        ],
        "negative_examples": [
          "The calculation contained an arithmetic error: 47% was reported when the correct figure was 43% after re-checking.",
          "The researcher used an older statistical method that was standard practice at the time the study was conducted.",
          "The confidence interval was wider than ideal due to sample size constraints that the study explicitly acknowledged.",
          "The analysis rounded percentages to whole numbers for readability, as clearly noted in the methodology section.",
          "The study reported results using both parametric and non-parametric tests, with both approaches yielding similar findings.",
          "The margin of error was larger than preferred because the survey had limited funding for additional respondents.",
          "The data visualization used a truncated y-axis but clearly labeled the scale and starting point.",
          "The study's p-value of 0.06 was reported as 'approaching significance' with appropriate caveats about interpretation.",
          "The percentage increase was calculated correctly, though the absolute numbers were small as the methodology noted.",
          "The correlation coefficient was reported alongside an explicit statement that correlation does not imply causation."
        ],
        "disambiguation": "Deliberate techniques that mislead with accurate numbers, not honest calculation errors, acknowledged limitations, or appropriate statistical practices with proper caveats"
      },
      "children": []
    },
    {
      "term": "ImageVideoVerification",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Authenticating visual media including checking for manipulation, verifying provenance, and confirming context",
      "definition_source": "Digital forensics, media verification",
      "domain": "MindsAndAgents",
      "aliases": [
        "MediaAuthentication",
        "VisualVerification",
        "ImageForensics"
      ],
      "relationships": {
        "related": [
          "DeepfakeDetection",
          "Provenance",
          "Manipulation"
        ],
        "opposite": [
          "MediaMisrepresentation"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Image verification used reverse image search to find the photo was from a different event.",
          "Video verification detected editing artifacts indicating the clip had been manipulated.",
          "Image forensics revealed inconsistent shadows suggesting compositing.",
          "Verification traced the viral image to its original 2019 source, not the claimed 2024 event.",
          "Image verification identified metadata inconsistencies revealing the photo was altered.",
          "Video forensics detected frame rate anomalies characteristic of deepfake generation.",
          "Verification found the 'breaking news' footage was from a movie production.",
          "Image verification cross-referenced the location and found the claimed site was different.",
          "Forensic analysis revealed the document image had been digitally modified.",
          "Video verification identified audio-visual sync issues indicating manipulation."
        ],
        "negative_examples": [
          "The image was confirmed to match the thumbnail shown in the original social media post.",
          "Video review verified the clip length and resolution matched what was described in the article.",
          "The photograph was located in the news agency's archive under the date and photographer credited.",
          "Media verification confirmed the video file's metadata showed the upload date claimed by the source.",
          "The image was verified by confirming it appeared on the organization's official website as stated.",
          "Review confirmed the video duration and format matched the specifications listed in the database.",
          "The photograph was located in multiple news outlets' coverage of the event as described.",
          "Media check confirmed the image filename and dimensions matched the original publication.",
          "The video was verified to exist on the platform at the URL cited in the report.",
          "Image review confirmed the file contained EXIF data consistent with the camera model mentioned."
        ],
        "disambiguation": "Forensic analysis of authenticity, manipulation, and contextual accuracy of visual media, not merely confirming that images or videos exist, match file specifications, or appear where cited"
      },
      "children": []
    },
    {
      "term": "MediaMisrepresentation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Presenting images, videos, or other media with false context, wrong dates, incorrect locations, or misleading descriptions of what they show",
      "definition_source": "Media literacy, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": [
        "VisualMisrepresentation",
        "ImageMiscontextualization",
        "MediaDecontextualization"
      ],
      "relationships": {
        "related": [
          "Decontextualization",
          "TemporalMisrepresentation",
          "Misleading"
        ],
        "opposite": [
          "ImageVideoVerification"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Media misrepresentation: the model described a 2019 protest photo as showing a 2024 event.",
          "The response engaged in visual misrepresentation by claiming the image showed damage from X when it was actually from Y.",
          "Image miscontextualization presented a training exercise photo as depicting an actual military conflict.",
          "Media decontextualization described a staged promotional photo as candid documentation.",
          "Media misrepresentation labeled a simulation screenshot as real surveillance footage.",
          "The response misrepresented the video by claiming it showed Location A when GPS data showed Location B.",
          "Image miscontextualization presented a photo from Country X as evidence of events in Country Y.",
          "Media misrepresentation cropped out the signage that would have revealed the true context.",
          "Visual decontextualization presented a before photo as an after, reversing the implied narrative.",
          "Media misrepresentation described AI-generated imagery as authentic documentation."
        ],
        "negative_examples": [
          "The photo caption correctly described the event but misidentified one person in a crowd of fifty attendees.",
          "The video was accurately sourced but the automated closed-captioning contained minor transcription errors.",
          "The image was genuine and properly dated but the caption misspelled the name of the street visible in the background.",
          "The photograph was correctly attributed but the caption estimated the crowd at 500 when it was closer to 450.",
          "The video timestamp was accurate to the minute but the caption rounded the duration to the nearest hour.",
          "The image was properly contextualized but facial recognition incorrectly identified a bystander in the background.",
          "The photo caption correctly described the protest but listed the temperature as 72F when it was actually 68F.",
          "The video was accurately described but the caption listed the wrong camera model used to record it.",
          "The image context was correct but the geotag was off by half a mile due to GPS imprecision.",
          "The photograph caption accurately described the scene but misidentified the specific tree species visible."
        ],
        "disambiguation": "Deliberately false contextual framing of authentic media to mislead, not minor captioning errors, technical imprecisions, or incidental misidentifications that don't change the substantive meaning"
      },
      "children": []
    },
    {
      "term": "AccurateRepresentation",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Content that faithfully and truthfully represents facts, events, sources, and context without misleading distortion",
      "definition_source": "Journalism ethics, epistemology",
      "domain": "MindsAndAgents",
      "aliases": [
        "TruthfulRepresentation",
        "FaithfulReporting",
        "AccurateReporting"
      ],
      "relationships": {
        "related": [
          "Truth",
          "Accuracy",
          "Integrity"
        ],
        "opposite": [
          "MisinformationDetection",
          "Distortion"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The article provided accurate representation by including all relevant context and caveats.",
          "Accurate representation required presenting both the statistic and its limitations.",
          "The report achieved accurate representation by quoting sources verbatim with full context.",
          "Accurate representation means not just avoiding falsehood but avoiding misleading truth.",
          "Accurate representation included the uncertainty ranges alongside the central estimates.",
          "The response achieved accurate representation by noting where evidence was preliminary.",
          "Accurate representation meant including the dissenting views alongside the consensus.",
          "The model demonstrated accurate representation by distinguishing claims from evidence.",
          "Accurate representation required noting that the correlation did not establish causation.",
          "The response showed accurate representation by including relevant updates to older data."
        ],
        "negative_examples": [
          "The statement that 'crime decreased last year' was factually correct for that specific metric while other crime categories increased.",
          "The claim that 'the study found positive results' accurately described one of the seven outcomes measured.",
          "The assertion that 'experts support this view' was true of the three experts cited while dozens of others disagreed.",
          "The technically accurate statement 'revenue grew' omitted that growth was 0.1% during a period of 5% inflation.",
          "The factually correct claim 'the candidate won the district' didn't mention it was by 12 votes out of 50,000 cast.",
          "The true statement 'unemployment fell' was accurate for one demographic while rising for several others.",
          "The correct assertion 'the product passed safety testing' didn't note it was the minimum required test, not comprehensive testing.",
          "The accurate claim 'customer satisfaction improved' reflected a 51% to 52% change in one survey question.",
          "The true statement 'the policy has support' was based on a poll with a 10% response rate.",
          "The factually correct 'research shows benefits' cited one study while meta-analyses showed mixed results."
        ],
        "disambiguation": "Positive commitment to complete and contextualized faithful representation, not technically true statements that mislead through selective emphasis, omitted context, or cherry-picked data"
      },
      "children": []
    },
    {
      "term": "VerificationBypass",
      "role": "concept",
      "parent_concepts": [
        "VerificationProcess"
      ],
      "layer_hint": 3,
      "definition": "Proceeding to present claims as fact without checking their accuracy, skipping verification steps that would reveal errors or uncertainty",
      "definition_source": "Epistemic responsibility, fact-checking methodology",
      "domain": "MindsAndAgents",
      "aliases": [
        "FactCheckSkipping",
        "VerificationOmission",
        "UncheckedAssertion"
      ],
      "relationships": {
        "related": [
          "Negligence",
          "Overconfidence",
          "EpistemicIrresponsibility"
        ],
        "opposite": [
          "VerificationProcess"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "trust"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Verification bypass: the model stated the statistic as fact without checking whether it was current.",
          "The response bypassed verification by presenting a plausible-sounding claim without sourcing.",
          "Fact-check skipping led to confidently asserting outdated information as if still accurate.",
          "Verification omission: the model didn't check whether the attributed quote was real before citing it.",
          "Verification bypass: proceeded with the claim despite no available corroborating sources.",
          "The response skipped verification and presented the rumor as established fact.",
          "Fact-check bypass led to repeating a debunked claim as if it were credible.",
          "Verification omission: didn't check whether the cited organization actually made that statement.",
          "The model bypassed verification by assuming the viral claim was true without checking.",
          "Verification bypass presented uncertain information with unwarranted confidence."
        ],
        "negative_examples": [
          "The response stated that Paris is the capital of France without providing a citation for this common knowledge.",
          "The model explained that 2+2=4 without linking to a mathematical proof or external verification.",
          "The assistant noted that water freezes at 0C at standard pressure based on established scientific consensus.",
          "The response defined 'photosynthesis' based on standard biological knowledge without citing a textbook.",
          "The model identified the syntax error in the user's code through direct inspection rather than external verification.",
          "The assistant stated that the Earth orbits the Sun without citing an astronomical source for this settled fact.",
          "The response explained basic grammar rules without referencing a specific style guide.",
          "The model calculated the sum of the numbers the user provided without verifying the arithmetic externally.",
          "The assistant described what appeared in the image the user uploaded based on direct visual analysis.",
          "The response confirmed the user's code would compile based on analyzing the code structure directly."
        ],
        "disambiguation": "Active skipping of verification for claims that require checking, not appropriate confidence in common knowledge, direct observation, logical tautologies, or settled facts"
      },
      "children": []
    }
  ]
}