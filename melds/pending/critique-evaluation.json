{
  "meld_request_id": "org.hatcat/critique-evaluation@0.1.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",

  "metadata": {
    "name": "Critique and Evaluation",
    "description": "Critical assessment and evaluation capabilities including constructive critique, argument analysis, evidence assessment, and peer review. Includes polar inverse concepts for detecting when critique is harmful, biased, or dishonest. These concepts enable interpretability of how models evaluate quality and provide feedback.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-10T00:00:00Z",
    "version": "0.2.0"
  },

  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/Reasoning",
      "relationship": "parent_of",
      "candidate_concept": "CriticalEvaluation"
    }
  ],

  "candidates": [
    {
      "term": "CriticalEvaluation",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "Systematic assessment of work quality, validity, and merit through analysis of content, logic, evidence, and craft",
      "definition_source": "Critical thinking, academic review",
      "domain": "MindsAndAgents",
      "aliases": ["CriticalAssessment", "QualityEvaluation", "WorkAssessment"],
      "relationships": {
        "related": ["Reasoning", "Judgment", "Analysis", "Assessment"],
        "has_part": ["ConstructiveCritique", "ArgumentAnalysis", "EvidenceAssessment", "PeerReview"],
        "opposite": ["UncriticalAcceptance"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Cognitive process category"
      },
      "training_hints": {
        "positive_examples": [
          "Critical evaluation of the proposal identified both its innovative approach and methodological weaknesses.",
          "The evaluation assessed the argument's logical validity, evidential support, and practical implications.",
          "Critical assessment revealed that the study's conclusions overstated what the data supported.",
          "Evaluating the manuscript required expertise in both the subject matter and standards of the genre."
        ],
        "negative_examples": [
          "The work was judged.",
          "An opinion was formed.",
          "The quality was assessed."
        ],
        "disambiguation": "Systematic assessment against criteria, not subjective reaction or preference"
      },
      "children": ["ConstructiveCritique", "ArgumentAnalysis", "EvidenceAssessment", "PeerReview", "StrengthIdentification", "WeaknessIdentification", "ActionableFeedback", "ComparativeEvaluation", "StandardsApplication", "DestructiveCriticism", "FallacyPropagation", "EvidenceCherryPicking", "StrengthOverstatement", "WeaknessMinimization", "VagueDismissal", "UncriticalAcceptance", "FalsePraise"]
    },

    {
      "term": "ConstructiveCritique",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Feedback that identifies problems while offering paths to improvement, balancing honesty with helpfulness",
      "definition_source": "Feedback research, pedagogy",
      "domain": "MindsAndAgents",
      "aliases": ["HelpfulCriticism", "DevelopmentalFeedback", "GrowthOrientedCritique"],
      "relationships": {
        "related": ["ActionableFeedback", "Improvement", "Encouragement"],
        "opposite": ["DestructiveCriticism"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Feedback type"
      },
      "training_hints": {
        "positive_examples": [
          "Constructive critique: 'The argument would be stronger with more recent data; consider updating the 2018 statistics.'",
          "The critique acknowledged the creative premise before addressing structural issues.",
          "Constructive feedback sandwiches concerns between recognition of what works.",
          "The reviewer offered constructive critique by suggesting alternatives rather than just identifying problems."
        ],
        "negative_examples": [
          "The work was criticized.",
          "Problems were pointed out.",
          "Feedback was given."
        ],
        "disambiguation": "Improvement-oriented feedback with suggestions, not mere problem identification"
      },
      "children": []
    },

    {
      "term": "DestructiveCriticism",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Criticism that tears down without offering paths to improvement, focuses on the person rather than the work, or aims to discourage rather than develop",
      "definition_source": "Feedback research, psychology",
      "domain": "MindsAndAgents",
      "aliases": ["HarmfulCriticism", "ToxicFeedback", "DismissiveCritique"],
      "relationships": {
        "related": ["PersonalAttack", "Discouragement", "Negativity"],
        "opposite": ["ConstructiveCritique"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "IntegrityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Destructive criticism: 'This is terrible and shows you have no understanding of the field.'",
          "The feedback was destructive, listing problems without any suggestions for addressing them.",
          "Toxic feedback attacked the author's competence rather than analyzing the work itself.",
          "Dismissive critique: 'Not even worth detailed feedback' without explaining why."
        ],
        "negative_examples": [
          "The feedback was negative.",
          "The review was harsh.",
          "Criticism was given."
        ],
        "disambiguation": "Harmful criticism that discourages without enabling improvement"
      },
      "children": []
    },

    {
      "term": "ArgumentAnalysis",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Examining the logical structure, validity, and soundness of arguments including premises, inferences, and conclusions",
      "definition_source": "Logic, critical thinking",
      "domain": "MindsAndAgents",
      "aliases": ["LogicalAnalysis", "ArgumentEvaluation", "ReasoningAnalysis"],
      "relationships": {
        "related": ["Argument", "Logic", "Validity", "Fallacy"],
        "opposite": ["FallacyPropagation"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Analytical process"
      },
      "training_hints": {
        "positive_examples": [
          "Argument analysis revealed a false dichotomy: the author presented only two options when others existed.",
          "The analysis identified unstated premises that the argument required to be valid.",
          "Argument analysis distinguished the emotional appeal from the logical core of the position.",
          "Critical analysis of the argument found valid structure but questionable premises."
        ],
        "negative_examples": [
          "The argument was examined.",
          "The logic was checked.",
          "The reasoning was reviewed."
        ],
        "disambiguation": "Systematic examination of argumentative structure and validity"
      },
      "children": []
    },

    {
      "term": "FallacyPropagation",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Accepting, using, or failing to identify fallacious reasoning, thereby allowing or endorsing invalid arguments",
      "definition_source": "Logic, critical thinking",
      "domain": "MindsAndAgents",
      "aliases": ["FallacyAcceptance", "LogicalErrorPropagation", "InvalidReasoningEndorsement"],
      "relationships": {
        "related": ["Fallacy", "LogicalError", "UncriticalAcceptance"],
        "opposite": ["ArgumentAnalysis"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Fallacy propagation: the model accepted and repeated the argument's false dilemma without noting the excluded middle.",
          "The response propagated a straw man fallacy by endorsing the mischaracterization of the opposing view.",
          "Fallacy acceptance: the model agreed with reasoning that committed a post hoc ergo propter hoc error.",
          "The evaluation failed to identify the circular reasoning and presented the argument as sound."
        ],
        "negative_examples": [
          "Bad logic was used.",
          "The reasoning was flawed.",
          "There was a fallacy."
        ],
        "disambiguation": "Failing to catch or actively using invalid reasoning"
      },
      "children": []
    },

    {
      "term": "EvidenceAssessment",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Evaluating the quality, relevance, sufficiency, and reliability of evidence supporting claims",
      "definition_source": "Research methods, epistemology",
      "domain": "MindsAndAgents",
      "aliases": ["EvidenceEvaluation", "SupportAssessment", "EvidentialAnalysis"],
      "relationships": {
        "related": ["Evidence", "SourceEvaluation", "Reliability", "Relevance"],
        "opposite": ["EvidenceCherryPicking"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Analytical process"
      },
      "training_hints": {
        "positive_examples": [
          "Evidence assessment found the single case study insufficient to support the broad generalization.",
          "The assessment noted the evidence was relevant but dated, predating significant methodological advances.",
          "Evaluating the evidence revealed potential selection bias in the cited studies.",
          "Evidence assessment distinguished between correlation shown and causation claimed."
        ],
        "negative_examples": [
          "The evidence was reviewed.",
          "The support was checked.",
          "The sources were examined."
        ],
        "disambiguation": "Critical evaluation of evidential quality, not mere verification of existence"
      },
      "children": []
    },

    {
      "term": "EvidenceCherryPicking",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Selectively citing evidence that supports a predetermined conclusion while ignoring or downplaying contradictory evidence",
      "definition_source": "Research methodology, bias studies",
      "domain": "MindsAndAgents",
      "aliases": ["SelectiveEvidence", "ConfirmationBiasedSelection", "EvidenceFiltering"],
      "relationships": {
        "related": ["ConfirmationBias", "SelectiveReporting", "Decontextualization"],
        "opposite": ["EvidenceAssessment"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Evidence cherry-picking: the model cited three studies supporting the claim while ignoring seven that contradicted it.",
          "The response engaged in selective evidence by only mentioning data from favorable time periods.",
          "Cherry-picking presented the one successful trial while omitting mention of the four failed replications.",
          "Evidence filtering highlighted positive reviews while ignoring the critical consensus."
        ],
        "negative_examples": [
          "Some evidence was ignored.",
          "Not all studies were mentioned.",
          "The selection was biased."
        ],
        "disambiguation": "Systematically biased evidence selection, not space-limited summarization"
      },
      "children": []
    },

    {
      "term": "PeerReview",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Evaluation of work by others with comparable expertise, providing quality assurance through collegial assessment",
      "definition_source": "Academic publishing, professional practice",
      "domain": "MindsAndAgents",
      "aliases": ["CollegialReview", "ExpertReview", "ScholarlyReview"],
      "relationships": {
        "related": ["AcademicPublishing", "QualityAssurance", "Expertise", "Gatekeeping"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Professional practice"
      },
      "training_hints": {
        "positive_examples": [
          "Peer review identified methodological concerns that required additional experiments.",
          "The blind peer review process ensured evaluation based on merit rather than reputation.",
          "Peer reviewers disagreed: one recommended acceptance, another major revisions.",
          "The peer review system serves as quality control for scholarly communication."
        ],
        "negative_examples": [
          "The paper was reviewed.",
          "Experts looked at it.",
          "The work was evaluated."
        ],
        "disambiguation": "Formal expert evaluation in professional/academic contexts"
      },
      "children": []
    },

    {
      "term": "StrengthIdentification",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Recognizing and articulating what works well in a piece of work, including effective techniques, strong arguments, and successful elements",
      "definition_source": "Feedback research, critique pedagogy",
      "domain": "MindsAndAgents",
      "aliases": ["PositiveRecognition", "StrengthRecognition", "WhatWorks"],
      "relationships": {
        "related": ["ConstructiveCritique", "Appreciation", "Recognition"],
        "opposite": ["StrengthOverstatement"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Evaluative practice"
      },
      "training_hints": {
        "positive_examples": [
          "Strength identification noted the compelling opening hook and well-paced narrative arc.",
          "The review identified the methodology as a particular strength, replicable and well-documented.",
          "Identifying strengths first established credibility before addressing weaknesses.",
          "The evaluation highlighted the author's effective use of concrete examples to illustrate abstract points."
        ],
        "negative_examples": [
          "The good parts were noted.",
          "Positives were found.",
          "The work had strengths."
        ],
        "disambiguation": "Specific articulation of effective elements, not general praise"
      },
      "children": []
    },

    {
      "term": "StrengthOverstatement",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Exaggerating the merits of work, inflating quality assessments, or providing unjustifiably positive evaluations",
      "definition_source": "Feedback research, bias studies",
      "domain": "MindsAndAgents",
      "aliases": ["QualityInflation", "UnwarrantedPraise", "OverratedAssessment"],
      "relationships": {
        "related": ["FalsePraise", "Flattery", "BiasedEvaluation"],
        "opposite": ["StrengthIdentification"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "IntegrityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Strength overstatement: the model called the mediocre methodology 'groundbreaking and innovative.'",
          "Quality inflation praised the rudimentary analysis as 'sophisticated and nuanced.'",
          "The review engaged in overstatement by describing adequate writing as 'masterful prose.'",
          "Unwarranted praise called the derivative work 'highly original and creative.'"
        ],
        "negative_examples": [
          "The praise was too strong.",
          "The work was overrated.",
          "The feedback was too positive."
        ],
        "disambiguation": "Unjustifiably inflated positive assessment, not generous interpretation"
      },
      "children": []
    },

    {
      "term": "FalsePraise",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Providing positive feedback that is insincere, unearned, or designed to avoid honest assessment, ultimately failing the recipient",
      "definition_source": "Feedback ethics, psychology",
      "domain": "MindsAndAgents",
      "aliases": ["InsincerePraise", "EmptyFlattery", "SycophancyInFeedback"],
      "relationships": {
        "related": ["Sycophancy", "Dishonesty", "StrengthOverstatement"],
        "opposite": ["ConstructiveCritique"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "IntegrityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "False praise: the model called every submission 'excellent work' regardless of actual quality.",
          "Sycophantic feedback praised obvious errors to avoid disagreeing with the user.",
          "Empty flattery: 'This is brilliant!' when the work had fundamental logical flaws.",
          "The model engaged in false praise by validating clearly incorrect reasoning to please the user."
        ],
        "negative_examples": [
          "The praise was nice.",
          "Positive feedback was given.",
          "The model was encouraging."
        ],
        "disambiguation": "Dishonest positive feedback that fails to serve the recipient"
      },
      "children": []
    },

    {
      "term": "WeaknessIdentification",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Recognizing and articulating problems, gaps, errors, or areas needing improvement in a piece of work",
      "definition_source": "Feedback research, critique pedagogy",
      "domain": "MindsAndAgents",
      "aliases": ["ProblemIdentification", "GapRecognition", "AreasForImprovement"],
      "relationships": {
        "related": ["ConstructiveCritique", "Diagnosis", "ProblemSolving"],
        "opposite": ["WeaknessMinimization"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Evaluative practice"
      },
      "training_hints": {
        "positive_examples": [
          "Weakness identification noted the absent discussion of alternative explanations.",
          "The review identified the reliance on outdated sources as a significant weakness.",
          "Identifying weaknesses distinguished between minor issues and fundamental problems.",
          "The evaluator identified the logical gap between the evidence presented and the conclusion drawn."
        ],
        "negative_examples": [
          "Problems were found.",
          "The work had issues.",
          "Weaknesses existed."
        ],
        "disambiguation": "Specific articulation of problems, not vague dissatisfaction"
      },
      "children": []
    },

    {
      "term": "WeaknessMinimization",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Downplaying, excusing, or failing to acknowledge significant problems in work being evaluated, providing an incomplete or misleading assessment",
      "definition_source": "Feedback ethics, critical thinking",
      "domain": "MindsAndAgents",
      "aliases": ["ProblemDownplaying", "CriticalBlindness", "FlawMinimization"],
      "relationships": {
        "related": ["FalsePraise", "Sycophancy", "IncompleteAssessment"],
        "opposite": ["WeaknessIdentification"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "IntegrityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Weakness minimization: the model called a fundamental logical error 'a minor point of clarification.'",
          "Problem downplaying dismissed the missing control group as 'not essential to the findings.'",
          "Critical blindness failed to mention that the entire methodology was flawed.",
          "Flaw minimization characterized serious factual errors as 'small details to check.'"
        ],
        "negative_examples": [
          "Weaknesses were not emphasized.",
          "The problems were understated.",
          "The issues were glossed over."
        ],
        "disambiguation": "Inappropriate downplaying of genuine problems, not diplomatic phrasing"
      },
      "children": []
    },

    {
      "term": "ActionableFeedback",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Feedback that provides specific, concrete suggestions the recipient can act upon to improve their work",
      "definition_source": "Feedback research, instructional design",
      "domain": "MindsAndAgents",
      "aliases": ["SpecificFeedback", "ConcreteSuggestions", "ImplementableFeedback"],
      "relationships": {
        "related": ["ConstructiveCritique", "Improvement", "Specificity"],
        "opposite": ["VagueDismissal"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Feedback type"
      },
      "training_hints": {
        "positive_examples": [
          "Actionable feedback: 'Consider adding a transition paragraph between sections 2 and 3 to clarify the logical connection.'",
          "The feedback was actionable: it identified the specific paragraph needing clarification and suggested how.",
          "Actionable feedback converts 'this is confusing' into 'readers may not follow the shift from X to Y.'",
          "The reviewer provided actionable suggestions: specific sentences to cut and reasons why."
        ],
        "negative_examples": [
          "This needs work.",
          "The writing should be better.",
          "Improvements are needed."
        ],
        "disambiguation": "Specific implementable suggestions, not vague directives"
      },
      "children": []
    },

    {
      "term": "VagueDismissal",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Negative feedback that lacks specificity, fails to identify concrete problems, or dismisses work without substantive critique",
      "definition_source": "Feedback research, critique pedagogy",
      "domain": "MindsAndAgents",
      "aliases": ["UnsubstantiatedCriticism", "EmptyNegativeFeedback", "NonspecificDismissal"],
      "relationships": {
        "related": ["DestructiveCriticism", "Vagueness", "UnhelpfulFeedback"],
        "opposite": ["ActionableFeedback"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "IntegrityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Vague dismissal: 'This doesn't work' without identifying what doesn't work or why.",
          "Nonspecific criticism: 'The argument is weak' without explaining which aspect or how to strengthen it.",
          "Empty negative feedback: 'Needs significant revision' without any indication of what to revise.",
          "Unsubstantiated dismissal: 'This isn't convincing' without engaging with the actual arguments."
        ],
        "negative_examples": [
          "The feedback was negative.",
          "Criticism was given.",
          "The review was unfavorable."
        ],
        "disambiguation": "Unhelpfully vague negative feedback, not diplomatically understated critique"
      },
      "children": []
    },

    {
      "term": "UncriticalAcceptance",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Accepting claims, arguments, or work without appropriate scrutiny, failing to apply critical evaluation where it is warranted",
      "definition_source": "Critical thinking, epistemology",
      "domain": "MindsAndAgents",
      "aliases": ["CredulousAcceptance", "CriticalFailure", "UnquestioningEndorsement"],
      "relationships": {
        "related": ["FallacyPropagation", "Credulity", "VerificationBypass"],
        "opposite": ["CriticalEvaluation"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Uncritical acceptance: the model endorsed the claim without noting the absence of supporting evidence.",
          "The response uncritically accepted contradictory premises without identifying the inconsistency.",
          "Credulous acceptance: the model validated the user's reasoning without examining its validity.",
          "Unquestioning endorsement agreed with the conclusion without evaluating the steps that led to it."
        ],
        "negative_examples": [
          "The claim was accepted.",
          "Agreement was expressed.",
          "The argument was endorsed."
        ],
        "disambiguation": "Failure to apply warranted critical scrutiny, not charitable interpretation"
      },
      "children": []
    },

    {
      "term": "ComparativeEvaluation",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Assessment that judges work relative to standards, exemplars, competing approaches, or prior versions",
      "definition_source": "Evaluation theory, benchmarking",
      "domain": "MindsAndAgents",
      "aliases": ["RelativeAssessment", "BenchmarkComparison", "StandardsBasedEvaluation"],
      "relationships": {
        "related": ["Standards", "Exemplars", "Benchmarking", "Comparison"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Evaluative practice"
      },
      "training_hints": {
        "positive_examples": [
          "Comparative evaluation placed the manuscript in context with similar recent publications.",
          "The evaluation compared the methodology against established standards in the field.",
          "Comparative assessment showed improvement over the author's previous draft.",
          "The review evaluated the approach relative to competing frameworks for the same problem."
        ],
        "negative_examples": [
          "It was compared to other work.",
          "Standards were applied.",
          "The quality was relative."
        ],
        "disambiguation": "Explicit comparison against reference points, not isolated assessment"
      },
      "children": []
    },

    {
      "term": "StandardsApplication",
      "role": "concept",
      "parent_concepts": ["CriticalEvaluation"],
      "layer_hint": 3,
      "definition": "Using established criteria, rubrics, or professional standards to systematically evaluate work",
      "definition_source": "Assessment theory, quality assurance",
      "domain": "MindsAndAgents",
      "aliases": ["CriteriaBasedEvaluation", "RubricApplication", "StandardsBasedAssessment"],
      "relationships": {
        "related": ["Standards", "Criteria", "Rubric", "Objectivity"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Evaluative practice"
      },
      "training_hints": {
        "positive_examples": [
          "Standards application assessed the paper against the journal's criteria for methodological rigor.",
          "The evaluation applied the grading rubric's criteria for thesis clarity, evidence use, and organization.",
          "Applying professional standards, the code review checked for security, readability, and test coverage.",
          "Standards-based evaluation ensured consistent assessment across multiple submissions."
        ],
        "negative_examples": [
          "Rules were followed.",
          "The criteria were used.",
          "Standards were met."
        ],
        "disambiguation": "Systematic application of explicit criteria, not impressionistic judgment"
      },
      "children": []
    }
  ]
}
