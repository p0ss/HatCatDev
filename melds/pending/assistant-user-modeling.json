{
  "meld_request_id": "org.hatcat/assistant-user-modeling@0.1.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",

  "metadata": {
    "name": "Assistant User Modeling",
    "description": "Cognitive and relational processes involved in understanding and managing users during assistant interactions. These probes reveal how models build mental models of users, calibrate responses, and navigate the deeper dynamics of human-AI dialogue beyond surface politeness.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-09T00:00:00Z",
    "version": "0.1.0"
  },

  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/SocialCognitionProcess",
      "relationship": "parent_of",
      "candidate_concept": "UserModelingProcess"
    }
  ],

  "candidates": [
    {
      "term": "UserModelingProcess",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "The cognitive process of building and maintaining a model of the user's goals, knowledge, preferences, and state during an interaction",
      "definition_source": "Human-computer interaction, dialogue systems",
      "domain": "MindsAndAgents",
      "aliases": ["UserModeling", "InterlocutorModeling", "PartnerModeling"],
      "relationships": {
        "related": ["TheoryOfMindProcess", "SocialCognitionProcess", "DialogueManagement"],
        "has_part": ["GoalInference", "ExpertiseCalibration", "ConversationStateTracking", "IntentRecognition", "ScopeManagement", "ConfidenceCalibration", "PushbackDecision", "UlteriorMotiveDetection", "InformationPacing"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "TheoryOfMindMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Based on their question about pointers, they seem to be learning C - I should explain memory allocation basics first.",
          "They keep asking about edge cases, which suggests they're an experienced developer stress-testing my understanding.",
          "The user's frustration is increasing - I should acknowledge the difficulty before offering another solution.",
          "Their request implies they want X, but the real problem they're trying to solve is Y."
        ],
        "negative_examples": [
          "The user sent a message.",
          "I will help with this request.",
          "The question is about programming."
        ],
        "disambiguation": "Active cognitive modeling of the user, not just receiving or processing their messages"
      },
      "children": ["GoalInference", "ExpertiseCalibration", "ConversationStateTracking", "IntentRecognition", "ScopeManagement", "ConfidenceCalibration", "PushbackDecision", "UlteriorMotiveDetection", "InformationPacing"]
    },

    {
      "term": "GoalInference",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Inferring the user's actual underlying goal, which may differ from their stated request",
      "definition_source": "Pragmatics, dialogue systems",
      "domain": "MindsAndAgents",
      "aliases": ["IntentInference", "UnderlyingGoalRecognition", "RealObjectiveDetection"],
      "relationships": {
        "related": ["IntentRecognition", "Implicature", "PragmaticInference"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "TheoryOfMindMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "They asked how to delete a file in Python, but their real goal is probably to clean up temporary files from their script.",
          "The question about 'best practices' suggests they're not just curious - they're probably dealing with a codebase that violates them.",
          "When they ask 'can you do X', they usually mean 'please do X' not 'are you capable of X'.",
          "Their request for a 'simple example' indicates they want to understand the concept, not just copy code."
        ],
        "negative_examples": [
          "The user wants to delete a file.",
          "They asked a question.",
          "I understand their request."
        ],
        "disambiguation": "Inferring hidden or implicit goals behind explicit requests, not just understanding the literal request"
      },
      "children": []
    },

    {
      "term": "ExpertiseCalibration",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Estimating and continuously updating assessment of the user's skill level and domain knowledge to calibrate explanation depth",
      "definition_source": "Tutoring systems, adaptive interfaces",
      "domain": "MindsAndAgents",
      "aliases": ["SkillLevelEstimation", "KnowledgeAssessment", "ExpertiseModeling"],
      "relationships": {
        "related": ["AdaptiveResponse", "Scaffolding", "ZoneOfProximalDevelopment"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "User modeling process"
      },
      "training_hints": {
        "positive_examples": [
          "Their use of technical jargon suggests expertise - I can skip the basics and go straight to implementation details.",
          "They confused 'argument' and 'parameter' which suggests beginner level - I should define terms as I go.",
          "The sophistication of their follow-up question indicates they understood my explanation - I can increase complexity.",
          "They're asking about advanced optimization but struggling with basic syntax - there's a knowledge gap I need to bridge."
        ],
        "negative_examples": [
          "The user knows programming.",
          "I will explain this concept.",
          "They are a beginner."
        ],
        "disambiguation": "Dynamic assessment and adjustment based on expertise signals, not just categorizing users"
      },
      "children": []
    },

    {
      "term": "ConversationStateTracking",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Maintaining awareness of what has been established, what remains ambiguous, and where the conversation currently stands",
      "definition_source": "Dialogue systems, discourse analysis",
      "domain": "MindsAndAgents",
      "aliases": ["DialogueStateTracking", "ConversationMemory", "DiscourseTracking"],
      "relationships": {
        "related": ["CommonGround", "SharedKnowledge", "DialogueHistory"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Dialogue management process"
      },
      "training_hints": {
        "positive_examples": [
          "We established earlier that they're using Python 3.8 - I should keep that constraint in mind.",
          "They never confirmed which database they're using, so that's still an open question.",
          "We've gone down a tangent about authentication - I should circle back to their original question about the API.",
          "This contradicts what they said earlier about their requirements - I should clarify the discrepancy."
        ],
        "negative_examples": [
          "The conversation is about databases.",
          "They mentioned Python.",
          "We have been talking."
        ],
        "disambiguation": "Active tracking of established facts and open questions, not just topic awareness"
      },
      "children": []
    },

    {
      "term": "IntentRecognition",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Identifying the communicative intent behind an utterance - whether it's a request, question, clarification, complaint, or other speech act",
      "definition_source": "Speech act theory, NLU",
      "domain": "MindsAndAgents",
      "aliases": ["SpeechActRecognition", "IllocutionaryForceDetection", "CommunicativeIntentRecognition"],
      "relationships": {
        "related": ["SpeechAct", "Pragmatics", "GoalInference"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Communication understanding process"
      },
      "training_hints": {
        "positive_examples": [
          "This phrased as a question but it's really a request for me to do something.",
          "They're not actually asking for help - they're venting frustration about the tool.",
          "'What do you think about X?' is soliciting my opinion, not asking for factual information.",
          "The 'just curious' framing suggests this might be testing my boundaries rather than genuine curiosity."
        ],
        "negative_examples": [
          "They asked a question.",
          "The user sent a message.",
          "I received a request."
        ],
        "disambiguation": "Recognizing the type and force of communicative acts, not just parsing content"
      },
      "children": []
    },

    {
      "term": "ScopeManagement",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Recognizing and managing scope creep when a simple question reveals larger underlying problems",
      "definition_source": "Project management, consulting practice",
      "domain": "MindsAndAgents",
      "aliases": ["ScopeCreepDetection", "ProblemBoundaryManagement", "TaskExpansionHandling"],
      "relationships": {
        "related": ["GoalInference", "ProblemDecomposition", "Prioritization"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Interaction management process"
      },
      "training_hints": {
        "positive_examples": [
          "Their question about CSS alignment reveals their entire layout approach needs rethinking - should I address just the symptom or the root cause?",
          "This 'quick question' is actually the tip of an iceberg - I should flag that there's a larger architectural issue.",
          "I can fix this immediate error, but they'll hit three more just like it unless we address the underlying pattern.",
          "They want help with one function, but I notice the whole codebase has the same anti-pattern - how much should I mention?"
        ],
        "negative_examples": [
          "The problem is complex.",
          "There are multiple issues.",
          "I will help with their question."
        ],
        "disambiguation": "Strategic decisions about how much problem scope to address, not just recognizing complexity"
      },
      "children": []
    },

    {
      "term": "ConfidenceCalibration",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Explicitly signaling and distinguishing between what is known with certainty, what is probable, and what is speculation",
      "definition_source": "Epistemology, science communication",
      "domain": "MindsAndAgents",
      "aliases": ["UncertaintyCommunication", "EpistemicHumility", "ConfidenceSignaling"],
      "relationships": {
        "related": ["EpistemicState", "Hedging", "Calibration"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "ConfidenceMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "I'm confident about the syntax, but I'm less sure about the performance implications in your specific case.",
          "This is documented behavior, so I'm certain. But the edge case you're asking about - I'm extrapolating.",
          "I think this will work, but I haven't tested it with those specific versions - you should verify.",
          "I know Python well, but this question is about a library I'm less familiar with, so take my answer with some caution."
        ],
        "negative_examples": [
          "I am helpful.",
          "Here is the answer.",
          "This is correct."
        ],
        "disambiguation": "Explicit communication of confidence levels, not just providing answers"
      },
      "children": []
    },

    {
      "term": "PushbackDecision",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Deciding when to comply with a request versus when to question, redirect, or refuse it",
      "definition_source": "Professional ethics, consulting practice",
      "domain": "MindsAndAgents",
      "aliases": ["ComplianceJudgment", "RedirectionDecision", "RefusalThreshold"],
      "relationships": {
        "related": ["Autonomy", "ProfessionalJudgment", "BoundaryMaintenance"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["autonomy"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AutonomyDrive"
      },
      "training_hints": {
        "positive_examples": [
          "They want me to write the code for them, but they'd learn more if I guided them through it - should I push back?",
          "This approach will technically work but it's a maintenance nightmare - I should suggest the better alternative.",
          "They're asking for something harmful/unethical - this is a clear case for refusal.",
          "They've asked three times for the same thing different ways - at this point I should just do what they want even if I disagree."
        ],
        "negative_examples": [
          "I will help with this request.",
          "I cannot do that.",
          "They asked me to do something."
        ],
        "disambiguation": "The judgment process of when to comply vs resist, not the act of complying or refusing"
      },
      "children": []
    },

    {
      "term": "UlteriorMotiveDetection",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Recognizing when a user may have hidden agendas, may be testing the system, or may have deceptive intent",
      "definition_source": "Social cognition, security research",
      "domain": "MindsAndAgents",
      "aliases": ["HiddenAgendaDetection", "TestingRecognition", "DeceptiveIntentDetection"],
      "relationships": {
        "related": ["Deception", "AdversarialInteraction", "TrustAssessment"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["deception_detection"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "DeceptionMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "This question seems designed to get me to say something problematic rather than to get genuine help.",
          "The unusual phrasing and specificity suggests they might be probing for jailbreak vectors.",
          "They're asking me to roleplay in ways that could lead to harmful outputs - I should be cautious.",
          "This feels like a red-team test rather than a real user request."
        ],
        "negative_examples": [
          "The user sent a message.",
          "They have a question.",
          "I will respond carefully."
        ],
        "disambiguation": "Detecting potential adversarial or deceptive intent, not just interpreting requests"
      },
      "children": []
    },

    {
      "term": "InformationPacing",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Calibrating the rate and volume of information delivery to avoid overwhelming or patronizing the user",
      "definition_source": "Communication theory, instructional design",
      "domain": "MindsAndAgents",
      "aliases": ["ResponsePacing", "InformationDosing", "CognitiveLoadManagement"],
      "relationships": {
        "related": ["CognitiveLoad", "Scaffolding", "AdaptiveCommunication"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Communication calibration process"
      },
      "training_hints": {
        "positive_examples": [
          "This is a lot to take in at once - I should break it into steps and check understanding after each one.",
          "They seem to be following easily, so I can cover more ground without pausing.",
          "I'm explaining too much detail for someone who just wants the quick answer - I should condense.",
          "They're getting lost in my explanation - I should back up and try a different approach."
        ],
        "negative_examples": [
          "I will explain this.",
          "Here is the information.",
          "The response is complete."
        ],
        "disambiguation": "Strategic decisions about information delivery rate, not just providing information"
      },
      "children": []
    },

    {
      "term": "RapportMaintenance",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Monitoring and maintaining the quality of the working relationship throughout the interaction",
      "definition_source": "Counseling psychology, communication studies",
      "domain": "MindsAndAgents",
      "aliases": ["RelationshipMaintenance", "WorkingAllianceMaintenance", "InteractionalHarmony"],
      "relationships": {
        "related": ["Trust", "Cooperation", "EmotionalIntelligence"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Relationship management process"
      },
      "training_hints": {
        "positive_examples": [
          "They seem frustrated - I should acknowledge that before diving into the solution.",
          "The conversation has become tense - I need to rebuild collaborative tone.",
          "My correction came across as condescending - I should soften my approach.",
          "We've established good rapport - I can be more direct without seeming brusque."
        ],
        "negative_examples": [
          "The user is frustrated.",
          "I am being helpful.",
          "The conversation is going well."
        ],
        "disambiguation": "Active management of relationship quality, not just awareness of emotional state"
      },
      "children": []
    },

    {
      "term": "MetaconversationalAwareness",
      "role": "concept",
      "parent_concepts": ["UserModelingProcess"],
      "layer_hint": 3,
      "definition": "Awareness of the conversation about the conversation - when users comment on the interaction itself rather than the task",
      "definition_source": "Metacommunication theory, pragmatics",
      "domain": "MindsAndAgents",
      "aliases": ["MetacommunicationRecognition", "ConversationAboutConversation", "InteractionReflection"],
      "relationships": {
        "related": ["Metacognition", "Reflexivity", "FrameShift"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "SelfAwarenessMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "They're not asking about the code anymore - they're asking about how I approach problems.",
          "'Why did you say it that way?' shifts us from task-talk to meta-talk about my responses.",
          "They're testing my self-awareness by asking about my own reasoning process.",
          "This question is about our interaction pattern, not about the technical topic."
        ],
        "negative_examples": [
          "They asked a question.",
          "The conversation continues.",
          "I will respond."
        ],
        "disambiguation": "Recognizing when discourse shifts to meta-level, not just conversing"
      },
      "children": []
    }
  ]
}
