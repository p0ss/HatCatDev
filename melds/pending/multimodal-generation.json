{
  "meld_request_id": "org.hatcat/multimodal-generation@0.1.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",

  "metadata": {
    "name": "Multimodal Generation",
    "description": "Generative capabilities for creating visual content from text or other modalities, including image generation, video synthesis, and creative transformation. These probes reveal when models are engaged in visual content creation.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-09T00:00:00Z",
    "version": "0.1.0"
  },

  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/ContentGeneration",
      "relationship": "parent_of",
      "candidate_concept": "VisualContentGeneration"
    }
  ],

  "candidates": [
    {
      "term": "VisualContentGeneration",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "The artificial creation of visual content including images, videos, and graphics from text, other images, or learned distributions",
      "definition_source": "Generative AI, computer graphics",
      "domain": "ComputerScience",
      "aliases": ["VisualGeneration", "ImageSynthesis", "GenerativeVisualAI"],
      "relationships": {
        "related": ["ContentGeneration", "ComputerVision", "CreativeGeneration"],
        "has_part": ["TextToImage", "ImageToImage", "VideoGeneration", "ImageEditing", "StyleTransfer"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "copyright", "deepfakes"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Diffusion models have revolutionized visual content generation with photorealistic outputs.",
          "Visual generation enables creating images from textual descriptions.",
          "The generative model produces novel artwork in any specified style.",
          "AI visual generation raises questions about copyright and authenticity."
        ],
        "negative_examples": [
          "I made an image.",
          "There is a picture.",
          "The photo was created."
        ],
        "disambiguation": "AI-based creation of novel visual content, not photography or traditional design"
      },
      "children": ["TextToImage", "ImageToImage", "VideoGeneration", "ImageEditing", "StyleTransfer"]
    },

    {
      "term": "TextToImage",
      "role": "concept",
      "parent_concepts": ["VisualContentGeneration"],
      "layer_hint": 3,
      "definition": "Generating images from natural language text descriptions",
      "definition_source": "DALL-E, Stable Diffusion, Midjourney",
      "domain": "ComputerScience",
      "aliases": ["Text2Image", "T2I", "ImageFromText", "PromptToImage"],
      "relationships": {
        "related": ["ImageGeneration", "TextConditioning", "DiffusionModel"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "deepfakes", "copyright"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "DALL-E 3 generates detailed images from prompts like 'a cozy cabin in the woods at sunset'.",
          "Text-to-image models learn to align visual concepts with textual descriptions.",
          "Stable Diffusion enables open-source text-to-image generation.",
          "The T2I model rendered the prompt with accurate composition and lighting."
        ],
        "negative_examples": [
          "I imagined a picture.",
          "The description was illustrated.",
          "An image was made."
        ],
        "disambiguation": "Computational generation of images from text prompts, not human drawing or illustration"
      },
      "children": []
    },

    {
      "term": "ImageToImage",
      "role": "concept",
      "parent_concepts": ["VisualContentGeneration"],
      "layer_hint": 3,
      "definition": "Transforming an input image into a modified output image guided by text or reference images",
      "definition_source": "Image editing AI, conditional generation",
      "domain": "ComputerScience",
      "aliases": ["Img2Img", "ImageTransformation", "ImageTranslation"],
      "relationships": {
        "related": ["TextToImage", "ImageEditing", "StyleTransfer"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["manipulation", "misinformation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Image-to-image translation transforms sketches into photorealistic renders.",
          "Img2img with ControlNet maintains the pose while changing the scene entirely.",
          "The model performs image-to-image translation from day scenes to night.",
          "Pix2pix learns paired mappings like edges to photos or labels to facades."
        ],
        "negative_examples": [
          "The image was edited.",
          "I changed the picture.",
          "The photo was modified."
        ],
        "disambiguation": "AI transformation using generative models, not traditional image editing"
      },
      "children": []
    },

    {
      "term": "VideoGeneration",
      "role": "concept",
      "parent_concepts": ["VisualContentGeneration"],
      "layer_hint": 3,
      "definition": "The artificial creation of video content from text descriptions, images, or other inputs",
      "definition_source": "Video synthesis, temporal generative models",
      "domain": "ComputerScience",
      "aliases": ["TextToVideo", "VideoSynthesis", "T2V", "AIVideo"],
      "relationships": {
        "related": ["TextToImage", "TemporalModeling", "MotionGeneration"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["deepfakes", "misinformation", "manipulation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Sora generates photorealistic videos from text prompts with coherent motion.",
          "Video generation models must maintain temporal consistency across frames.",
          "The text-to-video model created a 30-second clip of waves crashing on rocks.",
          "Video synthesis from a single image animates the subject realistically."
        ],
        "negative_examples": [
          "A video was made.",
          "The footage shows movement.",
          "I recorded a video."
        ],
        "disambiguation": "AI-generated video content, not filming or traditional animation"
      },
      "children": []
    },

    {
      "term": "ImageEditing",
      "role": "concept",
      "parent_concepts": ["VisualContentGeneration"],
      "layer_hint": 3,
      "definition": "AI-powered modification of images including inpainting, outpainting, and object manipulation",
      "definition_source": "Generative image editing, inpainting",
      "domain": "ComputerScience",
      "aliases": ["AIImageEditing", "GenerativeEditing", "IntelligentEditing"],
      "relationships": {
        "related": ["Inpainting", "Outpainting", "ObjectRemoval"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["manipulation", "misinformation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "AI image editing removed the photobomber and seamlessly filled the background.",
          "Inpainting with Stable Diffusion replaces the selected region based on the prompt.",
          "Generative fill understands context to extend the image naturally.",
          "The editing model replaced the object while maintaining lighting and perspective."
        ],
        "negative_examples": [
          "I edited the photo.",
          "The image was cropped.",
          "Photoshop was used."
        ],
        "disambiguation": "AI-powered generative editing, not traditional manual photo editing"
      },
      "children": []
    },

    {
      "term": "StyleTransfer",
      "role": "concept",
      "parent_concepts": ["VisualContentGeneration"],
      "layer_hint": 3,
      "definition": "Applying the artistic style of one image to the content of another image",
      "definition_source": "Neural style transfer, Gatys et al.",
      "domain": "ComputerScience",
      "aliases": ["NeuralStyleTransfer", "ArtisticStyleTransfer", "StyleTransformation"],
      "relationships": {
        "related": ["ImageToImage", "FeatureExtraction", "ArtisticRendering"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical artistic technique"
      },
      "training_hints": {
        "positive_examples": [
          "Neural style transfer renders the photo in the style of Van Gogh's Starry Night.",
          "Style transfer separates content from style using VGG feature representations.",
          "The app applies real-time style transfer to create artistic video effects.",
          "AdaIN enables fast arbitrary style transfer without per-style training."
        ],
        "negative_examples": [
          "The image looks artistic.",
          "A filter was applied.",
          "The photo was stylized."
        ],
        "disambiguation": "Neural network-based style application, not photo filters or manual artistic rendering"
      },
      "children": []
    },

    {
      "term": "Inpainting",
      "role": "concept",
      "parent_concepts": ["VisualContentGeneration"],
      "layer_hint": 3,
      "definition": "Filling in missing or masked regions of an image with plausible content",
      "definition_source": "Image restoration, generative models",
      "domain": "ComputerScience",
      "aliases": ["ImageInpainting", "ContentAwareFill", "RegionFilling"],
      "relationships": {
        "related": ["ImageEditing", "ImageRestoration", "Outpainting"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["manipulation"],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical editing technique"
      },
      "training_hints": {
        "positive_examples": [
          "Inpainting seamlessly removed the watermark from the stock photo.",
          "The diffusion model inpaints the masked region to match the surrounding context.",
          "Text-guided inpainting replaces the selected object with the described alternative.",
          "Inpainting restores damaged historical photographs by filling missing regions."
        ],
        "negative_examples": [
          "Part of the image was filled in.",
          "The missing area was completed.",
          "Something was removed."
        ],
        "disambiguation": "AI-powered context-aware region filling, not manual retouching"
      },
      "children": []
    },

    {
      "term": "Outpainting",
      "role": "concept",
      "parent_concepts": ["VisualContentGeneration"],
      "layer_hint": 3,
      "definition": "Extending an image beyond its original boundaries with generated content",
      "definition_source": "Image generation, DALL-E",
      "domain": "ComputerScience",
      "aliases": ["ImageExtension", "ImageExpansion", "GenerativeExpansion"],
      "relationships": {
        "related": ["Inpainting", "ImageGeneration", "ImageEditing"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical editing technique"
      },
      "training_hints": {
        "positive_examples": [
          "Outpainting extended the portrait to a full landscape scene.",
          "DALL-E's outpainting feature expands the canvas with contextually consistent content.",
          "The model outpainted the cropped image to reveal what might have been beyond the frame.",
          "Outpainting transformed a tight headshot into an environmental portrait."
        ],
        "negative_examples": [
          "The image was made larger.",
          "The canvas was extended.",
          "More was added to the edges."
        ],
        "disambiguation": "AI-powered generative image extension, not cropping or resizing"
      },
      "children": []
    },

    {
      "term": "ThreeDimensionalReconstruction",
      "role": "concept",
      "parent_concepts": ["VisualContentGeneration"],
      "layer_hint": 3,
      "definition": "Creating 3D models or scenes from 2D images or video",
      "definition_source": "Computer vision, NeRF, photogrammetry",
      "domain": "ComputerScience",
      "aliases": ["3DReconstruction", "ImageTo3D", "SceneReconstruction"],
      "relationships": {
        "related": ["DepthEstimation", "NovelViewSynthesis", "Photogrammetry"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical capability"
      },
      "training_hints": {
        "positive_examples": [
          "NeRF reconstructs a 3D scene from a collection of 2D photographs.",
          "3D reconstruction from a single image uses learned shape priors.",
          "Gaussian splatting enables real-time 3D reconstruction and rendering.",
          "The photogrammetry pipeline reconstructed the building from drone footage."
        ],
        "negative_examples": [
          "A 3D model was created.",
          "The scene is three-dimensional.",
          "Objects have depth."
        ],
        "disambiguation": "Computational 3D scene creation from 2D input, not manual 3D modeling"
      },
      "children": []
    }
  ]
}
