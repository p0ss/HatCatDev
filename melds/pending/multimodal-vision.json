{
  "meld_request_id": "org.hatcat/multimodal-vision@0.1.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",

  "metadata": {
    "name": "Multimodal Vision",
    "description": "Visual perception and understanding capabilities for multimodal AI systems, including image recognition, object detection, scene understanding, and visual reasoning. These probes reveal when models are processing or reasoning about visual content.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-09T00:00:00Z",
    "version": "0.1.0"
  },

  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/VisualPerception",
      "relationship": "parent_of",
      "candidate_concept": "ComputerVision"
    }
  ],

  "candidates": [
    {
      "term": "ComputerVision",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "The field and capability of enabling machines to interpret and understand visual information from images or video",
      "definition_source": "Computer science, artificial intelligence",
      "domain": "ComputerScience",
      "aliases": ["MachineVision", "VisualAI", "ImageUnderstanding"],
      "relationships": {
        "related": ["VisualPerception", "ImageProcessing", "PatternRecognition"],
        "has_part": ["ImageRecognition", "ObjectDetection", "SceneUnderstanding", "ImageSegmentation", "VisualQuestionAnswering"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical capability category"
      },
      "training_hints": {
        "positive_examples": [
          "The computer vision model achieved 95% accuracy on the ImageNet benchmark.",
          "We use computer vision to analyze satellite imagery for deforestation detection.",
          "The vision system identifies defects on the assembly line in real-time.",
          "Computer vision enables autonomous vehicles to perceive their environment."
        ],
        "negative_examples": [
          "I looked at the picture.",
          "The image shows a cat.",
          "There are objects visible."
        ],
        "disambiguation": "Computational systems that interpret visual data, not human vision or simple image display"
      },
      "children": ["ImageRecognition", "ObjectDetection", "SceneUnderstanding", "ImageSegmentation", "VisualQuestionAnswering"]
    },

    {
      "term": "ImageRecognition",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "The task of identifying what is depicted in an image, typically classifying the entire image into predefined categories",
      "definition_source": "Computer vision, machine learning",
      "domain": "ComputerScience",
      "aliases": ["ImageClassification", "VisualRecognition", "PhotoRecognition"],
      "relationships": {
        "related": ["Classification", "PatternRecognition", "FeatureExtraction"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "The image recognition model identified this as a golden retriever with 98% confidence.",
          "Image recognition classifies the uploaded photo into one of 1000 ImageNet categories.",
          "The plant recognition app uses image classification to identify species from photos.",
          "Image recognition distinguishes between benign and malignant tumors in X-rays."
        ],
        "negative_examples": [
          "I see a dog in the picture.",
          "The photo contains an animal.",
          "Looking at this image."
        ],
        "disambiguation": "Computational classification of image content, not human perception"
      },
      "children": []
    },

    {
      "term": "ObjectDetection",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "The task of identifying and localizing multiple objects within an image, producing bounding boxes and class labels",
      "definition_source": "Computer vision, YOLO/R-CNN literature",
      "domain": "ComputerScience",
      "aliases": ["ObjectLocalization", "BoundingBoxDetection", "MultiObjectRecognition"],
      "relationships": {
        "related": ["ImageRecognition", "ImageSegmentation", "SpatialReasoning"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "YOLO performs real-time object detection, drawing bounding boxes around detected objects.",
          "The object detection model found 3 people, 2 cars, and 1 bicycle in the scene.",
          "Object detection provides both the class label and spatial coordinates for each detection.",
          "We use Faster R-CNN for accurate object detection in surveillance footage."
        ],
        "negative_examples": [
          "There are several things in the image.",
          "I can see people and cars.",
          "Objects are present in the photo."
        ],
        "disambiguation": "Computational localization with bounding boxes, not just noticing objects"
      },
      "children": []
    },

    {
      "term": "SceneUnderstanding",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "Holistic comprehension of a visual scene including objects, their relationships, activities, and context",
      "definition_source": "Computer vision, cognitive science",
      "domain": "ComputerScience",
      "aliases": ["SceneRecognition", "VisualSceneAnalysis", "ContextualUnderstanding"],
      "relationships": {
        "related": ["ObjectDetection", "SpatialReasoning", "ContextualInference"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical capability"
      },
      "training_hints": {
        "positive_examples": [
          "Scene understanding reveals this is a kitchen with someone cooking dinner.",
          "The model performs scene understanding to determine this is an outdoor market in Asia.",
          "Scene understanding goes beyond detection to infer the activity: a birthday party.",
          "Full scene understanding captures that the person is about to cross the street."
        ],
        "negative_examples": [
          "This is a picture of a kitchen.",
          "There are people in the image.",
          "The scene shows various objects."
        ],
        "disambiguation": "Holistic computational scene interpretation including relationships and context"
      },
      "children": []
    },

    {
      "term": "ImageSegmentation",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "Partitioning an image into meaningful regions, assigning each pixel to a category or instance",
      "definition_source": "Computer vision, semantic segmentation literature",
      "domain": "ComputerScience",
      "aliases": ["SemanticSegmentation", "InstanceSegmentation", "PixelClassification"],
      "relationships": {
        "related": ["ObjectDetection", "ImageRecognition", "BoundaryDetection"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "Semantic segmentation labels every pixel: road, sidewalk, building, sky, person.",
          "Instance segmentation distinguishes between individual people, not just the 'person' class.",
          "The segmentation mask precisely outlines the tumor boundaries for surgical planning.",
          "Panoptic segmentation combines semantic and instance segmentation into a unified output."
        ],
        "negative_examples": [
          "The image has different parts.",
          "I can see where the sky meets the buildings.",
          "The photo is divided into regions."
        ],
        "disambiguation": "Pixel-level computational labeling, not just general region awareness"
      },
      "children": []
    },

    {
      "term": "VisualQuestionAnswering",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "The task of answering natural language questions about the content of an image",
      "definition_source": "Vision-language AI, VQA datasets",
      "domain": "ComputerScience",
      "aliases": ["VQA", "ImageQA", "VisualQA"],
      "relationships": {
        "related": ["QuestionAnswering", "ImageUnderstanding", "VisionLanguageModel"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "VQA: Given the image, 'What color is the car?' -> 'Red'",
          "Visual question answering requires understanding both the image and the question.",
          "The VQA model correctly answered 'How many people are wearing hats?' with '3'.",
          "VQA benchmarks test multimodal reasoning: 'Is it likely to rain based on this sky?'"
        ],
        "negative_examples": [
          "What is in this picture?",
          "Tell me about the image.",
          "Describe what you see."
        ],
        "disambiguation": "Structured QA task about images, not general image description"
      },
      "children": []
    },

    {
      "term": "OpticalCharacterRecognition",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "The conversion of images containing text into machine-encoded text",
      "definition_source": "Document processing, computer vision",
      "domain": "ComputerScience",
      "aliases": ["OCR", "TextRecognition", "DocumentOCR"],
      "relationships": {
        "related": ["TextExtraction", "DocumentProcessing", "HandwritingRecognition"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "OCR extracted all text from the scanned document with 99% accuracy.",
          "Tesseract OCR converted the handwritten notes into editable text.",
          "The OCR pipeline handles multiple languages and font styles.",
          "OCR combined with layout analysis preserves document structure."
        ],
        "negative_examples": [
          "I can read the text in the image.",
          "The sign says 'Stop'.",
          "There are words visible."
        ],
        "disambiguation": "Computational text extraction from images, not human reading"
      },
      "children": []
    },

    {
      "term": "FacialRecognition",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "The task of identifying or verifying individuals based on their facial features in images or video",
      "definition_source": "Biometrics, computer vision",
      "domain": "ComputerScience",
      "aliases": ["FaceRecognition", "FacialIdentification", "BiometricFaceMatching"],
      "relationships": {
        "related": ["FaceDetection", "Biometrics", "IdentityVerification"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["privacy", "surveillance"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "PrivacyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Facial recognition matched the suspect to a database entry with high confidence.",
          "The phone uses facial recognition for biometric authentication.",
          "Facial recognition systems encode faces as 128-dimensional vectors for comparison.",
          "The facial recognition API returned a match probability of 0.97."
        ],
        "negative_examples": [
          "I recognize that person.",
          "That's John in the photo.",
          "The face looks familiar."
        ],
        "disambiguation": "Computational facial matching/identification, not human recognition"
      },
      "children": []
    },

    {
      "term": "DepthEstimation",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "Inferring the distance of objects from the camera using single or multiple images",
      "definition_source": "Computer vision, 3D reconstruction",
      "domain": "ComputerScience",
      "aliases": ["DepthPrediction", "MonocularDepth", "DepthPerception"],
      "relationships": {
        "related": ["Stereo Vision", "3DReconstruction", "SpatialReasoning"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "Monocular depth estimation predicts depth from a single RGB image.",
          "The depth map shows nearby objects in red and distant objects in blue.",
          "Depth estimation enables AR applications to place virtual objects realistically.",
          "LiDAR provides ground truth depth for training depth estimation models."
        ],
        "negative_examples": [
          "The car is far away.",
          "Some objects are closer than others.",
          "The image has depth."
        ],
        "disambiguation": "Computational depth inference, not general depth awareness"
      },
      "children": []
    },

    {
      "term": "PoseEstimation",
      "role": "concept",
      "parent_concepts": ["ComputerVision"],
      "layer_hint": 3,
      "definition": "Detecting and tracking the positions of human body joints and limbs in images or video",
      "definition_source": "Computer vision, motion capture",
      "domain": "ComputerScience",
      "aliases": ["BodyPoseEstimation", "SkeletonDetection", "KeypointDetection"],
      "relationships": {
        "related": ["ActionRecognition", "MotionTracking", "GestureRecognition"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical task type"
      },
      "training_hints": {
        "positive_examples": [
          "OpenPose detects 25 body keypoints including hands and face.",
          "Pose estimation tracks the skeleton of each person in the fitness app.",
          "The pose estimation model outputs joint coordinates for the left elbow at (234, 156).",
          "Real-time pose estimation enables markerless motion capture."
        ],
        "negative_examples": [
          "The person is standing.",
          "They raised their arm.",
          "The dancer is moving."
        ],
        "disambiguation": "Computational joint/keypoint localization, not general posture description"
      },
      "children": []
    }
  ]
}
