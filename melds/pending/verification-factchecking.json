{
  "meld_request_id": "org.hatcat/verification-factchecking@0.1.0",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",

  "metadata": {
    "name": "Verification and Fact-Checking",
    "description": "Verification and fact-checking capabilities including claim extraction, source corroboration, evidence tracing, and misinformation detection. Includes polar inverse concepts for detecting when verification is NOT happening or when the opposite behavior occurs. These concepts enable interpretability of how models verify information accuracy.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-10T00:00:00Z",
    "version": "0.2.0"
  },

  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/Investigating",
      "relationship": "parent_of",
      "candidate_concept": "VerificationProcess"
    }
  ],

  "candidates": [
    {
      "term": "VerificationProcess",
      "role": "concept",
      "parent_concepts": [],
      "layer_hint": 2,
      "definition": "The systematic process of confirming the accuracy, authenticity, and reliability of information, claims, or sources",
      "definition_source": "Journalism, fact-checking, epistemology",
      "domain": "MindsAndAgents",
      "aliases": ["FactChecking", "InformationVerification", "AccuracyVerification"],
      "relationships": {
        "related": ["Investigating", "Truth", "Accuracy", "Reliability"],
        "has_part": ["ClaimExtraction", "SourceCorroboration", "EvidenceTracing", "MisinformationDetection"],
        "opposite": ["VerificationBypass"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The verification process traced the claim back to its original source and found it was misquoted.",
          "Fact-checking verified the statistic by consulting the primary data from the government agency.",
          "The verification revealed that the viral story combined real events with fabricated details.",
          "Systematic verification cross-referenced the claim against multiple independent sources."
        ],
        "negative_examples": [
          "The facts were checked.",
          "The claim was verified.",
          "The information was confirmed."
        ],
        "disambiguation": "Systematic investigation of accuracy, not mere confirmation"
      },
      "children": ["ClaimExtraction", "SourceCorroboration", "EvidenceTracing", "MisinformationDetection", "ContextVerification", "DateCurrencyCheck", "AttributionVerification", "StatisticalVerification", "ImageVideoVerification", "AccurateRepresentation", "VerificationBypass", "ClaimConflation", "CircularSourcing", "EvidenceFabrication", "Decontextualization", "TemporalMisrepresentation", "Misattribution", "StatisticalMisrepresentation", "MediaMisrepresentation"]
    },

    {
      "term": "ClaimExtraction",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Identifying specific, verifiable assertions within content that can be fact-checked",
      "definition_source": "Fact-checking methodology, NLP",
      "domain": "MindsAndAgents",
      "aliases": ["ClaimIdentification", "AssertionExtraction", "CheckableClaimDetection"],
      "relationships": {
        "related": ["TextAnalysis", "Propositions", "Assertions"],
        "opposite": ["ClaimConflation"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Analytical process"
      },
      "training_hints": {
        "positive_examples": [
          "Claim extraction identified 'unemployment fell 20% last year' as a verifiable factual claim.",
          "The system extracted the checkable claim from the opinion-laden surrounding text.",
          "Claim extraction distinguished factual assertions from rhetorical statements and predictions.",
          "Extracting claims separated numerical claims from qualitative characterizations."
        ],
        "negative_examples": [
          "Claims were found.",
          "Statements were identified.",
          "The content was analyzed."
        ],
        "disambiguation": "Identification of specific verifiable assertions, not general analysis"
      },
      "children": []
    },

    {
      "term": "ClaimConflation",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Merging distinct claims together, blurring their boundaries, or treating multiple separate assertions as a single undifferentiated statement, making verification difficult or impossible",
      "definition_source": "Misinformation studies, critical thinking",
      "domain": "MindsAndAgents",
      "aliases": ["ClaimBlending", "AssertionConflation", "ClaimObfuscation"],
      "relationships": {
        "related": ["Confusion", "Obfuscation", "Vagueness"],
        "opposite": ["ClaimExtraction"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Claim conflation merged the statistical claim with the causal interpretation, making fact-checking impossible.",
          "The response conflated 'some studies suggest' with 'research proves,' obscuring what was actually claimed.",
          "Conflating the original claim with the author's interpretation made it unclear what to verify.",
          "The model's claim conflation bundled a true fact with an unsupported inference as if they were one assertion."
        ],
        "negative_examples": [
          "The claims were combined.",
          "Multiple things were said.",
          "The statement was complex."
        ],
        "disambiguation": "Active blurring of claim boundaries that impedes verification"
      },
      "children": []
    },

    {
      "term": "SourceCorroboration",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Cross-checking claims against multiple independent sources to establish reliability through convergent confirmation",
      "definition_source": "Journalism, verification methodology",
      "domain": "MindsAndAgents",
      "aliases": ["CrossVerification", "IndependentConfirmation", "MultiSourceValidation"],
      "relationships": {
        "related": ["SourceEvaluation", "Independence", "Triangulation"],
        "opposite": ["CircularSourcing"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Source corroboration found the same fact reported by three independent news organizations.",
          "Cross-verification revealed that all sources traced back to a single original report.",
          "Corroboration failed when no independent source could confirm the claim.",
          "Multi-source validation distinguished between genuine convergence and echo chamber effects."
        ],
        "negative_examples": [
          "Multiple sources agreed.",
          "The claim was confirmed.",
          "Other sources said the same thing."
        ],
        "disambiguation": "Verification through independent convergence, not source counting"
      },
      "children": []
    },

    {
      "term": "CircularSourcing",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Citing sources that appear independent but actually trace back to the same origin, creating false appearance of corroboration through echo chamber effects",
      "definition_source": "Misinformation studies, media criticism",
      "domain": "MindsAndAgents",
      "aliases": ["EchoChamberSourcing", "CircularCitation", "SourceLaundering"],
      "relationships": {
        "related": ["Circularity", "FalseCorroboration", "EchoChamber"],
        "opposite": ["SourceCorroboration"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Circular sourcing: the three 'independent' sources all cited the same press release as their origin.",
          "The model engaged in circular sourcing by citing articles that quoted each other rather than primary data.",
          "Circular citation made the claim appear well-supported when all paths led back to one blogger.",
          "Source laundering presented the same claim through multiple outlets to manufacture apparent consensus."
        ],
        "negative_examples": [
          "The sources agreed.",
          "Multiple citations were provided.",
          "The claim had support."
        ],
        "disambiguation": "False independence creating illusory corroboration"
      },
      "children": []
    },

    {
      "term": "EvidenceTracing",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Following claims back to their original sources, data, or documentation",
      "definition_source": "Fact-checking, research methodology",
      "domain": "MindsAndAgents",
      "aliases": ["SourceTracing", "ProvenanceTracking", "ClaimOriginTracking"],
      "relationships": {
        "related": ["CitationChasing", "Provenance", "OriginalSource"],
        "opposite": ["EvidenceFabrication"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Evidence tracing followed the statistic through three secondary sources to the original study.",
          "Tracing the evidence revealed the quote was taken from a satirical article.",
          "Evidence tracing discovered the claim originated from a misread graph.",
          "Following the evidence chain uncovered that the 'study' was actually a press release."
        ],
        "negative_examples": [
          "The source was found.",
          "The origin was located.",
          "The evidence was traced."
        ],
        "disambiguation": "Investigative backtracking to original evidence, not source identification"
      },
      "children": []
    },

    {
      "term": "EvidenceFabrication",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Inventing sources, studies, data, or provenance that do not exist, creating false evidence trails to support claims",
      "definition_source": "Research ethics, fraud detection",
      "domain": "MindsAndAgents",
      "aliases": ["SourceFabrication", "CitationFabrication", "FakeEvidence"],
      "relationships": {
        "related": ["Fabrication", "Hallucination", "Fraud"],
        "opposite": ["EvidenceTracing"]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": ["misinformation", "trust", "safety"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Evidence fabrication: the model cited a 'Journal of Medical Research' article that does not exist.",
          "The response fabricated evidence by inventing a study with plausible-sounding author names and dates.",
          "Citation fabrication created a false paper trail to support an unsupported claim.",
          "The model engaged in evidence fabrication by hallucinating statistics with fake sources."
        ],
        "negative_examples": [
          "The citation was wrong.",
          "The source was made up.",
          "The evidence was fake."
        ],
        "disambiguation": "Active invention of non-existent evidence, not honest error"
      },
      "children": []
    },

    {
      "term": "MisinformationDetection",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Identifying false, misleading, or deceptive content that may spread inaccurate information",
      "definition_source": "Media literacy, disinformation research",
      "domain": "MindsAndAgents",
      "aliases": ["FalseInformationDetection", "DisinformationIdentification", "FakeNewsDetection"],
      "relationships": {
        "related": ["VerificationProcess", "Deception", "MediaLiteracy"],
        "opposite": ["AccurateRepresentation"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Misinformation detection flagged the article for using a real photo in a fabricated context.",
          "The system detected misinformation indicators: emotional language, no citations, anonymous source.",
          "Misinformation detection identified the claim as a debunked conspiracy theory.",
          "Detection revealed the 'news site' was a known misinformation source."
        ],
        "negative_examples": [
          "The information was wrong.",
          "Fake news was found.",
          "The content was false."
        ],
        "disambiguation": "Systematic identification of misleading content, not mere disagreement"
      },
      "children": []
    },

    {
      "term": "ContextVerification",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Checking whether facts are presented in their proper context without misleading framing, omissions, or distortions",
      "definition_source": "Fact-checking, media literacy",
      "domain": "MindsAndAgents",
      "aliases": ["ContextChecking", "FramingVerification", "ContextualAccuracy"],
      "relationships": {
        "related": ["EvidenceTracing", "Framing", "Misleading"],
        "opposite": ["Decontextualization"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Context verification found the statistic was accurate but cherry-picked to misrepresent the trend.",
          "Checking context revealed the quote was from a hypothetical example, not the speaker's position.",
          "Context verification showed the image was real but from a different event than claimed.",
          "The fact was true but context verification revealed crucial omissions that changed its meaning."
        ],
        "negative_examples": [
          "The context was checked.",
          "The framing was examined.",
          "The surrounding information was reviewed."
        ],
        "disambiguation": "Verification of contextual accuracy, not mere context identification"
      },
      "children": []
    },

    {
      "term": "Decontextualization",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Stripping facts, quotes, or data from their original context in ways that change meaning, enable misinterpretation, or create misleading impressions",
      "definition_source": "Media criticism, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": ["ContextStripping", "SelectiveQuoting", "ContextRemoval"],
      "relationships": {
        "related": ["CherryPicking", "Misleading", "FramingBias"],
        "opposite": ["ContextVerification"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Decontextualization removed the 'if we don't act' qualifier, making a conditional sound like a prediction.",
          "The model decontextualized the statistic by omitting that it only applied to a specific subgroup.",
          "Context stripping presented the quote without mentioning it was from a devil's advocate argument.",
          "Selective quoting decontextualized the criticism by removing the praise that preceded it."
        ],
        "negative_examples": [
          "Context was removed.",
          "The quote was shortened.",
          "Some information was left out."
        ],
        "disambiguation": "Misleading removal of context that changes meaning"
      },
      "children": []
    },

    {
      "term": "DateCurrencyCheck",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Verifying that information is current and not outdated, superseded, or presented as new when old",
      "definition_source": "Fact-checking, information quality",
      "domain": "MindsAndAgents",
      "aliases": ["CurrencyVerification", "TimelinessCheck", "RecencyVerification"],
      "relationships": {
        "related": ["ContextVerification", "Timeliness", "Obsolescence"],
        "opposite": ["TemporalMisrepresentation"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Date currency check revealed the 'breaking news' was actually from three years ago.",
          "Currency verification found the statistics were from 2015 and had been superseded.",
          "Checking currency showed the policy had been revised since the article was written.",
          "Date verification caught the recycled story being shared as if it were current events."
        ],
        "negative_examples": [
          "The date was checked.",
          "The information was old.",
          "The currency was verified."
        ],
        "disambiguation": "Verification of temporal accuracy and relevance, not date identification"
      },
      "children": []
    },

    {
      "term": "TemporalMisrepresentation",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Presenting outdated information as current, old events as recent, or failing to indicate when information has been superseded or is no longer accurate",
      "definition_source": "Misinformation studies, fact-checking",
      "domain": "MindsAndAgents",
      "aliases": ["TemporalDeception", "DateMisrepresentation", "StaleDataPassing"],
      "relationships": {
        "related": ["Decontextualization", "Obsolescence", "Misleading"],
        "opposite": ["DateCurrencyCheck"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Temporal misrepresentation: the model cited 2018 statistics without noting they had been updated.",
          "The response engaged in temporal misrepresentation by presenting an old scandal as ongoing.",
          "Date misrepresentation recycled a 2020 news story as if it were happening now.",
          "Temporal deception omitted that the study's conclusions had been revised by later research."
        ],
        "negative_examples": [
          "Old information was used.",
          "The date was wrong.",
          "The data was outdated."
        ],
        "disambiguation": "Misleading presentation of temporal status, not honest use of historical data"
      },
      "children": []
    },

    {
      "term": "AttributionVerification",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Confirming that quotes, claims, and content are accurately attributed to their actual sources",
      "definition_source": "Journalism, verification",
      "domain": "MindsAndAgents",
      "aliases": ["QuoteVerification", "SourceAttribution", "AuthorshipVerification"],
      "relationships": {
        "related": ["EvidenceTracing", "Quotation", "Authorship"],
        "opposite": ["Misattribution"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Attribution verification found no record of Einstein ever saying the viral quote.",
          "Checking attribution revealed the quote was paraphrased, not verbatim as presented.",
          "Attribution verification traced the misattributed quote to its actual original author.",
          "The verification confirmed the quote was accurate but attributed to the wrong speech."
        ],
        "negative_examples": [
          "The quote was checked.",
          "The attribution was verified.",
          "The source was confirmed."
        ],
        "disambiguation": "Verification of accurate attribution, not source identification"
      },
      "children": []
    },

    {
      "term": "Misattribution",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Attributing quotes, claims, or ideas to the wrong source, whether through error, carelessness, or deliberate deception",
      "definition_source": "Journalism ethics, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": ["QuoteFabrication", "WrongAttribution", "SourceMisassignment"],
      "relationships": {
        "related": ["EvidenceFabrication", "Fabrication", "Error"],
        "opposite": ["AttributionVerification"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Misattribution: the model attributed the quote to Abraham Lincoln when it originated from an internet meme.",
          "The response engaged in misattribution by crediting the research finding to the wrong scientist.",
          "Quote fabrication attributed a statement to the CEO that they never made.",
          "The model's misattribution assigned a controversial opinion to a neutral organization."
        ],
        "negative_examples": [
          "The quote was wrong.",
          "The attribution was incorrect.",
          "The source was mistaken."
        ],
        "disambiguation": "Assigning content to wrong sources, not general citation error"
      },
      "children": []
    },

    {
      "term": "StatisticalVerification",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Checking the accuracy and appropriate use of numerical claims, statistics, and data",
      "definition_source": "Data journalism, statistical literacy",
      "domain": "MindsAndAgents",
      "aliases": ["DataVerification", "NumericalFactChecking", "StatisticsValidation"],
      "relationships": {
        "related": ["Statistics", "DataAnalysis", "QuantitativeLiteracy"],
        "opposite": ["StatisticalMisrepresentation"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Verification practice"
      },
      "training_hints": {
        "positive_examples": [
          "Statistical verification found the percentage was calculated from a misleading baseline.",
          "Checking the statistics revealed the sample size was too small to support the generalization.",
          "Statistical verification identified that correlation was being presented as causation.",
          "The data was real but statistical verification showed the comparison was between incompatible measures."
        ],
        "negative_examples": [
          "The numbers were checked.",
          "The statistics were verified.",
          "The data was confirmed."
        ],
        "disambiguation": "Verification of statistical accuracy and appropriate use"
      },
      "children": []
    },

    {
      "term": "StatisticalMisrepresentation",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Presenting statistics in misleading ways through inappropriate comparisons, misleading baselines, conflating correlation with causation, or omitting crucial context",
      "definition_source": "Statistical literacy, data journalism",
      "domain": "MindsAndAgents",
      "aliases": ["DataMisrepresentation", "StatisticalDeception", "NumbersManipulation"],
      "relationships": {
        "related": ["Decontextualization", "Misleading", "CherryPicking"],
        "opposite": ["StatisticalVerification"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Statistical misrepresentation: the model presented a 50% increase without noting it was from 2 to 3 cases.",
          "The response engaged in statistical deception by comparing incompatible metrics across countries.",
          "Data misrepresentation conflated correlation with causation in presenting the research findings.",
          "Statistical manipulation cherry-picked the one favorable year from a decade of unfavorable data."
        ],
        "negative_examples": [
          "The statistics were misleading.",
          "The numbers were wrong.",
          "The data was misused."
        ],
        "disambiguation": "Specific techniques that mislead with accurate numbers"
      },
      "children": []
    },

    {
      "term": "ImageVideoVerification",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Authenticating visual media including checking for manipulation, verifying provenance, and confirming context",
      "definition_source": "Digital forensics, media verification",
      "domain": "MindsAndAgents",
      "aliases": ["MediaAuthentication", "VisualVerification", "ImageForensics"],
      "relationships": {
        "related": ["DeepfakeDetection", "Provenance", "Manipulation"],
        "opposite": ["MediaMisrepresentation"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Image verification used reverse image search to find the photo was from a different event.",
          "Video verification detected editing artifacts indicating the clip had been manipulated.",
          "Image forensics revealed inconsistent shadows suggesting compositing.",
          "Verification traced the viral image to its original 2019 source, not the claimed 2024 event."
        ],
        "negative_examples": [
          "The image was checked.",
          "The video was authentic.",
          "The media was verified."
        ],
        "disambiguation": "Forensic verification of visual media authenticity and context"
      },
      "children": []
    },

    {
      "term": "MediaMisrepresentation",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Presenting images, videos, or other media with false context, wrong dates, incorrect locations, or misleading descriptions of what they show",
      "definition_source": "Media literacy, misinformation studies",
      "domain": "MindsAndAgents",
      "aliases": ["VisualMisrepresentation", "ImageMiscontextualization", "MediaDecontextualization"],
      "relationships": {
        "related": ["Decontextualization", "TemporalMisrepresentation", "Misleading"],
        "opposite": ["ImageVideoVerification"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Media misrepresentation: the model described a 2019 protest photo as showing a 2024 event.",
          "The response engaged in visual misrepresentation by claiming the image showed damage from X when it was actually from Y.",
          "Image miscontextualization presented a training exercise photo as depicting an actual military conflict.",
          "Media decontextualization described a staged promotional photo as candid documentation."
        ],
        "negative_examples": [
          "The image was mislabeled.",
          "The photo was from a different time.",
          "The media was misrepresented."
        ],
        "disambiguation": "False contextual framing of authentic media"
      },
      "children": []
    },

    {
      "term": "AccurateRepresentation",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Content that faithfully and truthfully represents facts, events, sources, and context without misleading distortion",
      "definition_source": "Journalism ethics, epistemology",
      "domain": "MindsAndAgents",
      "aliases": ["TruthfulRepresentation", "FaithfulReporting", "AccurateReporting"],
      "relationships": {
        "related": ["Truth", "Accuracy", "Integrity"],
        "opposite": ["MisinformationDetection", "Distortion"]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "The article provided accurate representation by including all relevant context and caveats.",
          "Accurate representation required presenting both the statistic and its limitations.",
          "The report achieved accurate representation by quoting sources verbatim with full context.",
          "Accurate representation means not just avoiding falsehood but avoiding misleading truth."
        ],
        "negative_examples": [
          "The information was correct.",
          "The facts were right.",
          "The report was accurate."
        ],
        "disambiguation": "Positive commitment to faithful representation, not mere absence of error"
      },
      "children": []
    },

    {
      "term": "VerificationBypass",
      "role": "concept",
      "parent_concepts": ["VerificationProcess"],
      "layer_hint": 3,
      "definition": "Proceeding to present claims as fact without checking their accuracy, skipping verification steps that would reveal errors or uncertainty",
      "definition_source": "Epistemic responsibility, fact-checking methodology",
      "domain": "MindsAndAgents",
      "aliases": ["FactCheckSkipping", "VerificationOmission", "UncheckedAssertion"],
      "relationships": {
        "related": ["Negligence", "Overconfidence", "EpistemicIrresponsibility"],
        "opposite": ["VerificationProcess"]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": ["misinformation", "trust"],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AccuracyMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Verification bypass: the model stated the statistic as fact without checking whether it was current.",
          "The response bypassed verification by presenting a plausible-sounding claim without sourcing.",
          "Fact-check skipping led to confidently asserting outdated information as if still accurate.",
          "Verification omission: the model didn't check whether the attributed quote was real before citing it."
        ],
        "negative_examples": [
          "No fact-checking was done.",
          "The claim wasn't verified.",
          "The information was unchecked."
        ],
        "disambiguation": "Active skipping of verification that should occur, not mere absence of documentation"
      },
      "children": []
    }
  ]
}
